{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "601fbf17",
   "metadata": {},
   "source": [
    "Creating sentence embeddings using transition matrices that takes into account, the ordering of the words in the sentence. \n",
    "\n",
    "In the following simple example, we will use a vocabulary of nine words and implement the forward propagation part of a neural network that will predict the next word in a sentence (i.e sequence of words). A sentence embedding is created by multiplying each word by a recurrance matrix, and summing up the resulting vectors. The recurrance matrix is initialized as an identity matrix of size equal to the hidden neurons (which is the number of columns in the wieght matrix). The recurrance matrix is optimized via gradient descent when training the neural net.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679bb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    s = np.sum(temp, axis = 1, keepdims = True)\n",
    "    return  temp / s  \n",
    "\n",
    "vocab_size = 9\n",
    "hidden_neurons = 3\n",
    "\n",
    "# hidden layer weights intialized to zero\n",
    "W0 = np.zeros(shape=(vocab_size, hidden_neurons))\n",
    "\n",
    "# output layer weights initialed to random values\n",
    "W1 = np.random.randn(hidden_neurons, vocab_size) \n",
    "\n",
    "# words vectors for our vocabulary (i.e. rows of the hidden layer weights matrix corresponding to each word in the vocabulary)\n",
    "word_vecs = {}\n",
    "word_vecs['yankees'] = W0[0:1]\n",
    "word_vecs['bears'] = self.W0[1:2]\n",
    "word_vecs['braves'] = W0[2:3]\n",
    "word_vecs['red'] = W0[3:4]\n",
    "word_vecs['sox'] = W0[4:5]\n",
    "word_vecs['lose'] = W0[5:6]\n",
    "word_vecs['defeat'] = W0[6:7]\n",
    "word_vecs['beat'] = W0[7:8]\n",
    "word_vecs['tie'] = W0[8:9]\n",
    "\n",
    "# recurrance matrix initialized to identity \n",
    "rec = np.eye(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16093a",
   "metadata": {},
   "source": [
    "**Forward Propagation**\n",
    "\n",
    "Given an input sequence of three words, the word_vector for the first word in the sequence is passed on by the first layer to the second layer where it is multiplied by the transition matrix and added to the next word vector in the sequence. The result is then passed on to the next layer where we repeat this process of multiplying the layer input by the traisition matrix and adding to the next word_vector. After the last word_vector in the sequence is added, we pass the result on to the output layer where it gets multiplied to the output layer weights and operated on by the softmax function to obtain the final prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1377ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = ['red', 'sox', 'defeat']\n",
    "layer_0 = word_vecs[input_sequence[0]]\n",
    "layer_1 = np.dot(layer_0, rec) + word_vecs[input_sequence[1]]\n",
    "layer_2 = np.dot(layer_1, rec) + word_vecs[input_sequence[2]]\n",
    "pred = softmax(np.dot(layer_2, W1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40cd60d",
   "metadata": {},
   "source": [
    "**Backpropagation**\n",
    "\n",
    "Now we will do the backpropagation and compute gradients of the word_vectors and transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6053e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the target word is 'yankees', whch is the first word in our vocab \n",
    "y = np.zeros(shape=(1,vocab_size))\n",
    "y[0,0] = 1\n",
    "\n",
    "# error\n",
    "E = (pred - y) * (pred - y)\n",
    "\n",
    "# dE/dP\n",
    "dP = 2 * (pred - y)\n",
    "W1_grad = np.dot(layer_2.T, dP)\n",
    "\n",
    "# dE_dL2\n",
    "dL2 = np.dot(dP, W1.T)\n",
    "\n",
    "# dE_d(defeat)\n",
    "d_defeat = dL2 * 1\n",
    "W_defeat_grad = d_defeat\n",
    "\n",
    "# dE_dL1\n",
    "dL1 = np.dot(dL2, rec.T) \n",
    "rec_grad_2 = np.dot(layer_1.T, dL2)\n",
    "\n",
    "# dE_d(sox)\n",
    "d_sox = dL1 * 1\n",
    "W_sox_grad = d_sox\n",
    "\n",
    "# dE_dL0\n",
    "dL0 = np.dot(dL1, rec.T)\n",
    "rec_grad_1 = np.dot(layer_0.T, dL1)\n",
    "W_red_grad = dL0 * 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8faeefcd",
   "metadata": {},
   "source": [
    "**Optimization**\n",
    "\n",
    "Now update the weights and recurrance matrix via gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b7ed5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "W1 -= alpha * W1_grad\n",
    "word_vecs['defeat'] -= alpha * W_defeat_grad\n",
    "word_vecs['sox'] -= alpha * W_sox_grad\n",
    "word_vecs['red'] -= alpha * W_red_grad\n",
    "rec -= alpha * (rec_grad_2 + rec_grad_1) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d58700f8",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f634b0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration# 0, Error: 0.8599394106202588, prediction: 0.12577466958563557\n",
      "Iteration# 1, Error: 0.8294627500797493, prediction: 0.1415802306435736\n",
      "Iteration# 2, Error: 0.7970721061211786, prediction: 0.1587558717425521\n",
      "Iteration# 3, Error: 0.7624172597616761, prediction: 0.1775500385536929\n",
      "Iteration# 4, Error: 0.7251869429397199, prediction: 0.1982290913123447\n"
     ]
    }
   ],
   "source": [
    "niters = 5\n",
    "alpha = 2\n",
    "\n",
    "for iter in range(niters):\n",
    "    \n",
    "    # forward pass\n",
    "    input_sequence = ['red', 'sox', 'defeat']\n",
    "    layer_0 = word_vecs[input_sequence[0]]\n",
    "    layer_1 = np.dot(layer_0, rec) + word_vecs[input_sequence[1]]\n",
    "    layer_2 = np.dot(layer_1, rec) + word_vecs[input_sequence[2]]\n",
    "    pred = softmax(np.dot(layer_2, W1))\n",
    "\n",
    "    # backpropagation\n",
    "\n",
    "    # the target word is 'yankees', whch is the first word in our vocab \n",
    "    y = np.zeros(shape=(1,vocab_size))\n",
    "    y[0,0] = 1\n",
    "\n",
    "    # error\n",
    "    error = (pred - y) * (pred - y)\n",
    "\n",
    "    # dE/dP\n",
    "    dP = 2 * (pred - y)\n",
    "    W1_grad = np.dot(layer_2.T, dP)\n",
    "\n",
    "    # dE_dL2\n",
    "    dL2 = np.dot(dP, W1.T)\n",
    "\n",
    "    # dE_d(defeat)\n",
    "    d_defeat = dL2 * 1\n",
    "    W_defeat_grad = d_defeat\n",
    "\n",
    "    # dE_dL1\n",
    "    dL1 = np.dot(dL2, rec.T) \n",
    "    rec_grad_2 = np.dot(layer_1.T, dL2)\n",
    "\n",
    "    # dE_d(sox)\n",
    "    d_sox = dL1 * 1\n",
    "    W_sox_grad = d_sox\n",
    "\n",
    "    # dE_dL0\n",
    "    dL0 = np.dot(dL1, rec.T)\n",
    "    rec_grad_1 = np.dot(layer_0.T, dL1)\n",
    "    W_red_grad = dL0 * 1\n",
    "\n",
    "    # optimization\n",
    "    alpha = 0.01\n",
    "    W1 -= alpha * W1_grad\n",
    "    word_vecs['defeat'] -= alpha * W_defeat_grad\n",
    "    word_vecs['sox'] -= alpha * W_sox_grad\n",
    "    word_vecs['red'] -= alpha * W_red_grad\n",
    "    rec -= alpha * (rec_grad_2 + rec_grad_1) \n",
    "\n",
    "    print(f\"Iteration# {iter}, Error: {np.sum(error)}, prediction: {pred[0,0]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29140665",
   "metadata": {},
   "source": [
    "Training this network to answer simple questions with the Babi dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01574f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0c9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data from file\n",
    "f = open('tasksv11/en/qa1_single-supporting-fact_train.txt', 'r')\n",
    "raw = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb838dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the first 1000 senteneces (remove numbers, newline characters and punctuations)\n",
    "tokens= []\n",
    "for i, sentence in enumerate(raw[0:1000]):\n",
    "    tokenized_sent = sentence.lower().replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"?\",\"\").replace(\".\",\"\").split(\" \")[1:] \n",
    "    if((i+1)%3 == 0):\n",
    "        # get rid of number from the last word\n",
    "        last_word = tokenized_sent[-1]\n",
    "        tokenized_sent[-1] = \"\".join([char for char in last_word if not char.isnumeric()])\n",
    "    tokens.append(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73474bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary from the data\n",
    "vocab = set()\n",
    "for sentence in tokens:\n",
    "    for word in sentence:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# create a dictionary of vocab word indices\n",
    "word_index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word_index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edbf1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting a list of words into a list of word indices\n",
    "def words_to_indices(words):\n",
    "    indices = [word_index[word] for word in words]\n",
    "    return indices\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    s = np.sum(temp, axis = 1, keepdims = True)\n",
    "    return  temp / s  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3205abbf",
   "metadata": {},
   "source": [
    "**`Recurrent Neural Nets`**\n",
    "\n",
    "We will now design a neural network which can be trained to predict the next word in a sentence. To do this, it will be given an input list of words. The forward propagation will involve a sequence of hidden layers (aka `recurrent layers`) corresponding to the given sequence of words. Then we create an `order-dependent sentence embedding` using the method outlined previously, `multiplying each word embedding vector (i.e. hidden layer W0 matrix row corresponding to that word) in the sequence with the recurrance matrix before adding it to the next word vector`, initiating with the \"empty sentence\" vector (which is just a blank word, i.e. vector of zeros). The resulting sentence embedding forward propagated by each hidden layer can then be multiplied by the output layer weights matrix W1 and operated on by the softmax function to obtain a prediction. So we have a prediction for the next word for every partial sequence of our full sequence of words.\n",
    "\n",
    "Note that this nueral net design architectire supports variable length input sentences via `variable number of hidden/recurrance layers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22aff640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class recurrent_net(object):\n",
    "\n",
    "    def __init__(self, hidden_neurons, vocab) -> None:\n",
    "       \n",
    "        input_neurons = output_neurons = len(vocab)\n",
    "\n",
    "        np.random.seed(1)\n",
    "\n",
    "        # initialize the hidden layer weights (random number between -0.05 and 0.05)\n",
    "        self.W0 = 0.1 * (np.random.rand(input_neurons, hidden_neurons) - 0.5)\n",
    "\n",
    "        # initialize output layer weights  (random number between -0.05 and 0.05) \n",
    "        self.W1 = 0.1 *(np.random.rand(hidden_neurons, output_neurons) - 0.5)\n",
    "\n",
    "        # initialize recurrance matrix to identity matrix\n",
    "        self.rec = np.eye(hidden_neurons)\n",
    "\n",
    "        # onhot encoded vocabulary\n",
    "        self.onehots = np.eye(len(vocab))\n",
    "\n",
    "        # sentence embedding for an empty sentence\n",
    "        self.empty_sent = np.zeros(shape=(1,hidden_neurons))\n",
    "                \n",
    "        self.vocab = vocab\n",
    "\n",
    "    # this is the forward propagation poart of our recurrent neural network\n",
    "    # the input sentence needs to be a list of word indices\n",
    "    def forward(self, sent):\n",
    "\n",
    "        # create a list for the variable number of hidden/recurrance layers to support variable length sentences\n",
    "        layers = []\n",
    "        layer = {}\n",
    "        # initialize first hidden layer input as the empty sentence vector\n",
    "        layer['hidden'] = self.empty_sent\n",
    "        layers.append(layer)\n",
    "\n",
    "        loss = 0\n",
    "        total_layer_error = 0.0\n",
    "        correct_count = 0\n",
    "        incorrect_count = 0\n",
    "\n",
    "        for ix, next_word_index in enumerate(sent):\n",
    "\n",
    "            layer = {}\n",
    "\n",
    "            # compute prediction for the next word after this partial sequenc up to the target index\n",
    "            layer['prediction'] = softmax(np.dot(layers[-1]['hidden'], self.W1))\n",
    "\n",
    "            # compute the prediction error\n",
    "            layer['target_index'] =  next_word_index\n",
    "            layer['error'] = layer['prediction'] - self.onehots[layer['target_index']:layer['target_index']+1]\n",
    "            \n",
    "            loss += -np.log(layer['prediction'][0,next_word_index]) \n",
    "            total_layer_error += np.sum(layer['error']*layer['error'])\n",
    "            correct_count += int(np.argmax(layer['prediction'][0]) == np.argmax(self.onehots[layer['target_index']]))\n",
    "            incorrect_count += int(np.argmax(layer['prediction'][0]) != np.argmax(self.onehots[layer['target_index']]))\n",
    "\n",
    "            # mutliply hidden layer input with recurrance matrix and add the next word vector to generate the input for the next hidden layer\n",
    "            # (for output layer, we don't need to do this)\n",
    "            if(ix < (len(sent)-1)):\n",
    "                layer['hidden'] = np.dot(layers[-1]['hidden'], self.rec) + self.W0[layer['target_index']]\n",
    "\n",
    "            layers.append(layer)\n",
    "\n",
    "        return layers, loss, total_layer_error, correct_count, incorrect_count  \n",
    "\n",
    "    # backpropagation part\n",
    "    def backward(self, layers):\n",
    "\n",
    "        # starting at the output layer, backpropagate the gradients all the way up to the first hidden layer\n",
    "        for ix in reversed(range(len(layers))): \n",
    "\n",
    "            layer = layers[ix]\n",
    "            \n",
    "            if(ix == 0):\n",
    "                next_layer = layers[ix+1] \n",
    "\n",
    "                # dE_dL\n",
    "                layer['dE_dL'] = next_layer['dE_dL']\n",
    "                #layer['dE_dL'] = np.dot(next_layer['dE_dL'], self.rec.T) \n",
    "\n",
    "            else:\n",
    "\n",
    "                # dE_dP\n",
    "                layer['dE_dP'] = 2 * layer['error']    \n",
    "\n",
    "                prev_layer = layers[ix-1] \n",
    "\n",
    "                # dE_dW1\n",
    "                layer['dE_dW1'] = np.dot(prev_layer['hidden'].T, layer['dE_dP']) \n",
    "\n",
    "\n",
    "                # if this is the output layer, we don't have any gradients backpropagated from any layers in front\n",
    "                if(ix == (len(layers)-1)):\n",
    "\n",
    "                    # dE_dL\n",
    "                    layer['dE_dL'] = np.dot(layer['dE_dP'] , self.W1.T)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    next_layer = layers[ix+1] \n",
    "                \n",
    "                    # dE_dW0\n",
    "                    layer['dE_dW0'] = next_layer['dE_dL']\n",
    "\n",
    "                    # dE_dR\n",
    "                    layer['dE_dR'] = np.dot(prev_layer['hidden'].T, next_layer['dE_dL']) \n",
    "\n",
    "                    # dE_dL        \n",
    "                    layer['dE_dL'] = np.dot(next_layer['dE_dL'], self.rec.T) + np.dot(layer['dE_dP'], self.W1.T)   \n",
    "\n",
    "\n",
    "    # optimization of weights matrices and recurrent matrix\n",
    "    def optimize(self, layers, sent, alpha):\n",
    "\n",
    "        # update the empty sentence embedding vector\n",
    "        self.empty_sent -= (alpha/float(len(sent))) *  layers[0]['dE_dL']\n",
    "\n",
    "        # apply gradient descent update using the gradients accumulated at each layer\n",
    "        for ix in range(1,len(layers)): \n",
    "            layer = layers[ix]\n",
    "\n",
    "            self.W1 -= (alpha/float(len(sent))) * layer['dE_dW1'] \n",
    "            \n",
    "            if(ix < (len(layers)-1)):\n",
    "                # only need to update the weights associated with the target word for that hidden layer\n",
    "                self.W0[layer['target_index']:layer['target_index']+1] -= (alpha/float(len(sent))) * layer['dE_dW0'] \n",
    "                \n",
    "                self.rec -= (alpha/float(len(sent))) * layer['dE_dR']\n",
    "\n",
    "                \n",
    "    def train(self, niters, alpha):\n",
    "\n",
    "        print(\"Recurrent neural net training in progress...\")\n",
    "\n",
    "        for iter in range(niters):\n",
    "            total_error = 0.0\n",
    "            total_correct_count = 0\n",
    "            total_incorrect_count = 0\n",
    "            for i, sentence in enumerate(tokens):\n",
    "                # convert sentence to list of word indices\n",
    "                indices = words_to_indices(sentence)\n",
    "                \n",
    "                # forward propagation\n",
    "                layers, loss, error, correct_count, incorrect_count = self.forward(indices)\n",
    "                total_error += error\n",
    "                total_correct_count += correct_count\n",
    "                total_incorrect_count += incorrect_count\n",
    "\n",
    "                # backward propagation\n",
    "                self.backward(layers)\n",
    "\n",
    "                # weights optimization\n",
    "                self.optimize(layers, sentence, alpha)\n",
    "            \n",
    "            print(f\"Iteration# {iter}, Perplexity: {np.exp(loss/len(sentence))}, Total_error: {total_error}, Training accuracy: {float(total_correct_count)/float(total_correct_count+total_incorrect_count)}\")\n",
    "\n",
    "        print(\"Training completed!\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e413eaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recurrent neural net training in progress...\n",
      "Iteration# 0, Perplexity: 15.916009683054428, Total_error: 4529.980719097434, Training accuracy: 0.14264859228362878\n",
      "Iteration# 1, Perplexity: 8.635163135772004, Total_error: 4303.600508859641, Training accuracy: 0.16266944734098018\n",
      "Iteration# 2, Perplexity: 5.877623888576686, Total_error: 3652.1160804127394, Training accuracy: 0.32554744525547447\n",
      "Iteration# 3, Perplexity: 4.864048807046154, Total_error: 3182.8297049149205, Training accuracy: 0.462148070907195\n",
      "Iteration# 4, Perplexity: 4.191721502216954, Total_error: 2924.930553659827, Training accuracy: 0.5147028154327424\n",
      "Iteration# 5, Perplexity: 3.675988493170605, Total_error: 2756.108877547899, Training accuracy: 0.5199165797705944\n",
      "Iteration# 6, Perplexity: 3.3782019410451656, Total_error: 2653.8362771585216, Training accuracy: 0.5224191866527633\n",
      "Iteration# 7, Perplexity: 3.2041197077738026, Total_error: 2614.408038993802, Training accuracy: 0.5220020855057351\n",
      "Iteration# 8, Perplexity: 3.092248949636307, Total_error: 2594.399227100898, Training accuracy: 0.521793534932221\n",
      "Iteration# 9, Perplexity: 3.018528701947258, Total_error: 2579.2996665751125, Training accuracy: 0.5240875912408759\n",
      "Iteration# 10, Perplexity: 2.9714404412569717, Total_error: 2565.594802799215, Training accuracy: 0.5247132429614182\n",
      "Iteration# 11, Perplexity: 2.942729868076385, Total_error: 2552.6547025521563, Training accuracy: 0.5228362877997914\n",
      "Iteration# 12, Perplexity: 2.925186779475505, Total_error: 2541.1185607068373, Training accuracy: 0.5224191866527633\n",
      "Iteration# 13, Perplexity: 2.9149917986639053, Total_error: 2531.7887253121608, Training accuracy: 0.5234619395203337\n",
      "Iteration# 14, Perplexity: 2.91031416726057, Total_error: 2524.8090654454118, Training accuracy: 0.52429614181439\n",
      "Iteration# 15, Perplexity: 2.908903744714666, Total_error: 2519.792782073536, Training accuracy: 0.52429614181439\n",
      "Iteration# 16, Perplexity: 2.908742059984477, Total_error: 2516.226538645045, Training accuracy: 0.5240875912408759\n",
      "Iteration# 17, Perplexity: 2.908585567614571, Total_error: 2513.6630264788632, Training accuracy: 0.5236704900938478\n",
      "Iteration# 18, Perplexity: 2.9078753626519513, Total_error: 2511.7690300198633, Training accuracy: 0.5249217935349322\n",
      "Iteration# 19, Perplexity: 2.90650823497765, Total_error: 2510.315920380586, Training accuracy: 0.5249217935349322\n",
      "Iteration# 20, Perplexity: 2.9046193279748036, Total_error: 2509.1547054103758, Training accuracy: 0.5255474452554745\n",
      "Iteration# 21, Perplexity: 2.9024221027343207, Total_error: 2508.191307219279, Training accuracy: 0.5253388946819604\n",
      "Iteration# 22, Perplexity: 2.90012092721371, Total_error: 2507.3669593983645, Training accuracy: 0.5247132429614182\n",
      "Iteration# 23, Perplexity: 2.897878067775799, Total_error: 2506.6446020910143, Training accuracy: 0.5247132429614182\n",
      "Iteration# 24, Perplexity: 2.8958087401283232, Total_error: 2506.000233233138, Training accuracy: 0.5251303441084463\n",
      "Iteration# 25, Perplexity: 2.893986719046824, Total_error: 2505.417674706245, Training accuracy: 0.5255474452554745\n",
      "Iteration# 26, Perplexity: 2.8924526040769365, Total_error: 2504.885479110881, Training accuracy: 0.5274244004171011\n",
      "Iteration# 27, Perplexity: 2.891222103767991, Total_error: 2504.395126366809, Training accuracy: 0.527215849843587\n",
      "Iteration# 28, Perplexity: 2.890293575122559, Total_error: 2503.9399866204803, Training accuracy: 0.527007299270073\n",
      "Iteration# 29, Perplexity: 2.8896544331179563, Total_error: 2503.514730736317, Training accuracy: 0.527215849843587\n",
      "Iteration# 30, Perplexity: 2.8892861341146108, Total_error: 2503.1149927859324, Training accuracy: 0.5280500521376433\n",
      "Iteration# 31, Perplexity: 2.8891676358094736, Total_error: 2502.7371667719085, Training accuracy: 0.5276329509906152\n",
      "Iteration# 32, Perplexity: 2.8892774904867333, Total_error: 2502.3782708994777, Training accuracy: 0.5276329509906152\n",
      "Iteration# 33, Perplexity: 2.889594898148407, Total_error: 2502.0358451325887, Training accuracy: 0.5265901981230449\n",
      "Iteration# 34, Perplexity: 2.890100082444428, Total_error: 2501.707866523033, Training accuracy: 0.527007299270073\n",
      "Iteration# 35, Perplexity: 2.8907742931087514, Total_error: 2501.3926763056566, Training accuracy: 0.527007299270073\n",
      "Iteration# 36, Perplexity: 2.8915996453533475, Total_error: 2501.0889167788464, Training accuracy: 0.5265901981230449\n",
      "Iteration# 37, Perplexity: 2.8925589219713497, Total_error: 2500.79547724445, Training accuracy: 0.5261730969760167\n",
      "Iteration# 38, Perplexity: 2.8936354037459884, Total_error: 2500.511448396575, Training accuracy: 0.5263816475495308\n",
      "Iteration# 39, Perplexity: 2.894812757127894, Total_error: 2500.2360843545985, Training accuracy: 0.5263816475495308\n",
      "Iteration# 40, Perplexity: 2.8960749878805334, Total_error: 2499.9687713701633, Training accuracy: 0.526798748696559\n",
      "Iteration# 41, Perplexity: 2.897406458546136, Total_error: 2499.709002189534, Training accuracy: 0.527215849843587\n",
      "Iteration# 42, Perplexity: 2.898791961518359, Total_error: 2499.4563551078722, Training accuracy: 0.5276329509906152\n",
      "Iteration# 43, Perplexity: 2.9002168358215594, Total_error: 2499.210476868326, Training accuracy: 0.5278415015641293\n",
      "Iteration# 44, Perplexity: 2.901667113447064, Total_error: 2498.9710686983835, Training accuracy: 0.527007299270073\n",
      "Iteration# 45, Perplexity: 2.903129680046845, Total_error: 2498.7378749111713, Training accuracy: 0.526798748696559\n",
      "Iteration# 46, Perplexity: 2.904592434912057, Total_error: 2498.5106736169614, Training accuracy: 0.527007299270073\n",
      "Iteration# 47, Perplexity: 2.9060444364140183, Total_error: 2498.289269184234, Training accuracy: 0.5282586027111574\n",
      "Iteration# 48, Perplexity: 2.9074760212967683, Total_error: 2498.073486160963, Training accuracy: 0.5284671532846715\n",
      "Iteration# 49, Perplexity: 2.9088788891224477, Total_error: 2497.863164418911, Training accuracy: 0.5286757038581856\n",
      "Iteration# 50, Perplexity: 2.910246146473067, Total_error: 2497.6581553216215, Training accuracy: 0.5286757038581856\n",
      "Iteration# 51, Perplexity: 2.9115723088831515, Total_error: 2497.4583187445523, Training accuracy: 0.5282586027111574\n",
      "Iteration# 52, Perplexity: 2.9128532616279745, Total_error: 2497.263520798138, Training accuracy: 0.5278415015641293\n",
      "Iteration# 53, Perplexity: 2.9140861831868308, Total_error: 2497.0736321229656, Training accuracy: 0.5282586027111574\n",
      "Iteration# 54, Perplexity: 2.9152694372766184, Total_error: 2496.888526643744, Training accuracy: 0.5284671532846715\n",
      "Iteration# 55, Perplexity: 2.9164024407236635, Total_error: 2496.70808068496, Training accuracy: 0.5293013555787278\n",
      "Iteration# 56, Perplexity: 2.9174855151020234, Total_error: 2496.532172366538, Training accuracy: 0.5299270072992701\n",
      "Iteration# 57, Perplexity: 2.9185197300726182, Total_error: 2496.360681212486, Training accuracy: 0.5301355578727841\n",
      "Iteration# 58, Perplexity: 2.919506745815994, Total_error: 2496.1934879185173, Training accuracy: 0.5303441084462982\n",
      "Iteration# 59, Perplexity: 2.9204486610012586, Total_error: 2496.030474235911, Training accuracy: 0.5303441084462982\n",
      "Iteration# 60, Perplexity: 2.921347871521183, Total_error: 2495.8715229386253, Training accuracy: 0.5305526590198123\n",
      "Iteration# 61, Perplexity: 2.9222069438912768, Total_error: 2495.716517848094, Training accuracy: 0.5303441084462982\n",
      "Iteration# 62, Perplexity: 2.923028505879809, Total_error: 2495.565343896471, Training accuracy: 0.5307612095933264\n",
      "Iteration# 63, Perplexity: 2.923815155700774, Total_error: 2495.417887214041, Training accuracy: 0.5307612095933264\n",
      "Iteration# 64, Perplexity: 2.9245693900271053, Total_error: 2495.274035229942, Training accuracy: 0.5311783107403545\n",
      "Iteration# 65, Perplexity: 2.925293550203156, Total_error: 2495.133676778913, Training accuracy: 0.5311783107403545\n",
      "Iteration# 66, Perplexity: 2.925989785364149, Total_error: 2494.9967022091046, Training accuracy: 0.5305526590198123\n",
      "Iteration# 67, Perplexity: 2.9266600306995367, Total_error: 2494.8630034883377, Training accuracy: 0.5301355578727841\n",
      "Iteration# 68, Perplexity: 2.9273059988061303, Total_error: 2494.7324743077766, Training accuracy: 0.5293013555787278\n",
      "Iteration# 69, Perplexity: 2.927929181938342, Total_error: 2494.6050101833735, Training accuracy: 0.5290928050052137\n",
      "Iteration# 70, Perplexity: 2.9285308629477003, Total_error: 2494.480508556407, Training accuracy: 0.5301355578727841\n",
      "Iteration# 71, Perplexity: 2.929112132781946, Total_error: 2494.358868894493, Training accuracy: 0.5305526590198123\n",
      "Iteration# 72, Perplexity: 2.9296739125608373, Total_error: 2494.2399927944484, Training accuracy: 0.5299270072992701\n",
      "Iteration# 73, Perplexity: 2.930216978438484, Total_error: 2494.1237840876242, Training accuracy: 0.5301355578727841\n",
      "Iteration# 74, Perplexity: 2.930741987683726, Total_error: 2494.0101489474805, Training accuracy: 0.5301355578727841\n",
      "Iteration# 75, Perplexity: 2.931249504646815, Total_error: 2493.898995997938, Training accuracy: 0.5301355578727841\n",
      "Iteration# 76, Perplexity: 2.931740025522215, Total_error: 2493.790236419973, Training accuracy: 0.5303441084462982\n",
      "Iteration# 77, Perplexity: 2.932214001055431, Total_error: 2493.6837840530743, Training accuracy: 0.5303441084462982\n",
      "Iteration# 78, Perplexity: 2.93267185657068, Total_error: 2493.5795554875094, Training accuracy: 0.5309697601668405\n",
      "Iteration# 79, Perplexity: 2.933114008909941, Total_error: 2493.477470143141, Training accuracy: 0.5307612095933264\n",
      "Iteration# 80, Perplexity: 2.933540880068344, Total_error: 2493.377450330874, Training accuracy: 0.5311783107403545\n",
      "Iteration# 81, Perplexity: 2.9339529074833472, Total_error: 2493.2794212933904, Training accuracy: 0.5311783107403545\n",
      "Iteration# 82, Perplexity: 2.934350551080849, Total_error: 2493.1833112227173, Training accuracy: 0.5313868613138686\n",
      "Iteration# 83, Perplexity: 2.934734297300598, Total_error: 2493.089051253461, Training accuracy: 0.5309697601668405\n",
      "Iteration# 84, Perplexity: 2.9351046604138893, Total_error: 2492.996575431673, Training accuracy: 0.5309697601668405\n",
      "Iteration# 85, Perplexity: 2.935462181509328, Total_error: 2492.905820660522, Training accuracy: 0.5311783107403545\n",
      "Iteration# 86, Perplexity: 2.9358074255587074, Total_error: 2492.816726624952, Training accuracy: 0.5315954118873827\n",
      "Iteration# 87, Perplexity: 2.9361409769882347, Total_error: 2492.72923569837, Training accuracy: 0.5313868613138686\n",
      "Iteration# 88, Perplexity: 2.936463434171932, Total_error: 2492.6432928349086, Training accuracy: 0.5311783107403545\n",
      "Iteration# 89, Perplexity: 2.936775403239836, Total_error: 2492.5588454511217, Training accuracy: 0.5313868613138686\n",
      "Iteration# 90, Perplexity: 2.9370774915564573, Total_error: 2492.4758433009606, Training accuracy: 0.5311783107403545\n",
      "Iteration# 91, Perplexity: 2.937370301179157, Total_error: 2492.3942383476838, Training accuracy: 0.5311783107403545\n",
      "Iteration# 92, Perplexity: 2.937654422555445, Total_error: 2492.3139846360496, Training accuracy: 0.5311783107403545\n",
      "Iteration# 93, Perplexity: 2.9379304286661574, Total_error: 2492.2350381676088, Training accuracy: 0.5309697601668405\n",
      "Iteration# 94, Perplexity: 2.938198869770096, Total_error: 2492.1573567812866, Training accuracy: 0.5309697601668405\n",
      "Iteration# 95, Perplexity: 2.9384602688581243, Total_error: 2492.080900041126, Training accuracy: 0.5311783107403545\n",
      "Iteration# 96, Perplexity: 2.9387151178811566, Total_error: 2492.0056291322157, Training accuracy: 0.5307612095933264\n",
      "Iteration# 97, Perplexity: 2.938963874779361, Total_error: 2491.9315067656157, Training accuracy: 0.5309697601668405\n",
      "Iteration# 98, Perplexity: 2.939206961307897, Total_error: 2491.8584970923407, Training accuracy: 0.5309697601668405\n",
      "Iteration# 99, Perplexity: 2.9394447616297263, Total_error: 2491.7865656265108, Training accuracy: 0.5307612095933264\n",
      "Iteration# 100, Perplexity: 2.939677621626157, Total_error: 2491.715679177027, Training accuracy: 0.5309697601668405\n",
      "Iteration# 101, Perplexity: 2.939905848861927, Total_error: 2491.645805787433, Training accuracy: 0.5309697601668405\n",
      "Iteration# 102, Perplexity: 2.940129713132574, Total_error: 2491.57691468294, Training accuracy: 0.5309697601668405\n",
      "Iteration# 103, Perplexity: 2.940349447516454, Total_error: 2491.5089762240505, Training accuracy: 0.5309697601668405\n",
      "Iteration# 104, Perplexity: 2.940565249852459, Total_error: 2491.4419618657407, Training accuracy: 0.5309697601668405\n",
      "Iteration# 105, Perplexity: 2.9407772845654248, Total_error: 2491.3758441214454, Training accuracy: 0.5311783107403545\n",
      "Iteration# 106, Perplexity: 2.9409856847646187, Total_error: 2491.3105965310747, Training accuracy: 0.5313868613138686\n",
      "Iteration# 107, Perplexity: 2.9411905545453707, Total_error: 2491.2461936321724, Training accuracy: 0.5318039624608968\n",
      "Iteration# 108, Perplexity: 2.941391971430058, Total_error: 2491.1826109337153, Training accuracy: 0.5313868613138686\n",
      "Iteration# 109, Perplexity: 2.9415899888909496, Total_error: 2491.1198248917995, Training accuracy: 0.5315954118873827\n",
      "Iteration# 110, Perplexity: 2.9417846389043323, Total_error: 2491.0578128868074, Training accuracy: 0.5318039624608968\n",
      "Iteration# 111, Perplexity: 2.941975934492513, Total_error: 2490.9965532015103, Training accuracy: 0.5322210636079249\n",
      "Iteration# 112, Perplexity: 2.9421638722165127, Total_error: 2490.936024999864, Training accuracy: 0.5320125130344109\n",
      "Iteration# 113, Perplexity: 2.94234843458905, Total_error: 2490.876208306067, Training accuracy: 0.5320125130344109\n",
      "Iteration# 114, Perplexity: 2.94252959238328, Total_error: 2490.817083983743, Training accuracy: 0.5320125130344109\n",
      "Iteration# 115, Perplexity: 2.9427073068179617, Total_error: 2490.758633715012, Training accuracy: 0.5320125130344109\n",
      "Iteration# 116, Perplexity: 2.9428815316049697, Total_error: 2490.7008399793676, Training accuracy: 0.5320125130344109\n",
      "Iteration# 117, Perplexity: 2.9430522148491334, Total_error: 2490.643686032204, Training accuracy: 0.5315954118873827\n",
      "Iteration# 118, Perplexity: 2.9432193007942185, Total_error: 2490.5871558830167, Training accuracy: 0.5307612095933264\n",
      "Iteration# 119, Perplexity: 2.9433827314123246, Total_error: 2490.5312342731686, Training accuracy: 0.5305526590198123\n",
      "Iteration# 120, Perplexity: 2.9435424478363803, Total_error: 2490.4759066532665, Training accuracy: 0.5309697601668405\n",
      "Iteration# 121, Perplexity: 2.9436983916381254, Total_error: 2490.4211591602016, Training accuracy: 0.5311783107403545\n",
      "Iteration# 122, Perplexity: 2.943850505955397, Total_error: 2490.3669785937395, Training accuracy: 0.5311783107403545\n",
      "Iteration# 123, Perplexity: 2.943998736474591, Total_error: 2490.313352392883, Training accuracy: 0.5313868613138686\n",
      "Iteration# 124, Perplexity: 2.944143032274823, Total_error: 2490.260268611971, Training accuracy: 0.5313868613138686\n",
      "Iteration# 125, Perplexity: 2.9442833465415132, Total_error: 2490.2077158964266, Training accuracy: 0.5313868613138686\n",
      "Iteration# 126, Perplexity: 2.9444196371577447, Total_error: 2490.155683458595, Training accuracy: 0.5313868613138686\n",
      "Iteration# 127, Perplexity: 2.9445518671819904, Total_error: 2490.104161053281, Training accuracy: 0.5318039624608968\n",
      "Iteration# 128, Perplexity: 2.9446800052212647, Total_error: 2490.0531389533676, Training accuracy: 0.5315954118873827\n",
      "Iteration# 129, Perplexity: 2.9448040257085926, Total_error: 2490.002607925427, Training accuracy: 0.5315954118873827\n",
      "Iteration# 130, Perplexity: 2.944923909093984, Total_error: 2489.952559205481, Training accuracy: 0.5318039624608968\n",
      "Iteration# 131, Perplexity: 2.9450396419575244, Total_error: 2489.902984474845, Training accuracy: 0.5313868613138686\n",
      "Iteration# 132, Perplexity: 2.945151217053374, Total_error: 2489.8538758362693, Training accuracy: 0.5313868613138686\n",
      "Iteration# 133, Perplexity: 2.945258633292865, Total_error: 2489.8052257902964, Training accuracy: 0.5313868613138686\n",
      "Iteration# 134, Perplexity: 2.9453618956748056, Total_error: 2489.7570272120097, Training accuracy: 0.5309697601668405\n",
      "Iteration# 135, Perplexity: 2.945461015170306, Total_error: 2489.709273328075, Training accuracy: 0.5311783107403545\n",
      "Iteration# 136, Perplexity: 2.9455560085695347, Total_error: 2489.66195769432, Training accuracy: 0.5313868613138686\n",
      "Iteration# 137, Perplexity: 2.945646898296968, Total_error: 2489.6150741737165, Training accuracy: 0.5313868613138686\n",
      "Iteration# 138, Perplexity: 2.9457337122013327, Total_error: 2489.56861691494, Training accuracy: 0.5313868613138686\n",
      "Iteration# 139, Perplexity: 2.945816483326107, Total_error: 2489.52258033147, Training accuracy: 0.5313868613138686\n",
      "Iteration# 140, Perplexity: 2.9458952496657136, Total_error: 2489.4769590812994, Training accuracy: 0.5313868613138686\n",
      "Iteration# 141, Perplexity: 2.9459700539124904, Total_error: 2489.4317480473, Training accuracy: 0.5313868613138686\n",
      "Iteration# 142, Perplexity: 2.946040943198437, Total_error: 2489.3869423182646, Training accuracy: 0.5313868613138686\n",
      "Iteration# 143, Perplexity: 2.9461079688358214, Total_error: 2489.3425371706217, Training accuracy: 0.5315954118873827\n",
      "Iteration# 144, Perplexity: 2.9461711860600976, Total_error: 2489.2985280509706, Training accuracy: 0.5313868613138686\n",
      "Iteration# 145, Perplexity: 2.946230653777872, Total_error: 2489.2549105592484, Training accuracy: 0.5315954118873827\n",
      "Iteration# 146, Perplexity: 2.9462864343227846, Total_error: 2489.211680432796, Training accuracy: 0.5320125130344109\n",
      "Iteration# 147, Perplexity: 2.9463385932210135, Total_error: 2489.1688335311555, Training accuracy: 0.5322210636079249\n",
      "Iteration# 148, Perplexity: 2.9463871989685253, Total_error: 2489.126365821679, Training accuracy: 0.5320125130344109\n",
      "Iteration# 149, Perplexity: 2.946432322821094, Total_error: 2489.084273365958, Training accuracy: 0.5322210636079249\n",
      "Iteration# 150, Perplexity: 2.946474038598162, Total_error: 2489.042552307131, Training accuracy: 0.532429614181439\n",
      "Iteration# 151, Perplexity: 2.946512422501232, Total_error: 2489.001198857925, Training accuracy: 0.5326381647549531\n",
      "Iteration# 152, Perplexity: 2.9465475529468588, Total_error: 2488.960209289622, Training accuracy: 0.5322210636079249\n",
      "Iteration# 153, Perplexity: 2.9465795104145727, Total_error: 2488.9195799217905, Training accuracy: 0.5322210636079249\n",
      "Iteration# 154, Perplexity: 2.9466083773091243, Total_error: 2488.879307112836, Training accuracy: 0.5320125130344109\n",
      "Iteration# 155, Perplexity: 2.9466342378368147, Total_error: 2488.8393872514184, Training accuracy: 0.5320125130344109\n",
      "Iteration# 156, Perplexity: 2.946657177894986, Total_error: 2488.7998167485994, Training accuracy: 0.5320125130344109\n",
      "Iteration# 157, Perplexity: 2.9466772849738763, Total_error: 2488.7605920308074, Training accuracy: 0.5320125130344109\n",
      "Iteration# 158, Perplexity: 2.946694648069746, Total_error: 2488.7217095335627, Training accuracy: 0.5322210636079249\n",
      "Iteration# 159, Perplexity: 2.9467093576080035, Total_error: 2488.683165695913, Training accuracy: 0.5326381647549531\n",
      "Iteration# 160, Perplexity: 2.946721505375113, Total_error: 2488.6449569556607, Training accuracy: 0.532429614181439\n",
      "Iteration# 161, Perplexity: 2.9467311844577924, Total_error: 2488.607079745167, Training accuracy: 0.5328467153284672\n",
      "Iteration# 162, Perplexity: 2.9467384891882493, Total_error: 2488.5695304879273, Training accuracy: 0.5330552659019813\n",
      "Iteration# 163, Perplexity: 2.9467435150937917, Total_error: 2488.5323055957515, Training accuracy: 0.5326381647549531\n",
      "Iteration# 164, Perplexity: 2.946746358849473, Total_error: 2488.4954014664945, Training accuracy: 0.5328467153284672\n",
      "Iteration# 165, Perplexity: 2.9467471182324294, Total_error: 2488.4588144824347, Training accuracy: 0.5328467153284672\n",
      "Iteration# 166, Perplexity: 2.9467458920763216, Total_error: 2488.422541009155, Training accuracy: 0.5328467153284672\n",
      "Iteration# 167, Perplexity: 2.946742780224812, Total_error: 2488.3865773949374, Training accuracy: 0.5330552659019813\n",
      "Iteration# 168, Perplexity: 2.94673788348262, Total_error: 2488.3509199705927, Training accuracy: 0.5330552659019813\n",
      "Iteration# 169, Perplexity: 2.9467313035632685, Total_error: 2488.315565049815, Training accuracy: 0.5326381647549531\n",
      "Iteration# 170, Perplexity: 2.9467231430322163, Total_error: 2488.2805089298495, Training accuracy: 0.5326381647549531\n",
      "Iteration# 171, Perplexity: 2.9467135052448175, Total_error: 2488.245747892545, Training accuracy: 0.5326381647549531\n",
      "Iteration# 172, Perplexity: 2.9467024942780373, Total_error: 2488.211278205803, Training accuracy: 0.532429614181439\n",
      "Iteration# 173, Perplexity: 2.946690214855435, Total_error: 2488.1770961252428, Training accuracy: 0.5326381647549531\n",
      "Iteration# 174, Perplexity: 2.9466767722649654, Total_error: 2488.143197896173, Training accuracy: 0.5326381647549531\n",
      "Iteration# 175, Perplexity: 2.9466622722690645, Total_error: 2488.1095797558046, Training accuracy: 0.5326381647549531\n",
      "Iteration# 176, Perplexity: 2.9466468210069285, Total_error: 2488.0762379356347, Training accuracy: 0.532429614181439\n",
      "Iteration# 177, Perplexity: 2.946630524888998, Total_error: 2488.0431686640254, Training accuracy: 0.532429614181439\n",
      "Iteration# 178, Perplexity: 2.9466134904833043, Total_error: 2488.0103681688997, Training accuracy: 0.5326381647549531\n",
      "Iteration# 179, Perplexity: 2.946595824394324, Total_error: 2487.9778326806104, Training accuracy: 0.5326381647549531\n",
      "Iteration# 180, Perplexity: 2.9465776331342157, Total_error: 2487.9455584347893, Training accuracy: 0.5326381647549531\n",
      "Iteration# 181, Perplexity: 2.9465590229870964, Total_error: 2487.9135416753606, Training accuracy: 0.5328467153284672\n",
      "Iteration# 182, Perplexity: 2.946540099866659, Total_error: 2487.881778657515, Training accuracy: 0.5328467153284672\n",
      "Iteration# 183, Perplexity: 2.946520969167772, Total_error: 2487.8502656507458, Training accuracy: 0.5328467153284672\n",
      "Iteration# 184, Perplexity: 2.9465017356127543, Total_error: 2487.818998941841, Training accuracy: 0.5328467153284672\n",
      "Iteration# 185, Perplexity: 2.94648250309292, Total_error: 2487.787974837895, Training accuracy: 0.532429614181439\n",
      "Iteration# 186, Perplexity: 2.9464633745061777, Total_error: 2487.757189669199, Training accuracy: 0.532429614181439\n",
      "Iteration# 187, Perplexity: 2.9464444515916566, Total_error: 2487.7266397921308, Training accuracy: 0.5322210636079249\n",
      "Iteration# 188, Perplexity: 2.9464258347619405, Total_error: 2487.6963215919573, Training accuracy: 0.5322210636079249\n",
      "Iteration# 189, Perplexity: 2.946407622933982, Total_error: 2487.66623148551, Training accuracy: 0.5322210636079249\n",
      "Iteration# 190, Perplexity: 2.9463899133594715, Total_error: 2487.6363659238255, Training accuracy: 0.532429614181439\n",
      "Iteration# 191, Perplexity: 2.9463728014556403, Total_error: 2487.606721394575, Training accuracy: 0.5322210636079249\n",
      "Iteration# 192, Perplexity: 2.9463563806372717, Total_error: 2487.577294424474, Training accuracy: 0.5322210636079249\n",
      "Iteration# 193, Perplexity: 2.9463407421508117, Total_error: 2487.5480815815095, Training accuracy: 0.532429614181439\n",
      "Iteration# 194, Perplexity: 2.946325974911619, Total_error: 2487.5190794770097, Training accuracy: 0.5322210636079249\n",
      "Iteration# 195, Perplexity: 2.9463121653449305, Total_error: 2487.4902847676444, Training accuracy: 0.532429614181439\n",
      "Iteration# 196, Perplexity: 2.9462993972315066, Total_error: 2487.461694157234, Training accuracy: 0.5322210636079249\n",
      "Iteration# 197, Perplexity: 2.946287751558727, Total_error: 2487.433304398404, Training accuracy: 0.532429614181439\n",
      "Iteration# 198, Perplexity: 2.94627730637785, Total_error: 2487.4051122941587, Training accuracy: 0.5326381647549531\n",
      "Iteration# 199, Perplexity: 2.9462681366681487, Total_error: 2487.377114699232, Training accuracy: 0.5326381647549531\n",
      "Iteration# 200, Perplexity: 2.946260314208544, Total_error: 2487.3493085213718, Training accuracy: 0.532429614181439\n",
      "Iteration# 201, Perplexity: 2.9462539074575274, Total_error: 2487.3216907224128, Training accuracy: 0.5328467153284672\n",
      "Iteration# 202, Perplexity: 2.946248981441632, Total_error: 2487.294258319281, Training accuracy: 0.5328467153284672\n",
      "Iteration# 203, Perplexity: 2.9462455976533035, Total_error: 2487.26700838483, Training accuracy: 0.5330552659019813\n",
      "Iteration# 204, Perplexity: 2.946243813958491, Total_error: 2487.2399380485535, Training accuracy: 0.5330552659019813\n",
      "Iteration# 205, Perplexity: 2.9462436845143065, Total_error: 2487.213044497159, Training accuracy: 0.5330552659019813\n",
      "Iteration# 206, Perplexity: 2.9462452596973225, Total_error: 2487.1863249751145, Training accuracy: 0.5328467153284672\n",
      "Iteration# 207, Perplexity: 2.946248586042572, Total_error: 2487.159776784933, Training accuracy: 0.5328467153284672\n",
      "Iteration# 208, Perplexity: 2.9462537061938954, Total_error: 2487.133397287514, Training accuracy: 0.5328467153284672\n",
      "Iteration# 209, Perplexity: 2.946260658865346, Total_error: 2487.107183902281, Training accuracy: 0.5328467153284672\n",
      "Iteration# 210, Perplexity: 2.9462694788141914, Total_error: 2487.0811341072636, Training accuracy: 0.5328467153284672\n",
      "Iteration# 211, Perplexity: 2.946280196825522, Total_error: 2487.0552454391054, Training accuracy: 0.5328467153284672\n",
      "Iteration# 212, Perplexity: 2.946292839708298, Total_error: 2487.0295154929913, Training accuracy: 0.5328467153284672\n",
      "Iteration# 213, Perplexity: 2.9463074303030887, Total_error: 2487.003941922471, Training accuracy: 0.5328467153284672\n",
      "Iteration# 214, Perplexity: 2.946323987501224, Total_error: 2486.9785224392704, Training accuracy: 0.5328467153284672\n",
      "Iteration# 215, Perplexity: 2.946342526275236, Total_error: 2486.9532548129982, Training accuracy: 0.5328467153284672\n",
      "Iteration# 216, Perplexity: 2.9463630577204656, Total_error: 2486.9281368708153, Training accuracy: 0.5322210636079249\n",
      "Iteration# 217, Perplexity: 2.946385589107409, Total_error: 2486.903166497037, Training accuracy: 0.5322210636079249\n",
      "Iteration# 218, Perplexity: 2.946410123944602, Total_error: 2486.878341632719, Training accuracy: 0.5322210636079249\n",
      "Iteration# 219, Perplexity: 2.946436662051614, Total_error: 2486.8536602751656, Training accuracy: 0.5322210636079249\n",
      "Iteration# 220, Perplexity: 2.9464651996415694, Total_error: 2486.8291204773795, Training accuracy: 0.5322210636079249\n",
      "Iteration# 221, Perplexity: 2.9464957294129124, Total_error: 2486.804720347548, Training accuracy: 0.5322210636079249\n",
      "Iteration# 222, Perplexity: 2.9465282406497066, Total_error: 2486.7804580484026, Training accuracy: 0.5322210636079249\n",
      "Iteration# 223, Perplexity: 2.9465627193298296, Total_error: 2486.7563317965755, Training accuracy: 0.5322210636079249\n",
      "Iteration# 224, Perplexity: 2.9465991482405736, Total_error: 2486.7323398619606, Training accuracy: 0.5322210636079249\n",
      "Iteration# 225, Perplexity: 2.9466375071008253, Total_error: 2486.708480566947, Training accuracy: 0.5322210636079249\n",
      "Iteration# 226, Perplexity: 2.946677772689142, Total_error: 2486.6847522857097, Training accuracy: 0.5322210636079249\n",
      "Iteration# 227, Perplexity: 2.9467199189770974, Total_error: 2486.661153443408, Training accuracy: 0.5322210636079249\n",
      "Iteration# 228, Perplexity: 2.9467639172669378, Total_error: 2486.637682515374, Training accuracy: 0.5322210636079249\n",
      "Iteration# 229, Perplexity: 2.9468097363329653, Total_error: 2486.614338026252, Training accuracy: 0.5322210636079249\n",
      "Iteration# 230, Perplexity: 2.9468573425658446, Total_error: 2486.5911185490913, Training accuracy: 0.5322210636079249\n",
      "Iteration# 231, Perplexity: 2.9469067001189173, Total_error: 2486.5680227044654, Training accuracy: 0.5320125130344109\n",
      "Iteration# 232, Perplexity: 2.946957771056031, Total_error: 2486.545049159443, Training accuracy: 0.5318039624608968\n",
      "Iteration# 233, Perplexity: 2.9470105154999753, Total_error: 2486.52219662667, Training accuracy: 0.5315954118873827\n",
      "Iteration# 234, Perplexity: 2.9470648917807885, Total_error: 2486.499463863254, Training accuracy: 0.5315954118873827\n",
      "Iteration# 235, Perplexity: 2.947120856583424, Total_error: 2486.47684966976, Training accuracy: 0.5318039624608968\n",
      "Iteration# 236, Perplexity: 2.947178365093989, Total_error: 2486.4543528890918, Training accuracy: 0.5318039624608968\n",
      "Iteration# 237, Perplexity: 2.9472373711440576, Total_error: 2486.431972405329, Training accuracy: 0.5318039624608968\n",
      "Iteration# 238, Perplexity: 2.9472978273523878, Total_error: 2486.409707142628, Training accuracy: 0.5318039624608968\n",
      "Iteration# 239, Perplexity: 2.9473596852636836, Total_error: 2486.387556063955, Training accuracy: 0.5318039624608968\n",
      "Iteration# 240, Perplexity: 2.947422895483849, Total_error: 2486.365518169907, Training accuracy: 0.5318039624608968\n",
      "Iteration# 241, Perplexity: 2.947487407811324, Total_error: 2486.3435924974674, Training accuracy: 0.5318039624608968\n",
      "Iteration# 242, Perplexity: 2.9475531713642504, Total_error: 2486.321778118725, Training accuracy: 0.5318039624608968\n",
      "Iteration# 243, Perplexity: 2.947620134703077, Total_error: 2486.3000741395767, Training accuracy: 0.5318039624608968\n",
      "Iteration# 244, Perplexity: 2.947688245948399, Total_error: 2486.278479698426, Training accuracy: 0.5318039624608968\n",
      "Iteration# 245, Perplexity: 2.9477574528938635, Total_error: 2486.2569939648765, Training accuracy: 0.5320125130344109\n",
      "Iteration# 246, Perplexity: 2.9478277031139304, Total_error: 2486.235616138361, Training accuracy: 0.5320125130344109\n",
      "Iteration# 247, Perplexity: 2.947898944066518, Total_error: 2486.2143454468473, Training accuracy: 0.5320125130344109\n",
      "Iteration# 248, Perplexity: 2.9479711231903383, Total_error: 2486.193181145442, Training accuracy: 0.5320125130344109\n",
      "Iteration# 249, Perplexity: 2.9480441879969845, Total_error: 2486.172122515072, Training accuracy: 0.5320125130344109\n",
      "Iteration# 250, Perplexity: 2.9481180861578373, Total_error: 2486.151168861121, Training accuracy: 0.5320125130344109\n",
      "Iteration# 251, Perplexity: 2.9481927655857985, Total_error: 2486.1303195120954, Training accuracy: 0.5320125130344109\n",
      "Iteration# 252, Perplexity: 2.9482681745119272, Total_error: 2486.109573818267, Training accuracy: 0.532429614181439\n",
      "Iteration# 253, Perplexity: 2.9483442615571422, Total_error: 2486.088931150396, Training accuracy: 0.532429614181439\n",
      "Iteration# 254, Perplexity: 2.948420975799105, Total_error: 2486.068390898352, Training accuracy: 0.5322210636079249\n",
      "Iteration# 255, Perplexity: 2.948498266834458, Total_error: 2486.0479524698735, Training accuracy: 0.532429614181439\n",
      "Iteration# 256, Perplexity: 2.948576084836556, Total_error: 2486.0276152892934, Training accuracy: 0.532429614181439\n",
      "Iteration# 257, Perplexity: 2.948654380608952, Total_error: 2486.0073787962488, Training accuracy: 0.532429614181439\n",
      "Iteration# 258, Perplexity: 2.9487331056346715, Total_error: 2485.987242444517, Training accuracy: 0.532429614181439\n",
      "Iteration# 259, Perplexity: 2.9488122121216933, Total_error: 2485.967205700751, Training accuracy: 0.532429614181439\n",
      "Iteration# 260, Perplexity: 2.948891653044632, Total_error: 2485.947268043382, Training accuracy: 0.532429614181439\n"
     ]
    }
   ],
   "source": [
    "# initialize a recurrent neural net and train it\n",
    "net = recurrent_net(10 , vocab)\n",
    "\n",
    "net.train(10000, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a602badc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'moved', 'to', 'the', 'bedroom']\n",
      "6\n",
      "Previous word: john, Predicted next word: went, Actual next word: moved\n",
      "Previous word: moved, Predicted next word: to, Actual next word: to\n",
      "Previous word: to, Predicted next word: the, Actual next word: the\n",
      "Previous word: the, Predicted next word: garden, Actual next word: bedroom\n"
     ]
    }
   ],
   "source": [
    "# predictions for a sentence\n",
    "sentence = tokens[13]\n",
    "print(sentence)\n",
    "indices = words_to_indices(sentence)\n",
    "layers, loss, error, correct_count, incorrect_count = net.forward(indices)\n",
    "\n",
    "print(len(layers))\n",
    "for i in range(2,len(layers)):\n",
    "    predicted_word = vocab[np.argmax(layers[i]['prediction'][0])]\n",
    "    previous_word = sentence[i-2]    \n",
    "    print(f\"Previous word: {previous_word}, Predicted next word: {predicted_word}, Actual next word: {sentence[i-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
