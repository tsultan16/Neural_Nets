{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "601fbf17",
   "metadata": {},
   "source": [
    "Creating sentence embeddings using transition matrices that takes into account, the ordering of the words in the sentence. \n",
    "\n",
    "In the following simple example, we will use a vocabulary of nine words and impleent the forward propagation part of a neural network that will predict the next word in a sentence (i.e sequence of words). A sentence embedding is created by multiplying each word by a transition matrix, and summing up the resulting vectors. The traisiotn matrix is initialized as an identity matrix of size equal to the hidden neurons (which is the number of columns in the wieght matrix). The traisition matrix is optimized vi gradient descent during training the neural net.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "679bb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    s = np.sum(temp, axis = 1, keepdims = True)\n",
    "    return  temp / s  \n",
    "\n",
    "vocab_size = 9\n",
    "hidden_neurons = 3\n",
    "\n",
    "# hidden layer weights intialized to zero\n",
    "W0 = np.zeros(shape=(vocab_size, hidden_neurons))\n",
    "\n",
    "# output layer weights initialed to random values\n",
    "W1 = np.random.randn(hidden_neurons, vocab_size) \n",
    "\n",
    "# words vectors for our vocabulary (i.e. rows of the hidden layer weights matrix corresponding to each word in the vocabulary)\n",
    "word_vecs = {}\n",
    "word_vecs['yankees'] = W0[0:1]\n",
    "word_vecs['bears'] = W0[1:2]\n",
    "word_vecs['braves'] = W0[2:3]\n",
    "word_vecs['red'] = W0[3:4]\n",
    "word_vecs['sox'] = W0[4:5]\n",
    "word_vecs['lose'] = W0[5:6]\n",
    "word_vecs['defeat'] = W0[6:7]\n",
    "word_vecs['beat'] = W0[7:8]\n",
    "word_vecs['tie'] = W0[8:9]\n",
    "\n",
    "# transition matrix initialized to identity \n",
    "tmat = np.eye(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16093a",
   "metadata": {},
   "source": [
    "**Forward Propagation**\n",
    "\n",
    "Given an input sequence of three words, the word_vector for the first word in the sequence is passed on by the first layer to the second layer where it is multiplied by the transition matrix and added to the next word vector in the sequence. The result is then passed on to the next layer where we repeat this process of multiplying the layer input by the traisition matrix and adding to the next word_vector. After the last word_vector in the sequence is added, we pass the result on to the output layer where it gets multiplied to the output layer weights and operated on by the softmax function to obtain the final prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1377ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = ['red', 'sox', 'defeat']\n",
    "layer_0 = word_vecs[input_sequence[0]]\n",
    "layer_1 = np.dot(layer_0, tmat) + word_vecs[input_sequence[1]]\n",
    "layer_2 = np.dot(layer_1, tmat) + word_vecs[input_sequence[2]]\n",
    "pred = softmax(np.dot(layer_2, W1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40cd60d",
   "metadata": {},
   "source": [
    "**Backpropagation**\n",
    "\n",
    "Now we will do the backpropagation and compute gradients of the word_vectors and transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6053e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the target word is 'yankees', whch is the first word in our vocab \n",
    "y = np.zeros(shape=(1,vocab_size))\n",
    "y[0,0] = 1\n",
    "\n",
    "# error\n",
    "E = (pred - y) * (pred - y)\n",
    "\n",
    "# dE/dP\n",
    "dP = 2 * (pred - y)\n",
    "W1_grad = np.dot(layer_2.T, dP)\n",
    "\n",
    "# dE_dL2\n",
    "dL2 = np.dot(dP, W1.T)\n",
    "\n",
    "# dE_d(defeat)\n",
    "d_defeat = dL2 * 1\n",
    "W_defeat_grad = d_defeat\n",
    "\n",
    "# dE_dL1\n",
    "dL1 = np.dot(dL2, tmat.T) \n",
    "tmat_grad_2 = np.dot(layer_1.T, dL2)\n",
    "\n",
    "# dE_d(sox)\n",
    "d_sox = dL1 * 1\n",
    "W_sox_grad = d_sox\n",
    "\n",
    "# dE_dL0\n",
    "dL0 = np.dot(dL1, tmat.T)\n",
    "tmat_grad_1 = np.dot(layer_0.T, dL1)\n",
    "W_red_grad = dL0 * 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faeefcd",
   "metadata": {},
   "source": [
    "**Optimization**\n",
    "\n",
    "Now update the weights and traision matrix via gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b7ed5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "W1 -= alpha * W1_grad\n",
    "word_vecs['defeat'] -= alpha * W_defeat_grad\n",
    "word_vecs['sox'] -= alpha * W_sox_grad\n",
    "word_vecs['red'] -= alpha * W_red_grad\n",
    "tmat -= alpha * (tmat_grad_2 + tmat_grad_1) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d58700f8",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f634b0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration# 0, Error: 4.2055422642825566e-05, prediction: 0.9951947824251023\n",
      "Iteration# 1, Error: 4.1405383410469566e-05, prediction: 0.9952323056064151\n",
      "Iteration# 2, Error: 4.076912216257302e-05, prediction: 0.9952693172827043\n",
      "Iteration# 3, Error: 4.0146270125834764e-05, prediction: 0.9953058272856068\n",
      "Iteration# 4, Error: 3.953647040234927e-05, prediction: 0.9953418452043689\n",
      "Iteration# 5, Error: 3.893937752397905e-05, prediction: 0.9953773803931074\n",
      "Iteration# 6, Error: 3.835465702575492e-05, prediction: 0.9954124419778183\n",
      "Iteration# 7, Error: 3.778198503744368e-05, prediction: 0.9954470388631365\n",
      "Iteration# 8, Error: 3.722104789235672e-05, prediction: 0.9954811797388643\n",
      "Iteration# 9, Error: 3.667154175263774e-05, prediction: 0.995514873086272\n",
      "Iteration# 10, Error: 3.613317225022991e-05, prediction: 0.9955481271841806\n",
      "Iteration# 11, Error: 3.5605654142761783e-05, prediction: 0.9955809501148405\n",
      "Iteration# 12, Error: 3.508871098373735e-05, prediction: 0.9956133497696024\n",
      "Iteration# 13, Error: 3.458207480624743e-05, prediction: 0.9956453338544042\n",
      "Iteration# 14, Error: 3.408548581969247e-05, prediction: 0.9956769098950673\n",
      "Iteration# 15, Error: 3.359869211884619e-05, prediction: 0.9957080852424168\n",
      "Iteration# 16, Error: 3.312144940472541e-05, prediction: 0.9957388670772316\n",
      "Iteration# 17, Error: 3.265352071670974e-05, prediction: 0.9957692624150304\n",
      "Iteration# 18, Error: 3.219467617546038e-05, prediction: 0.9957992781106954\n",
      "Iteration# 19, Error: 3.1744692736045535e-05, prediction: 0.9958289208629505\n",
      "Iteration# 20, Error: 3.1303353950932374e-05, prediction: 0.9958581972186873\n",
      "Iteration# 21, Error: 3.0870449742313975e-05, prediction: 0.995887113577154\n",
      "Iteration# 22, Error: 3.0445776183423298e-05, prediction: 0.9959156761940057\n",
      "Iteration# 23, Error: 3.002913528838413e-05, prediction: 0.995943891185227\n",
      "Iteration# 24, Error: 2.9620334810275524e-05, prediction: 0.9959717645309256\n",
      "Iteration# 25, Error: 2.92191880469996e-05, prediction: 0.9959993020790081\n",
      "Iteration# 26, Error: 2.882551365465739e-05, prediction: 0.9960265095487363\n",
      "Iteration# 27, Error: 2.843913546807934e-05, prediction: 0.9960533925341727\n",
      "Iteration# 28, Error: 2.8059882328230227e-05, prediction: 0.9960799565075154\n",
      "Iteration# 29, Error: 2.768758791613331e-05, prediction: 0.9961062068223335\n",
      "Iteration# 30, Error: 2.7322090593146466e-05, prediction: 0.996132148716694\n",
      "Iteration# 31, Error: 2.6963233247181842e-05, prediction: 0.9961577873161994\n",
      "Iteration# 32, Error: 2.661086314474209e-05, prediction: 0.9961831276369263\n",
      "Iteration# 33, Error: 2.6264831788450517e-05, prediction: 0.9962081745882756\n",
      "Iteration# 34, Error: 2.592499477987642e-05, prediction: 0.9962329329757357\n",
      "Iteration# 35, Error: 2.5591211687421718e-05, prediction: 0.996257407503562\n",
      "Iteration# 36, Error: 2.526334591908995e-05, prediction: 0.9962816027773723\n",
      "Iteration# 37, Error: 2.494126459988716e-05, prediction: 0.9963055233066677\n",
      "Iteration# 38, Error: 2.4624838453701647e-05, prediction: 0.9963291735072751\n",
      "Iteration# 39, Error: 2.431394168945799e-05, prediction: 0.9963525577037198\n",
      "Iteration# 40, Error: 2.4008451891412895e-05, prediction: 0.9963756801315213\n",
      "Iteration# 41, Error: 2.370824991334379e-05, prediction: 0.9963985449394297\n",
      "Iteration# 42, Error: 2.3413219776571292e-05, prediction: 0.9964211561915879\n",
      "Iteration# 43, Error: 2.312324857157521e-05, prediction: 0.9964435178696368\n",
      "Iteration# 44, Error: 2.2838226363127882e-05, prediction: 0.9964656338747544\n",
      "Iteration# 45, Error: 2.2558046098763055e-05, prediction: 0.9964875080296371\n",
      "Iteration# 46, Error: 2.2282603520445646e-05, prediction: 0.9965091440804265\n",
      "Iteration# 47, Error: 2.2011797079368705e-05, prediction: 0.996530545698574\n",
      "Iteration# 48, Error: 2.174552785367463e-05, prediction: 0.9965517164826598\n",
      "Iteration# 49, Error: 2.1483699469063647e-05, prediction: 0.9965726599601532\n",
      "Iteration# 50, Error: 2.122621802212281e-05, prediction: 0.9965933795891265\n",
      "Iteration# 51, Error: 2.0972992006301487e-05, prediction: 0.9966138787599188\n",
      "Iteration# 52, Error: 2.0723932240400958e-05, prediction: 0.9966341607967549\n",
      "Iteration# 53, Error: 2.0478951799532753e-05, prediction: 0.9966542289593148\n",
      "Iteration# 54, Error: 2.0237965948382075e-05, prediction: 0.9966740864442637\n",
      "Iteration# 55, Error: 2.0000892076753008e-05, prediction: 0.9966937363867364\n",
      "Iteration# 56, Error: 1.976764963726315e-05, prediction: 0.9967131818617824\n",
      "Iteration# 57, Error: 1.9538160085141962e-05, prediction: 0.9967324258857685\n",
      "Iteration# 58, Error: 1.931234682002194e-05, prediction: 0.9967514714177457\n",
      "Iteration# 59, Error: 1.9090135129666458e-05, prediction: 0.9967703213607771\n",
      "Iteration# 60, Error: 1.8871452135558593e-05, prediction: 0.9967889785632306\n",
      "Iteration# 61, Error: 1.8656226740289093e-05, prediction: 0.9968074458200338\n",
      "Iteration# 62, Error: 1.8444389576652166e-05, prediction: 0.9968257258738993\n",
      "Iteration# 63, Error: 1.8235872958418603e-05, prediction: 0.9968438214165133\n",
      "Iteration# 64, Error: 1.8030610832701062e-05, prediction: 0.9968617350896942\n",
      "Iteration# 65, Error: 1.782853873387312e-05, prediction: 0.9968794694865184\n",
      "Iteration# 66, Error: 1.7629593738951207e-05, prediction: 0.9968970271524195\n",
      "Iteration# 67, Error: 1.743371442443633e-05, prediction: 0.9969144105862543\n",
      "Iteration# 68, Error: 1.7240840824515466e-05, prediction: 0.9969316222413442\n",
      "Iteration# 69, Error: 1.705091439060793e-05, prediction: 0.9969486645264866\n",
      "Iteration# 70, Error: 1.6863877952187307e-05, prediction: 0.9969655398069408\n",
      "Iteration# 71, Error: 1.6679675678837365e-05, prediction: 0.99698225040539\n",
      "Iteration# 72, Error: 1.649825304352207e-05, prediction: 0.9969987986028737\n",
      "Iteration# 73, Error: 1.6319556786977042e-05, prediction: 0.9970151866397013\n",
      "Iteration# 74, Error: 1.6143534883234426e-05, prediction: 0.9970314167163393\n",
      "Iteration# 75, Error: 1.597013650621917e-05, prediction: 0.9970474909942744\n",
      "Iteration# 76, Error: 1.5799311997364127e-05, prediction: 0.9970634115968581\n",
      "Iteration# 77, Error: 1.563101283423281e-05, prediction: 0.9970791806101283\n",
      "Iteration# 78, Error: 1.5465191600119022e-05, prediction: 0.9970948000836077\n",
      "Iteration# 79, Error: 1.5301801954555458e-05, prediction: 0.9971102720310853\n",
      "Iteration# 80, Error: 1.5140798604731274e-05, prediction: 0.9971255984313769\n",
      "Iteration# 81, Error: 1.4982137277778673e-05, prediction: 0.9971407812290669\n",
      "Iteration# 82, Error: 1.4825774693898613e-05, prediction: 0.99715582233523\n",
      "Iteration# 83, Error: 1.4671668540286853e-05, prediction: 0.9971707236281381\n",
      "Iteration# 84, Error: 1.451977744585605e-05, prediction: 0.997185486953946\n",
      "Iteration# 85, Error: 1.4370060956707467e-05, prediction: 0.9972001141273626\n",
      "Iteration# 86, Error: 1.4222479512324306e-05, prediction: 0.9972146069323059\n",
      "Iteration# 87, Error: 1.4076994422491389e-05, prediction: 0.9972289671225391\n",
      "Iteration# 88, Error: 1.3933567844878183e-05, prediction: 0.9972431964222944\n",
      "Iteration# 89, Error: 1.3792162763292386e-05, prediction: 0.9972572965268801\n",
      "Iteration# 90, Error: 1.3652742966568267e-05, prediction: 0.9972712691032727\n",
      "Iteration# 91, Error: 1.3515273028071585e-05, prediction: 0.9972851157906956\n",
      "Iteration# 92, Error: 1.33797182858052e-05, prediction: 0.9972988382011828\n",
      "Iteration# 93, Error: 1.3246044823082383e-05, prediction: 0.9973124379201311\n",
      "Iteration# 94, Error: 1.3114219449772056e-05, prediction: 0.9973259165068362\n",
      "Iteration# 95, Error: 1.2984209684072914e-05, prediction: 0.9973392754950184\n",
      "Iteration# 96, Error: 1.2855983734819448e-05, prediction: 0.9973525163933343\n",
      "Iteration# 97, Error: 1.2729510484280098e-05, prediction: 0.9973656406858786\n",
      "Iteration# 98, Error: 1.2604759471462307e-05, prediction: 0.9973786498326712\n",
      "Iteration# 99, Error: 1.2481700875879973e-05, prediction: 0.9973915452701348\n"
     ]
    }
   ],
   "source": [
    "niters = 100\n",
    "\n",
    "for iter in range(niters):\n",
    "    \n",
    "    # forward pass\n",
    "    input_sequence = ['red', 'sox', 'defeat']\n",
    "    layer_0 = word_vecs[input_sequence[0]]\n",
    "    layer_1 = np.dot(layer_0, tmat) + word_vecs[input_sequence[1]]\n",
    "    layer_2 = np.dot(layer_1, tmat) + word_vecs[input_sequence[2]]\n",
    "    pred = softmax(np.dot(layer_2, W1))\n",
    "\n",
    "    # backpropagation\n",
    "\n",
    "    # the target word is 'yankees', whch is the first word in our vocab \n",
    "    y = np.zeros(shape=(1,vocab_size))\n",
    "    y[0,0] = 1\n",
    "\n",
    "    # error\n",
    "    error = (pred - y) * (pred - y)\n",
    "\n",
    "    # dE/dP\n",
    "    dP = 2 * (pred - y)\n",
    "    W1_grad = np.dot(layer_2.T, dP)\n",
    "\n",
    "    # dE_dL2\n",
    "    dL2 = np.dot(dP, W1.T)\n",
    "\n",
    "    # dE_d(defeat)\n",
    "    d_defeat = dL2 * 1\n",
    "    W_defeat_grad = d_defeat\n",
    "\n",
    "    # dE_dL1\n",
    "    dL1 = np.dot(dL2, tmat.T) \n",
    "    tmat_grad_2 = np.dot(layer_1.T, dL2)\n",
    "\n",
    "    # dE_d(sox)\n",
    "    d_sox = dL1 * 1\n",
    "    W_sox_grad = d_sox\n",
    "\n",
    "    # dE_dL0\n",
    "    dL0 = np.dot(dL1, tmat.T)\n",
    "    tmat_grad_1 = np.dot(layer_0.T, dL1)\n",
    "    W_red_grad = dL0 * 1\n",
    "\n",
    "    # optimization\n",
    "    alpha = 0.01\n",
    "    W1 -= alpha * W1_grad\n",
    "    word_vecs['defeat'] -= alpha * W_defeat_grad\n",
    "    word_vecs['sox'] -= alpha * W_sox_grad\n",
    "    word_vecs['red'] -= alpha * W_red_grad\n",
    "    tmat -= alpha * (tmat_grad_2 + tmat_grad_1) \n",
    "\n",
    "    print(f\"Iteration# {iter}, Error: {np.sum(error)}, prediction: {pred[0,0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01574f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
