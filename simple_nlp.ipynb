{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Author: Tanzid Sultan\n",
    "\n",
    "**Simple Natural Language Processing Models** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bag of words`: This is a simple way of creating numerical representation of text data. Given a vocabulary, i.e. a set of words, we can use one-hot encoding to assign a vector to each word. The dimensions of the vector equals the size of the vocabulary. The vector corresponding to any given word has a single `1` and the rest of the elements are zeros. \n",
    "\n",
    "Then given a sentence, we can contruct a corresponding vector by adding up the vectors for each unique word in that sentence.\n",
    "\n",
    "This is demonstrated in the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence vector: [1. 0. 1. 0. 1. 1. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# all words lower-case for convenience\n",
    "vocabulary = ['loves', 'cute','chocolate', 'is', 'cat', 'my', 'the', 'eating', 'i', 'four']\n",
    "\n",
    "# create onehot encoded word vectors and store them in a dictionary\n",
    "onehots = {}\n",
    "for i, word in enumerate(vocabulary):\n",
    "    word_vector = np.zeros(shape=(len(vocabulary)))\n",
    "    word_vector[i] = 1\n",
    "    onehots[word] = word_vector\n",
    "\n",
    "\n",
    "# test sentence\n",
    "sentence = ['my', 'cat', 'loves', 'eating', 'chocolate']\n",
    "sentence = set(sentence) # to remove duplicate words\n",
    "\n",
    "# vector representation of the test sentence\n",
    "sentence_vec = np.zeros(shape=(len(vocabulary)))\n",
    "for word in sentence:\n",
    "    sentence_vec += onehots[word]\n",
    "\n",
    "print(f\"Sentence vector: {sentence_vec}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding `IMDB movie reviews`: We will now read in text from a file containing IMDB movie reviews and create onehot encoded vocabulary. We will also read in text from a file containing the movie rating corresponding to these reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file and store every line of text in a list (each line is a separate movie review)\n",
    "f = open('reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of unique words pulled from every review (map iterates over every review and uses the lambda function to extract all unique words from the review)\n",
    "\n",
    "reviews_unique_words = list(map(lambda x: set(x.split(\" \")), raw_reviews))\n",
    "\n",
    "# now create a vocabulary from all unique words across all the reviews\n",
    "vocab = set()\n",
    "for review in reviews_unique_words:\n",
    "    for word in review:\n",
    "        if(len(word) > 0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# enumerate the words in the vocabulary and store the indices in a dictionary\n",
    "word_index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word_index[word] = i\n",
    "\n",
    "# now store the indices of all unique words appearing in a review\n",
    "input_dataset = []\n",
    "for review in reviews_unique_words:\n",
    "    review_indices = []\n",
    "    for word in review:\n",
    "        try:\n",
    "            review_indices.append(word_index[word])\n",
    "        except:\n",
    "            \"\"    \n",
    "    input_dataset.append(review_indices)\n",
    "\n",
    "# now store all the movie rating in a list\n",
    "target_dataset = []\n",
    "for rating in raw_labels:\n",
    "    if(rating  == 'positive\\n'):\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a simple 3 layer neural network and train it to predict the target label (i.e. positive or negative) for a given review text. We will use a sigmoid activation function in the hidden layer. Since the input is the one-hot encoded vector corresponding to review, it ois a vector of size equal to the vocabulary size and is filled with mostly 0s and a relatively small number of 1s. So, instead of doing a vector multiplication of this input vector with the weights matrix, we will simply sum up only the weight components corresponding to the non-zero components of the input vector, avoiding the unnecessary multiplications by 0s. Our input dataset is set up in a way that will facilitate this (since the inputs are just a list of all the position indices of 1s)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "    Input layer class: Input layer does not perform any operations\n",
    "'''\n",
    "class input_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    ''' \n",
    "        Input layer forward pass\n",
    "    '''\n",
    "    def forward(self, L_0):\n",
    "        self.L_0 = L_0\n",
    "        return self.L_0\n",
    "    \n",
    "''' \n",
    "    Hidden layer class: Hidden layer performs 2 operations. First it performs matrix multiplication\n",
    "                        of inputs L_0 with weights W_0. Then it operates on this result with the Relu\n",
    "                        function.\n",
    "'''    \n",
    "class hidden_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self, W) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "\n",
    "    ''' \n",
    "        Hidden layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L): \n",
    "        self.L = L\n",
    "        return self.forward_matrix_mult()\n",
    "\n",
    "    def forward_matrix_mult(self):\n",
    "        # instead of matrix multiplication, we just sum up the relevant weight components\n",
    "        self.Z = np.sum(self.W(self.L), axis = 0)\n",
    "           \n",
    "        return self.forward_sigmoid()\n",
    "    \n",
    "    def forward_sigmoid(self):\n",
    "        return sigmoid(self.Z)\n",
    "   \n",
    "    ''' \n",
    "        Hidden layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self, D):\n",
    "        self.backward_sigmoid(D)\n",
    "\n",
    "    def backward_sigmoid(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * sigmoid_deriv(self.Z) \n",
    "        self.backward_matrix_mult(dE_dZ)\n",
    "\n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW0\n",
    "        self.W_grad = np.dot((self.L).T, D)\n",
    "\n",
    "    ''' \n",
    "        Gradient descent optimization of hidden layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        self.W -= alpha * self.W_grad\n",
    "\n",
    "       \n",
    "\n",
    "''' \n",
    "    Ouput layer class: Performs two operations, first matrix multiplication of inputs L_1 with weights\n",
    "                       W_1. This result is then operated on by squared error function.  \n",
    "'''\n",
    "class output_layer(object):\n",
    "    \n",
    "    ''' \n",
    "        class constructor\n",
    "    '''\n",
    "\n",
    "    def __init__(self, W) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "\n",
    "    ''' \n",
    "        Output layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L, Y, soft):\n",
    "        self.L = L\n",
    "        self.Y = Y\n",
    "        return self.forward_matrix_mult(soft)\n",
    "\n",
    "    def forward_matrix_mult(self, soft):\n",
    "        self.P = np.dot(self.L, self.W) \n",
    "        if(soft):\n",
    "            self.P = softmax(self.P)\n",
    "        \n",
    "        return self.P, self.forward_error()\n",
    " \n",
    "    def forward_error(self):\n",
    "        return np.sum((self.P - self.Y)**2) / self.P.shape[0]\n",
    "\n",
    "    '''     \n",
    "        Output layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self):\n",
    "        return self.backward_error()\n",
    "\n",
    "    def backward_error(self):\n",
    "        # dE/dP\n",
    "        dE_dP = 2*(self.P - self.Y) / self.P.shape[0]\n",
    "        return self.backward_matrix_mult(dE_dP)\n",
    "\n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW1\n",
    "        self.W_grad = np.dot((self.L).T, D)\n",
    "        # dE/dL1\n",
    "        dE_dL = np.dot(D, (self.W).T)\n",
    "        return dE_dL\n",
    "    \n",
    "    ''' \n",
    "        Gradient descent optimization of output layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        self.W -= alpha * self.W_grad\n",
    "\n",
    "'''\n",
    "    A 3-layer neural network class\n",
    "'''\n",
    "class three_layer_net(object):\n",
    "    ''' \n",
    "        class constructor: Takes in the following parameters- number of neurons in input layer (which is the number of feature attributes for each instance), number of hidden layers (has to be at least 1 and can be arbitrarily large), number of neurons in the output layer (which is the number of target attributes) and gradient descent step-size (alpha)\n",
    "    '''\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons) -> None:\n",
    "        self.input_neurons  = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        \n",
    "        np.random.seed(1)\n",
    "        # initialize weights W0 between input layer and hidden layer \n",
    "        W0 = 0.2*np.random.random(size=(input_neurons, hidden_neurons)) - 0.1\n",
    "        # initialize weights W1 between hidden layer and output layer\n",
    "        W1 = 0.2*np.random.random(size=(hidden_neurons, output_neurons)) - 0.1 \n",
    "\n",
    "        # initialize layer objects\n",
    "        self.layer_0 = input_layer()\n",
    "        self.layer_1 = hidden_layer(W0)\n",
    "        self.layer_2 = output_layer(W1)\n",
    "\n",
    "    ''' \n",
    "        neural network forward pass\n",
    "    '''\n",
    "    def forward_net(self, L0, Y, soft):\n",
    "        # input layer forward pass\n",
    "        self.L0 = self.layer_0.forward(L0) \n",
    "        # hidden layer forward pass \n",
    "        self.L1 = self.layer_1.forward(self.L0) \n",
    "        # output layer forward pass\n",
    "        self.L2, error = self.layer_2.forward(self.L1, Y, soft) \n",
    "\n",
    "        return self.L2, error\n",
    "\n",
    "    ''' \n",
    "        neural network backward pass\n",
    "    ''' \n",
    "    def backward_net(self):\n",
    "       # output layer backpropagation\n",
    "       D = self.layer_2.backward() \n",
    "       # hidden layer backpropagation\n",
    "       self.layer_1.backward(D) \n",
    "\n",
    "    '''     \n",
    "        weight optimization\n",
    "    '''\n",
    "    def optimize(self, alpha):\n",
    "        # update output layer weights\n",
    "        self.layer_2.update_weights(alpha)\n",
    "        # update hidden layer weights\n",
    "        self.layer_1.update_weights(alpha)\n",
    "\n",
    "    '''     \n",
    "        train the network\n",
    "    ''' \n",
    "    def train(self, X_train, y_train, X_test, y_test, alpha, niters=1, soft=False):\n",
    "        print(f\"Softmax Enabled: {soft}\")\n",
    "        print(f\"Alpha: {alpha}\")\n",
    "        print(\"Training in progress...\")\n",
    "        #training iterations\n",
    "        for i in range(niters):\n",
    "            total_error = 0.0\n",
    "            train_correct_count = 0\n",
    "            # train using batch of instances\n",
    "            for j in range(len(X_train)):\n",
    "\n",
    "                X = X_train[j]\n",
    "                y = y_train[j]\n",
    "\n",
    "                # forward propagation\n",
    "                prediction, error = self.forward_net(X, y, soft)\n",
    "                total_error += error\n",
    "                \n",
    "                train_correct_count += int(np.abs(prediction-y) < 0.5)\n",
    "                \n",
    "                #if(i == (niters-1)):\n",
    "                #    print(f\"Instance# {j+1}, Target: {y}, Prediction: {prediction}\")\n",
    "\n",
    "                # backpropagation\n",
    "                self.backward_net()\n",
    "\n",
    "                # weight optimization\n",
    "                self.optimize(alpha)\n",
    "\n",
    "            # predict using test instances\n",
    "            test_correct_count = 0\n",
    "            for j in range(len(X_test)):\n",
    "                X = X_test[j]\n",
    "                y = y_test[j]\n",
    "\n",
    "                # forward propagation\n",
    "                prediction, error = self.forward_net(X, y, soft=False)\n",
    "                test_correct_count += int(np.abs(prediction-y) < 0.5)\n",
    "\n",
    "            print(f\"Iteration# {i+1}, Total error: {total_error}, Training accuracy: {train_correct_count/len(y_train)}, Testing accuracy: {test_correct_count/len(y_test)}\")\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-1.0 * x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x)) \n",
    "\n",
    "def softmax(x): \n",
    "    ex = np.exp(x)\n",
    "    return ex/np.sum(ex, axis = 1, keepdims = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a three layer network object\n",
    "input_neurons = len(vocab)\n",
    "hidden_neurons = 100\n",
    "output_neurons = 1 \n",
    "net = three_layer_net(input_neurons, hidden_neurons, output_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Enabled: True\n",
      "Alpha: 0.01\n",
      "Training in progress...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22228\\2006471548.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# train the network with the reviews dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mniters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22228\\4196195402.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_train, y_train, X_test, y_test, alpha, niters, soft)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[1;31m# forward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m                 \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoft\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m                 \u001b[0mtotal_error\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22228\\4196195402.py\u001b[0m in \u001b[0;36mforward_net\u001b[1;34m(self, L0, Y, soft)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;31m# hidden layer forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m         \u001b[1;31m# output layer forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoft\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22228\\4196195402.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, L)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_matrix_mult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_matrix_mult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22228\\4196195402.py\u001b[0m in \u001b[0;36mforward_matrix_mult\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_matrix_mult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# instead of matrix multiplication, we just sum up the relevant weight components\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_sigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "# preprocess the training and testing datasets\n",
    "nreviews = len(input_dataset)\n",
    "X_train = input_dataset[:nreviews-1000]\n",
    "y_train = target_dataset[:nreviews-1000]\n",
    "X_test = input_dataset[nreviews-1000:]\n",
    "y_test = target_dataset[nreviews-1000:]\n",
    "\n",
    "# train the network with the reviews dataset\n",
    "net.train(X_train, y_train, X_test, y_test, alpha = 0.01, niters = 2, soft = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
