{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Author: Tanzid Sultan\n",
    "\n",
    "**Simple Natural Language Processing Models** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bag of words`: This is a simple way of creating numerical representation of text data. Given a vocabulary, i.e. a set of words, we can use one-hot encoding to assign a vector to each word. The dimensions of the vector equals the size of the vocabulary. The vector corresponding to any given word has a single `1` and the rest of the elements are zeros. \n",
    "\n",
    "Then given a sentence, we can contruct a corresponding vector by adding up the vectors for each unique word in that sentence.\n",
    "\n",
    "This is demonstrated in the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence vector: [1. 0. 1. 0. 1. 1. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# all words lower-case for convenience\n",
    "vocabulary = ['loves', 'cute','chocolate', 'is', 'cat', 'my', 'the', 'eating', 'i', 'four']\n",
    "\n",
    "# create onehot encoded word vectors and store them in a dictionary\n",
    "onehots = {}\n",
    "for i, word in enumerate(vocabulary):\n",
    "    word_vector = np.zeros(shape=(len(vocabulary)))\n",
    "    word_vector[i] = 1\n",
    "    onehots[word] = word_vector\n",
    "\n",
    "\n",
    "# test sentence\n",
    "sentence = ['my', 'cat', 'loves', 'eating', 'chocolate']\n",
    "sentence = set(sentence) # to remove duplicate words\n",
    "\n",
    "# vector representation of the test sentence\n",
    "sentence_vec = np.zeros(shape=(len(vocabulary)))\n",
    "for word in sentence:\n",
    "    sentence_vec += onehots[word]\n",
    "\n",
    "print(f\"Sentence vector: {sentence_vec}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding `IMDB movie reviews`: We will now read in text from a file containing IMDB movie reviews and create onehot encoded vocabulary. We will also read in text from a file containing the movie rating corresponding to these reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file and store every line of text in a list (each line is a separate movie review)\n",
    "f = open('reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of unique words pulled from every review (map iterates over every review and uses the lambda function to extract all unique words from the review)\n",
    "\n",
    "reviews_unique_words = list(map(lambda x: set(x.split(\" \")), raw_reviews))\n",
    "\n",
    "# now create a vocabulary from all unique words across all the reviews\n",
    "vocab = set()\n",
    "for review in reviews_unique_words:\n",
    "    for word in review:\n",
    "        if(len(word) > 0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# enumerate the words in the vocabulary and store the indices in a dictionary\n",
    "word_index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word_index[word] = i\n",
    "\n",
    "# now store the indices of all unique words appearing in a review\n",
    "input_dataset = []\n",
    "for review in reviews_unique_words:\n",
    "    review_indices = []\n",
    "    for word in review:\n",
    "        try:\n",
    "            review_indices.append(word_index[word])\n",
    "        except:\n",
    "            \"\"    \n",
    "    input_dataset.append(review_indices)\n",
    "\n",
    "# now store all the movie rating in a list\n",
    "target_dataset = []\n",
    "for rating in raw_labels:\n",
    "    if(rating  == 'positive\\n'):\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a simple 3 layer neural network and train it to predict the target label (i.e. positive or negative) for a given review text. We will use a sigmoid activation function in the hidden layer. Since the input is the one-hot encoded vector corresponding to review, it ois a vector of size equal to the vocabulary size and is filled with mostly 0s and a relatively small number of 1s. So, instead of doing a vector multiplication of this input vector with the weights matrix, we will simply sum up only the weight components corresponding to the non-zero components of the input vector, avoiding the unnecessary multiplications by 0s. Our input dataset is set up in a way that will facilitate this (since the inputs are just a list of all the position indices of 1s)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "    Input layer class: Input layer does not perform any operations\n",
    "'''\n",
    "class input_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    ''' \n",
    "        Input layer forward pass\n",
    "    '''\n",
    "    def forward(self, L_0):\n",
    "        self.L_0 = L_0\n",
    "        return self.L_0\n",
    "    \n",
    "''' \n",
    "    Hidden layer class: Hidden layer performs 2 operations. First it performs matrix multiplication\n",
    "                        of inputs L_0 with weights W_0. Then it operates on this result with the Relu\n",
    "                        function.\n",
    "'''    \n",
    "class hidden_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self, W) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "\n",
    "    ''' \n",
    "        Hidden layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L): \n",
    "        self.L = L\n",
    "        return self.forward_matrix_mult()\n",
    "\n",
    "    def forward_matrix_mult(self):\n",
    "        # instead of matrix multiplication, we just sum up the relevant weight components\n",
    "        self.Z = np.sum(self.W[self.L], axis = 0)\n",
    "        self.Z = (self.Z).reshape(1,(self.W).shape[1])   \n",
    "\n",
    "        return self.forward_sigmoid()\n",
    "    \n",
    "    def forward_sigmoid(self):\n",
    "        return sigmoid(self.Z)\n",
    "   \n",
    "    ''' \n",
    "        Hidden layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self, D):\n",
    "        self.backward_sigmoid(D)\n",
    "\n",
    "    def backward_sigmoid(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * sigmoid_deriv(self.Z) \n",
    "        self.backward_matrix_mult(dE_dZ)\n",
    "\n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW0\n",
    "        self.W_grad = D\n",
    "\n",
    "    ''' \n",
    "        Gradient descent optimization of hidden layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        # only need to update the weights in the relevant rows (i.e. the weights that contribute to the non-zero input components)\n",
    "        self.W[self.L] -= alpha * self.W_grad\n",
    "\n",
    "       \n",
    "\n",
    "''' \n",
    "    Ouput layer class: Performs two operations, first matrix multiplication of inputs L_1 with weights\n",
    "                       W_1. This result is then operated on by squared error function.  \n",
    "'''\n",
    "class output_layer(object):\n",
    "    \n",
    "    ''' \n",
    "        class constructor\n",
    "    '''\n",
    "\n",
    "    def __init__(self, W) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "\n",
    "    ''' \n",
    "        Output layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L, Y):\n",
    "        self.L = L\n",
    "        self.Y = Y\n",
    "        return self.forward_matrix_mult()\n",
    "\n",
    "    def forward_matrix_mult(self):\n",
    "        self.P = np.dot(self.L, self.W) \n",
    "        # apply sigmoid function\n",
    "        self.P = sigmoid(self.P)\n",
    "        \n",
    "        return self.P, self.forward_error()\n",
    " \n",
    "    def forward_error(self):\n",
    "        return np.sum((self.P - self.Y)**2) / self.P.shape[0]\n",
    "\n",
    "    '''     \n",
    "        Output layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self):\n",
    "        return self.backward_error()\n",
    "\n",
    "    def backward_error(self):\n",
    "        # dE/dP\n",
    "        dE_dP = 2*(self.P - self.Y) / self.P.shape[0]\n",
    "        return self.backward_matrix_mult(dE_dP)\n",
    "\n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW1\n",
    "        self.W_grad = np.dot((self.L).T, D)\n",
    "        # dE/dL1\n",
    "        dE_dL = np.dot(D, (self.W).T)\n",
    "        return dE_dL\n",
    "    \n",
    "    ''' \n",
    "        Gradient descent optimization of output layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        self.W -= alpha * self.W_grad\n",
    "\n",
    "'''\n",
    "    A 3-layer neural network class\n",
    "'''\n",
    "class three_layer_net(object):\n",
    "    ''' \n",
    "        class constructor: Takes in the following parameters- number of neurons in input layer (which is the number of feature attributes for each instance), number of hidden layers (has to be at least 1 and can be arbitrarily large), number of neurons in the output layer (which is the number of target attributes) and gradient descent step-size (alpha)\n",
    "    '''\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons) -> None:\n",
    "        self.input_neurons  = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        \n",
    "        np.random.seed(1)\n",
    "        # initialize weights W0 between input layer and hidden layer \n",
    "        W0 = 0.2*np.random.random(size=(input_neurons, hidden_neurons)) - 0.1\n",
    "        # initialize weights W1 between hidden layer and output layer\n",
    "        W1 = 0.2*np.random.random(size=(hidden_neurons, output_neurons)) - 0.1 \n",
    "\n",
    "        # initialize layer objects\n",
    "        self.layer_0 = input_layer()\n",
    "        self.layer_1 = hidden_layer(W0)\n",
    "        self.layer_2 = output_layer(W1)\n",
    "\n",
    "    ''' \n",
    "        neural network forward pass\n",
    "    '''\n",
    "    def forward_net(self, L0, Y):\n",
    "        # input layer forward pass\n",
    "        self.L0 = self.layer_0.forward(L0) \n",
    "        # hidden layer forward pass \n",
    "        self.L1 = self.layer_1.forward(self.L0) \n",
    "        # output layer forward pass\n",
    "        self.L2, error = self.layer_2.forward(self.L1, Y) \n",
    "\n",
    "        return self.L2, error\n",
    "\n",
    "    ''' \n",
    "        neural network backward pass\n",
    "    ''' \n",
    "    def backward_net(self):\n",
    "       # output layer backpropagation\n",
    "       D = self.layer_2.backward() \n",
    "       # hidden layer backpropagation\n",
    "       self.layer_1.backward(D) \n",
    "\n",
    "    '''     \n",
    "        weight optimization\n",
    "    '''\n",
    "    def optimize(self, alpha):\n",
    "        # update output layer weights\n",
    "        self.layer_2.update_weights(alpha)\n",
    "        # update hidden layer weights\n",
    "        self.layer_1.update_weights(alpha)\n",
    "\n",
    "    '''     \n",
    "        train the network\n",
    "    ''' \n",
    "    def train(self, X_train, y_train, X_test, y_test, alpha, niters=1):\n",
    "        print(f\"Alpha: {alpha}\")\n",
    "        print(\"Training in progress...\")\n",
    "        #training iterations\n",
    "        for i in range(niters):\n",
    "            total_error = 0.0\n",
    "            train_correct_count = 0\n",
    "            # train using batch of instances\n",
    "            for j in range(len(X_train)):\n",
    "\n",
    "                X = X_train[j]\n",
    "                y = y_train[j]\n",
    "\n",
    "                # forward propagation\n",
    "                prediction, error = self.forward_net(X, y)\n",
    "                total_error += error\n",
    "                \n",
    "                train_correct_count += int(np.abs(prediction-y) < 0.25)\n",
    "                \n",
    "                #if(i == (niters-1)):\n",
    "                #    print(f\"Instance# {j+1}, Target: {y}, Prediction: {prediction}\")\n",
    "\n",
    "                # backpropagation\n",
    "                self.backward_net()\n",
    "\n",
    "                # weight optimization\n",
    "                self.optimize(alpha)\n",
    "\n",
    "            # predict using test instances\n",
    "            test_correct_count = 0\n",
    "            for j in range(len(X_test)):\n",
    "                X = X_test[j]\n",
    "                y = y_test[j]\n",
    "\n",
    "                # forward propagation\n",
    "                prediction, error = self.forward_net(X, y)\n",
    "                test_correct_count += int(np.abs(prediction-y) < 0.25)\n",
    "\n",
    "            print(f\"Iteration# {i+1}, Total error: {total_error}, Training accuracy: {train_correct_count/len(y_train)}, Testing accuracy: {test_correct_count/len(y_test)}\")\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-1.0 * x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001\n",
      "Training in progress...\n",
      "Iteration# 1, Total error: 5123.032497962775, Training accuracy: 0.04741666666666667, Testing accuracy: 0.226\n",
      "Iteration# 2, Total error: 3073.3804864169633, Training accuracy: 0.4891666666666667, Testing accuracy: 0.567\n",
      "Iteration# 3, Total error: 2389.4911494813973, Training accuracy: 0.663375, Testing accuracy: 0.659\n",
      "Iteration# 4, Total error: 2061.0156175406064, Training accuracy: 0.7236666666666667, Testing accuracy: 0.692\n",
      "Iteration# 5, Total error: 1837.2715147111196, Training accuracy: 0.757125, Testing accuracy: 0.715\n",
      "Iteration# 6, Total error: 1665.0586115262136, Training accuracy: 0.7802083333333333, Testing accuracy: 0.729\n",
      "Iteration# 7, Total error: 1522.7122002663696, Training accuracy: 0.798125, Testing accuracy: 0.734\n",
      "Iteration# 8, Total error: 1400.062176274717, Training accuracy: 0.8125, Testing accuracy: 0.741\n",
      "Iteration# 9, Total error: 1291.7678381462074, Training accuracy: 0.8246666666666667, Testing accuracy: 0.746\n",
      "Iteration# 10, Total error: 1194.4477937424335, Training accuracy: 0.835875, Testing accuracy: 0.748\n",
      "Iteration# 11, Total error: 1105.8019249605795, Training accuracy: 0.847375, Testing accuracy: 0.748\n",
      "Iteration# 12, Total error: 1024.2026960907538, Training accuracy: 0.857625, Testing accuracy: 0.752\n",
      "Iteration# 13, Total error: 948.5528047260264, Training accuracy: 0.867625, Testing accuracy: 0.757\n",
      "Iteration# 14, Total error: 878.2507069194501, Training accuracy: 0.87675, Testing accuracy: 0.759\n",
      "Iteration# 15, Total error: 813.0070914362625, Training accuracy: 0.88425, Testing accuracy: 0.759\n",
      "Iteration# 16, Total error: 752.4773854617833, Training accuracy: 0.8924583333333334, Testing accuracy: 0.765\n",
      "Iteration# 17, Total error: 696.1972948901366, Training accuracy: 0.899625, Testing accuracy: 0.768\n",
      "Iteration# 18, Total error: 643.7220207438294, Training accuracy: 0.9064583333333334, Testing accuracy: 0.772\n",
      "Iteration# 19, Total error: 594.6716151314039, Training accuracy: 0.9127916666666667, Testing accuracy: 0.774\n",
      "Iteration# 20, Total error: 548.7880219140085, Training accuracy: 0.9192916666666666, Testing accuracy: 0.775\n",
      "Iteration# 21, Total error: 505.9645839861937, Training accuracy: 0.92525, Testing accuracy: 0.779\n",
      "Iteration# 22, Total error: 466.1567755831372, Training accuracy: 0.93075, Testing accuracy: 0.781\n",
      "Iteration# 23, Total error: 429.2648209255974, Training accuracy: 0.9358333333333333, Testing accuracy: 0.78\n",
      "Iteration# 24, Total error: 395.11213613410405, Training accuracy: 0.9409166666666666, Testing accuracy: 0.781\n",
      "Iteration# 25, Total error: 363.50511704469585, Training accuracy: 0.9455, Testing accuracy: 0.781\n",
      "Iteration# 26, Total error: 334.289271556138, Training accuracy: 0.9495833333333333, Testing accuracy: 0.783\n",
      "Iteration# 27, Total error: 307.3477314924138, Training accuracy: 0.9540416666666667, Testing accuracy: 0.786\n",
      "Iteration# 28, Total error: 282.5576559095024, Training accuracy: 0.957875, Testing accuracy: 0.785\n",
      "Iteration# 29, Total error: 259.7817839096389, Training accuracy: 0.9612916666666667, Testing accuracy: 0.786\n",
      "Iteration# 30, Total error: 238.8851883984291, Training accuracy: 0.9644583333333333, Testing accuracy: 0.787\n",
      "Iteration# 31, Total error: 219.7355880501152, Training accuracy: 0.9678333333333333, Testing accuracy: 0.789\n",
      "Iteration# 32, Total error: 202.19961411391276, Training accuracy: 0.97075, Testing accuracy: 0.79\n",
      "Iteration# 33, Total error: 186.14536014329323, Training accuracy: 0.973375, Testing accuracy: 0.792\n",
      "Iteration# 34, Total error: 171.4462008115798, Training accuracy: 0.9757916666666666, Testing accuracy: 0.792\n",
      "Iteration# 35, Total error: 157.98314154851556, Training accuracy: 0.9777916666666666, Testing accuracy: 0.793\n",
      "Iteration# 36, Total error: 145.6463456449017, Training accuracy: 0.9795, Testing accuracy: 0.794\n",
      "Iteration# 37, Total error: 134.33623570338253, Training accuracy: 0.9816666666666667, Testing accuracy: 0.795\n",
      "Iteration# 38, Total error: 123.96376674647848, Training accuracy: 0.9830833333333333, Testing accuracy: 0.795\n",
      "Iteration# 39, Total error: 114.44946427604573, Training accuracy: 0.9844583333333333, Testing accuracy: 0.795\n",
      "Iteration# 40, Total error: 105.7214761578471, Training accuracy: 0.9855833333333334, Testing accuracy: 0.794\n",
      "Iteration# 41, Total error: 97.71355460518829, Training accuracy: 0.9869166666666667, Testing accuracy: 0.795\n"
     ]
    }
   ],
   "source": [
    "# initialize a three layer network object\n",
    "input_neurons = len(vocab)\n",
    "hidden_neurons = 100\n",
    "output_neurons = 1 \n",
    "net = three_layer_net(input_neurons, hidden_neurons, output_neurons)\n",
    "\n",
    "# preprocess the training and testing datasets\n",
    "nreviews = len(input_dataset)\n",
    "X_train = input_dataset[:nreviews-1000]\n",
    "y_train = target_dataset[:nreviews-1000]\n",
    "X_test = input_dataset[nreviews-1000:]\n",
    "y_test = target_dataset[nreviews-1000:]\n",
    "\n",
    "# train the network with the reviews dataset\n",
    "net.train(X_train, y_train, X_test, y_test, alpha = 0.001, niters = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
