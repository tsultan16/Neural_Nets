{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Author: Tanzid Sultan\n",
    "\n",
    "**Simple Natural Language Processing Models** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bag of words`: This is a simple way of creating numerical representation of text data. Given a vocabulary, i.e. a set of words, we can use one-hot encoding to assign a vector to each word. The dimensions of the vector equals the size of the vocabulary. The vector corresponding to any given word has a single `1` and the rest of the elements are zeros. \n",
    "\n",
    "Then given a sentence, we can contruct a corresponding vector by adding up the vectors for each unique word in that sentence.\n",
    "\n",
    "This is demonstrated in the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence vector: [1. 0. 1. 0. 1. 1. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# all words lower-case for convenience\n",
    "vocabulary = ['loves', 'cute','chocolate', 'is', 'cat', 'my', 'the', 'eating', 'i', 'four']\n",
    "\n",
    "# create onehot encoded word vectors and store them in a dictionary\n",
    "onehots = {}\n",
    "for i, word in enumerate(vocabulary):\n",
    "    word_vector = np.zeros(shape=(len(vocabulary)))\n",
    "    word_vector[i] = 1\n",
    "    onehots[word] = word_vector\n",
    "\n",
    "\n",
    "# test sentence\n",
    "sentence = ['my', 'cat', 'loves', 'eating', 'chocolate']\n",
    "sentence = set(sentence) # to remove duplicate words\n",
    "\n",
    "# vector representation of the test sentence\n",
    "sentence_vec = np.zeros(shape=(len(vocabulary)))\n",
    "for word in sentence:\n",
    "    sentence_vec += onehots[word]\n",
    "\n",
    "print(f\"Sentence vector: {sentence_vec}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding `IMDB movie reviews`: We will now read in text from a file containing IMDB movie reviews and create onehot encoded vocabulary. We will also read in text from a file containing the movie rating corresponding to these reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file and store every line of text in a list (each line is a separate movie review)\n",
    "f = open('reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of unique words pulled from every review (map iterates over every review and uses the lambda function to extract all unique words from the review)\n",
    "\n",
    "reviews_unique_words = list(map(lambda x: set(x.split(\" \")), raw_reviews))\n",
    "\n",
    "# now create a vocabulary from all unique words across all the reviews\n",
    "vocab = set()\n",
    "for review in reviews_unique_words:\n",
    "    for word in review:\n",
    "        if(len(word) > 0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# enumerate the words in the vocabulary and store the indices in a dictionary\n",
    "word_index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word_index[word] = i\n",
    "\n",
    "# now store the indices of all unique words appearing in a review\n",
    "input_dataset = []\n",
    "for review in reviews_unique_words:\n",
    "    review_indices = []\n",
    "    for word in review:\n",
    "        try:\n",
    "            review_indices.append(word_index[word])\n",
    "        except:\n",
    "            \"\"    \n",
    "    input_dataset.append(review_indices)\n",
    "\n",
    "# now store all the movie rating in a list\n",
    "target_dataset = []\n",
    "for rating in raw_labels:\n",
    "    if(rating  == 'positive\\n'):\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Word-Embedding`**\n",
    "\n",
    "We will now build a simple 3 layer neural network and train it to predict the target label (i.e. positive or negative) for a given review text. We will use a sigmoid activation function in the hidden layer. Since the input is the one-hot encoded vector corresponding to review, it is a vector of size equal to the vocabulary size and is filled with mostly 0s and a relatively small number of 1s. So, instead of doing a vector multiplication of this input vector with the weights matrix, we will simply sum up only the weight components corresponding to the non-zero components of the input vector, avoiding the unnecessary multiplications by 0s. Our input dataset is set up in a way that will facilitate this (since the inputs are just a list of all the position indices of 1s)\n",
    "\n",
    "The `hidden layer weights W0` is also called the `word-embedding matrix` since each row in W0 mathematically represents the `\"meaning\"` of the word corresponding to that row index, i.e. it is the `embedding vector for that word`. Optimizing these weights would then result in words which are similar/closer in meaning to have embedding vectors that have greater similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "'''\n",
    "    Input layer class: Input layer does not perform any operations\n",
    "'''\n",
    "class input_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    ''' \n",
    "        Input layer forward pass\n",
    "    '''\n",
    "    def forward(self, L_0):\n",
    "        self.L_0 = L_0\n",
    "        return self.L_0\n",
    "    \n",
    "''' \n",
    "    Hidden layer class: Hidden layer performs 2 operations. First it performs matrix multiplication\n",
    "                        of inputs L_0 with weights W_0. Then it operates on this result with the Relu\n",
    "                        function.\n",
    "'''    \n",
    "class hidden_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self, W) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "\n",
    "    ''' \n",
    "        Hidden layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L): \n",
    "        self.L = L\n",
    "        return self.forward_matrix_mult()\n",
    "\n",
    "    def forward_matrix_mult(self):\n",
    "        # instead of matrix multiplication, we just sum up the relevant weight components\n",
    "        self.Z = np.sum(self.W[self.L], axis = 0)\n",
    "        self.Z = (self.Z).reshape(1,(self.W).shape[1])   \n",
    "\n",
    "        return self.forward_sigmoid()\n",
    "    \n",
    "    def forward_sigmoid(self):\n",
    "        return sigmoid(self.Z)\n",
    "   \n",
    "    ''' \n",
    "        Hidden layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self, D):\n",
    "        self.backward_sigmoid(D)\n",
    "\n",
    "    def backward_sigmoid(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * sigmoid_deriv(self.Z) \n",
    "        self.backward_matrix_mult(dE_dZ)\n",
    "\n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW0\n",
    "        self.W_grad = D\n",
    "\n",
    "    ''' \n",
    "        Gradient descent optimization of hidden layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        # only need to update the weights in the relevant rows (i.e. the weights that contribute to the non-zero input components)\n",
    "        self.W[self.L] -= alpha * self.W_grad\n",
    "\n",
    "       \n",
    "\n",
    "''' \n",
    "    Ouput layer class: Performs two operations, first matrix multiplication of inputs L_1 with weights\n",
    "                       W_1. This result is then operated on by squared error function.  \n",
    "'''\n",
    "class output_layer(object):\n",
    "    \n",
    "    ''' \n",
    "        class constructor\n",
    "    '''\n",
    "\n",
    "    def __init__(self, W) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "\n",
    "    ''' \n",
    "        Output layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L, Y):\n",
    "        self.L = L\n",
    "        self.Y = Y\n",
    "        return self.forward_matrix_mult()\n",
    "\n",
    "    def forward_matrix_mult(self):\n",
    "        self.P = np.dot(self.L, self.W) \n",
    "        # apply sigmoid function\n",
    "        self.P = sigmoid(self.P)\n",
    "        \n",
    "        return self.P, self.forward_error()\n",
    " \n",
    "    def forward_error(self):\n",
    "        return np.sum((self.P - self.Y)**2) / self.P.shape[0]\n",
    "\n",
    "    '''     \n",
    "        Output layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self):\n",
    "        return self.backward_error()\n",
    "\n",
    "    def backward_error(self):\n",
    "        # dE/dP\n",
    "        dE_dP = 2*(self.P - self.Y) / self.P.shape[0]\n",
    "        return self.backward_matrix_mult(dE_dP)\n",
    "\n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW1\n",
    "        self.W_grad = np.dot((self.L).T, D)\n",
    "        # dE/dL1\n",
    "        dE_dL = np.dot(D, (self.W).T)\n",
    "        return dE_dL\n",
    "    \n",
    "    ''' \n",
    "        Gradient descent optimization of output layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        self.W -= alpha * self.W_grad\n",
    "\n",
    "'''\n",
    "    A 3-layer neural network class\n",
    "'''\n",
    "class three_layer_net(object):\n",
    "    ''' \n",
    "        class constructor: Takes in the following parameters- number of neurons in input layer (which is the number of feature attributes for each instance), number of hidden layers (has to be at least 1 and can be arbitrarily large), number of neurons in the output layer (which is the number of target attributes) and gradient descent step-size (alpha)\n",
    "    '''\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons) -> None:\n",
    "        self.input_neurons  = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        \n",
    "        np.random.seed(1)\n",
    "        # initialize weights W0 between input layer and hidden layer \n",
    "        W0 = 0.2*np.random.random(size=(input_neurons, hidden_neurons)) - 0.1\n",
    "        # initialize weights W1 between hidden layer and output layer\n",
    "        W1 = 0.2*np.random.random(size=(hidden_neurons, output_neurons)) - 0.1 \n",
    "\n",
    "\n",
    "        # initialize layer objects\n",
    "        self.layer_0 = input_layer()\n",
    "        self.layer_1 = hidden_layer(W0)\n",
    "        self.layer_2 = output_layer(W1)\n",
    "\n",
    "    ''' \n",
    "        neural network forward pass\n",
    "    '''\n",
    "    def forward_net(self, L0, Y):\n",
    "        # input layer forward pass\n",
    "        self.L0 = self.layer_0.forward(L0) \n",
    "        # hidden layer forward pass \n",
    "        self.L1 = self.layer_1.forward(self.L0) \n",
    "        # output layer forward pass\n",
    "        self.L2, error = self.layer_2.forward(self.L1, Y) \n",
    "\n",
    "        return self.L2, error\n",
    "\n",
    "    ''' \n",
    "        neural network backward pass\n",
    "    ''' \n",
    "    def backward_net(self):\n",
    "       # output layer backpropagation\n",
    "       D = self.layer_2.backward() \n",
    "       # hidden layer backpropagation\n",
    "       self.layer_1.backward(D) \n",
    "\n",
    "    '''     \n",
    "        weight optimization\n",
    "    '''\n",
    "    def optimize(self, alpha):\n",
    "        # update output layer weights\n",
    "        self.layer_2.update_weights(alpha)\n",
    "        # update hidden layer weights\n",
    "        self.layer_1.update_weights(alpha)\n",
    "\n",
    "    '''     \n",
    "        train the network\n",
    "    ''' \n",
    "    def train(self, X_train, y_train, X_test, y_test, alpha, niters=1):\n",
    "        print(f\"Alpha: {alpha}\")\n",
    "        print(\"Training in progress...\")\n",
    "        #training iterations\n",
    "        for i in range(niters):\n",
    "            total_error = 0.0\n",
    "            train_correct_count = 0\n",
    "            # train using batch of instances\n",
    "            for j in range(len(X_train)):\n",
    "\n",
    "                X = X_train[j]\n",
    "                y = y_train[j]\n",
    "\n",
    "                # forward propagation\n",
    "                prediction, error = self.forward_net(X, y)\n",
    "                total_error += error\n",
    "                \n",
    "                train_correct_count += int(np.abs(prediction-y) < 0.5)\n",
    "                \n",
    "                #if(i == (niters-1)):\n",
    "                #    print(f\"Instance# {j+1}, Target: {y}, Prediction: {prediction}\")\n",
    "\n",
    "                # backpropagation\n",
    "                self.backward_net()\n",
    "\n",
    "                # weight optimization\n",
    "                self.optimize(alpha)\n",
    "\n",
    "            # predict using test instances\n",
    "            test_correct_count = 0\n",
    "            for j in range(len(X_test)):\n",
    "                X = X_test[j]\n",
    "                y = y_test[j]\n",
    "\n",
    "                # forward propagation\n",
    "                prediction, error = self.forward_net(X, y)\n",
    "                test_correct_count += int(np.abs(prediction-y) < 0.5)\n",
    "\n",
    "            print(f\"Iteration# {i+1}, Total error: {total_error}, Training accuracy: {train_correct_count/len(y_train)}, Testing accuracy: {test_correct_count/len(y_test)}\")\n",
    "\n",
    "    # use trained neural network weights to finds 10 words which may have similar meaning to a target word\n",
    "    def similar_words(self, target, word_index):\n",
    "        \n",
    "        # two words can be considered to have similar meaning if the set of weights connecting one word to all the hidden neurons\n",
    "        # is similar to the set of weights connecting the other word to all the hidden neurons, The weights for any specific word\n",
    "        # is just the row corresponding to that word in the W0 weights matrix. To measure similarity between two words, we will\n",
    "        # compute the euclidean distance between the corresponding two rows of W0  \n",
    "        if(target in word_index):\n",
    "            target_row_index = word_index[target]  \n",
    "            target_weights =  self.layer_1.W[target_row_index]\n",
    "            similarities = Counter()\n",
    "\n",
    "            # measure similarity with all words in the vocabulary, and find the 10 words with the higest similarities\n",
    "            for word,index in word_index.items():\n",
    "                word_weights = self.layer_1.W[index]\n",
    "                diff = target_weights - word_weights \n",
    "                squared_dist = math.sqrt(np.sum(diff * diff)) \n",
    "                \n",
    "                # define negative distance as similarity, means that most similar words will be the least negative (i.e. distance closest to zero) \n",
    "                # also makes it conveninent to extract these values from the Counter object using the most_common method\n",
    "                similarity = -squared_dist\n",
    "                similarities[word] = similarity\n",
    "            \n",
    "            most_similar_words = similarities.most_common(10)\n",
    "            return most_similar_words\n",
    "\n",
    "        else:\n",
    "            print(\"ERROR! Target does not exist in vocabulary!\")\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-1.0 * x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01\n",
      "Training in progress...\n",
      "Iteration# 1, Total error: 2885.648401665013, Training accuracy: 0.8171666666666667, Testing accuracy: 0.855\n",
      "Iteration# 2, Total error: 1678.234589679232, Training accuracy: 0.9072916666666667, Testing accuracy: 0.855\n",
      "Iteration# 3, Total error: 1224.7013356524926, Training accuracy: 0.9335, Testing accuracy: 0.858\n"
     ]
    }
   ],
   "source": [
    "# initialize a three layer network object\n",
    "input_neurons = len(vocab)\n",
    "hidden_neurons = 100\n",
    "output_neurons = 1 \n",
    "net = three_layer_net(input_neurons, hidden_neurons, output_neurons)\n",
    "\n",
    "# preprocess the training and testing datasets\n",
    "nreviews = len(input_dataset)\n",
    "X_train = input_dataset[:nreviews-1000]\n",
    "y_train = target_dataset[:nreviews-1000]\n",
    "X_test = input_dataset[nreviews-1000:]\n",
    "y_test = target_dataset[nreviews-1000:]\n",
    "\n",
    "# train the network with the reviews dataset\n",
    "net.train(X_train, y_train, X_test, y_test, alpha = 0.01, niters = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('excellent', -0.0),\n",
       " ('noir', -0.7785588749677791),\n",
       " ('rare', -0.7875624448592276),\n",
       " ('perfect', -0.7965609387727814),\n",
       " ('superb', -0.805372592806488),\n",
       " ('amazing', -0.8313731968730242),\n",
       " ('wonderfully', -0.8327719264000264),\n",
       " ('subtle', -0.8368666697618214),\n",
       " ('today', -0.8398486723370631),\n",
       " ('incredible', -0.8493702723041733)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.similar_words('excellent', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('awful', -0.0),\n",
       " ('poorly', -0.7333796422359578),\n",
       " ('worst', -0.8426149699529178),\n",
       " ('boring', -0.8509156465208111),\n",
       " ('disappointing', -0.8707427207042696),\n",
       " ('lacks', -0.8851981068712745),\n",
       " ('fails', -0.8890817135905027),\n",
       " ('disappointment', -0.89706320449745),\n",
       " ('waste', -0.9069090786367358),\n",
       " ('mess', -0.9324828669005073)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.similar_words('awful', word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amazing', -0.0),\n",
       " ('loved', -0.7128342465078742),\n",
       " ('noir', -0.7264585180265127),\n",
       " ('rare', -0.7411112604421914),\n",
       " ('brilliant', -0.7514086206069763),\n",
       " ('fascinating', -0.7607812748237435),\n",
       " ('perfect', -0.761149648676041),\n",
       " ('fantastic', -0.7688480503700348),\n",
       " ('touching', -0.770137246064581),\n",
       " ('delightful', -0.7714280593197891)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.similar_words('awful', word_index)\n",
    "\n",
    "net.similar_words('amazing', word_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Training a neural network to fill in the blanks in a phrase`**\n",
    "\n",
    "Next, we will train a neural network to predict a missing word in a phrase (group of consecutive words selected from within a sentence), using the movie reviews data set. The training process involves taking a given phrase from a sentence, removing one of the words and using that word as the target label and optimizing the weights to make this target the most likely prediction. The training is done over every every phrase in a sentenece, over many sentences. In this case, the target label can be any word from the vocabulary, so the output size is very large (compared to the previous example problem of prediciting movie ratings where the output was binary). To make to computations more tractable, we can constrain the output labels to only a (randomly chosen) small subset of the vocabulary. This approximation still yields accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of unique words pulled from every review (map iterates over every review and uses the lambda function to extract all unique words from the review)\n",
    "reviews_unique_words = list(map(lambda x: set(x.split(\" \")), raw_reviews))\n",
    "\n",
    "# now create a vocabulary from all unique words across all the reviews\n",
    "vocab = set()\n",
    "for review in reviews_unique_words:\n",
    "    for word in review:\n",
    "        if(len(word) > 0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# enumerate the words in the vocabulary and store the indices in a dictionary\n",
    "word_index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word_index[word] = i\n",
    "\n",
    "\n",
    "# now create a list of all reviews/sentences \n",
    "reviews_sentences = list(map(lambda x: x.split(\" \"), raw_reviews))\n",
    "\n",
    "# create a list of word indices for the sequence of words in each sentence \n",
    "input_dataset = []\n",
    "concatenated = []\n",
    "for sentence in reviews_sentences:\n",
    "    sentence_indices = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sentence_indices.append(word_index[word])\n",
    "            concatenated.append(word_index[word])\n",
    "        except:\n",
    "            \"\"    \n",
    "    input_dataset.append(sentence_indices)\n",
    "\n",
    "# convert all the concatenated word indices into a numpy array\n",
    "concatenated = np.array(concatenated)\n",
    "\n",
    "# shuffle the odering of the sentences\n",
    "random.shuffle(input_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we design a three layer neural network for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Input layer class: Input layer does not perform any operations\n",
    "'''\n",
    "class input_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    ''' \n",
    "        Input layer forward pass\n",
    "    '''\n",
    "    def forward(self, L_0):\n",
    "        self.L_0 = L_0\n",
    "        return self.L_0\n",
    "    \n",
    "''' \n",
    "    Hidden layer class: Hidden layer performs 2 operations. First it performs matrix multiplication\n",
    "                        of inputs L_0 with weights W_0. Then it operates on this result with the Relu\n",
    "                        function.\n",
    "'''    \n",
    "class hidden_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self, W) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "\n",
    "    ''' \n",
    "        Hidden layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L): \n",
    "        self.L = L\n",
    "        return self.forward_matrix_mult()\n",
    "\n",
    "    def forward_matrix_mult(self):\n",
    "        # due to the sparsity of the input vector (because it has mostly 0s except for a few 1s at the positions corresponding to the indices of the words in the input phrase), instead of matrix multiplication, we just sum up the rows in matrix W0 that correspond to the word indices of the words in the input phrase (like in the previous example)\n",
    "        self.Z = np.sum(self.W[self.L], axis=0, keepdims=True)\n",
    "        #self.Z = (self.Z).reshape(1,(self.W).shape[1])   \n",
    "        return self.Z\n",
    "    \n",
    "    ''' \n",
    "        Hidden layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self, D):\n",
    "        self.backward_matrix_mult(D)\n",
    "\n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW0 = dot-product(L0.T, D) \n",
    "        # since the input vector L0 is just a row vector with 0s everywhere except for a few 1s at the positions corresponding to the indices of the words in the input phrase, the dot-product between L0.T and D is a matrix with most rows filled with zeros except for the rows corresponding to the indices for the input phrase words, which contain a copy of D. So instead of storing this entire matrix, we can only store a single copy of the identical nonzero rows which is just D     \n",
    "        self.W_grad = D\n",
    "\n",
    "    ''' \n",
    "        Gradient descent optimization of hidden layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        # only need to update the weights in the relevant rows (i.e. the weights that contribute to the non-zero input components)\n",
    "        # all the relevant rows get updated with the same gradient row vector (i.e. D)\n",
    "        self.W[self.L] -= alpha * self.W_grad\n",
    "\n",
    "\n",
    "''' \n",
    "    Ouput layer class: Performs two operations, first matrix multiplication of inputs L_1 with weights\n",
    "                       W_1. This result is then operated on by squared error function.  \n",
    "'''\n",
    "class output_layer(object):\n",
    "    \n",
    "    ''' \n",
    "        class constructor\n",
    "    '''\n",
    "\n",
    "    def __init__(self, W) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "\n",
    "    ''' \n",
    "        Output layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L, Y, target_label_indices):\n",
    "        self.L = L\n",
    "        self.Y = Y\n",
    "        self.target_label_indices = target_label_indices\n",
    "        return self.forward_matrix_mult()\n",
    "\n",
    "    def forward_matrix_mult(self):\n",
    "        # since we're only going to be predicting target labels from a small subset of the vocabulary, we only need to multiply with the weights in rows for those secific words. Since we store the transpose of the W1 matrix, we need to tarnsbose back to the original shape before matrix multiplication\n",
    "        self.R = np.dot(self.L, self.W[self.target_label_indices].T) \n",
    "        # apply sigmoid function\n",
    "        self.P = self.R.copy()\n",
    "        self.P = sigmoid(self.P)\n",
    "        \n",
    "        return self.P, self.forward_error()\n",
    " \n",
    "    def forward_error(self):\n",
    "        return np.sum((self.P - self.Y)**2) / self.P.shape[0]\n",
    "\n",
    "    '''     \n",
    "        Output layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self):\n",
    "        return self.backward_error()\n",
    "\n",
    "    def backward_error(self):\n",
    "        # dE/dP\n",
    "        dE_dP = 2*(self.P - self.Y) / self.P.shape[0]\n",
    "        return self.backward_sigmoid(dE_dP)\n",
    "\n",
    "    def backward_sigmoid(self, D):\n",
    "        # dE/dR\n",
    "        dE_dR = D * sigmoid_deriv(self.R) \n",
    "        return self.backward_matrix_mult(dE_dR)\n",
    "\n",
    "\n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW1 \n",
    "        # also take the transpose of this gradient matrix since we're storing the transpose of the weights matrix\n",
    "        #self.W_grad = np.dot((self.L).T, D)\n",
    "        self.W_grad = (np.dot((self.L).T, D)).T\n",
    "        # dE/dL1\n",
    "        # since W is already transposed, we don't need to transpose it inside the dot product\n",
    "        # also only need to include the specific rows for words in the output labels list\n",
    "        #dE_dL = np.dot(D, (self.W).T)\n",
    "        dE_dL = np.dot(D, self.W[self.target_label_indices])\n",
    "        return dE_dL\n",
    "    \n",
    "    ''' \n",
    "        Gradient descent optimization of output layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        # only need to update the rows for the words in the output labels list\n",
    "        self.W[self.target_label_indices] -= alpha * self.W_grad\n",
    "\n",
    "'''\n",
    "    A 3-layer neural network class\n",
    "'''\n",
    "class three_layer_net(object):\n",
    "    ''' \n",
    "        class constructor: Takes in the following parameters- number of neurons in input layer (which is the number of feature attributes for each instance), number of hidden layers (has to be at least 1 and can be arbitrarily large), number of neurons in the output layer (which is the number of target attributes) and gradient descent step-size (alpha)\n",
    "    '''\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons, restart = False) -> None:\n",
    "        self.input_neurons  = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        \n",
    "        np.random.seed(1)\n",
    "        if(not restart):\n",
    "            # initialize weights W0 between input layer and hidden layer \n",
    "            W0 = 0.2*np.random.random(size=(input_neurons, hidden_neurons)) - 0.1\n",
    "        \n",
    "            # initialize weights W1 between hidden layer and output layer \n",
    "            # Note: We initialize this as the transpose of W1, so the first dimension has size of output neurons instead of hidden neurons.\n",
    "            # We do this so that when computing the prediction, we can extract out the specific rows that we want (coressponding to the target labels subset) and then transpose those rows back to the original shape before matrix multiplication with the layer inputs  \n",
    "            W1 = 0.02*np.random.random(size=(output_neurons, hidden_neurons)) - 0.01 \n",
    "        else:\n",
    "            # load weights from file\n",
    "            file_contents = np.load('nlp_weights.npz')\n",
    "            W0 = file_contents['arr_0']\n",
    "            W1 = file_contents['arr_1']\n",
    "            print(\"Neural net restarted with pre-trained weights.\")\n",
    "\n",
    "        # initialize layer objects\n",
    "        self.layer_0 = input_layer()\n",
    "        self.layer_1 = hidden_layer(W0)\n",
    "        self.layer_2 = output_layer(W1)\n",
    "\n",
    "    ''' \n",
    "        neural network forward pass\n",
    "    '''\n",
    "    def forward_net(self, L0, Y, target_label_indices):\n",
    "        # input layer forward pass\n",
    "        self.L0 = self.layer_0.forward(L0) \n",
    "        # hidden layer forward pass \n",
    "        self.L1 = self.layer_1.forward(self.L0) \n",
    "        # output layer forward pass\n",
    "        self.L2, error = self.layer_2.forward(self.L1, Y, target_label_indices) \n",
    "\n",
    "        return self.L2, error\n",
    "\n",
    "    ''' \n",
    "        neural network backward pass\n",
    "    ''' \n",
    "    def backward_net(self):\n",
    "       # output layer backpropagation\n",
    "       D = self.layer_2.backward() \n",
    "       # hidden layer backpropagation\n",
    "       self.layer_1.backward(D) \n",
    "\n",
    "    '''     \n",
    "        weight optimization\n",
    "    '''\n",
    "    def optimize(self, alpha):\n",
    "        # update output layer weights\n",
    "        self.layer_2.update_weights(alpha)\n",
    "        # update hidden layer weights\n",
    "        self.layer_1.update_weights(alpha)\n",
    "\n",
    "    '''     \n",
    "        train the network\n",
    "    ''' \n",
    "    def train(self, sentences, concatenated, word_index, target_size, phrase_half_length, alpha, niters=1):\n",
    "        print(f\"Alpha: {alpha}\")\n",
    "        print(\"Training in progress...\")\n",
    "        #training iterations\n",
    "        for iter in range(niters):\n",
    "            total_error = 0.0\n",
    "            train_correct_count = 0\n",
    "            counter = 0\n",
    "            percent_done = 0\n",
    "            # iterate over sentences\n",
    "            for i in range(len(sentences)):\n",
    "                sentence  = sentences[i]\n",
    "                if((int(i*100/len(sentences))%5) == 0 and (int(i*100/len(sentences)/5) > 0.0) and (int(i*100/len(sentences)) != percent_done)):\n",
    "                    print(f\"Iteration# {iter}, % completed: {int(i*100/len(sentences))}\")\n",
    "                    percent_done = int(i*100/len(sentences))\n",
    "                # iterate over phrases in sentence (i.e. focus on middle word which is going to be the missing word in the phrase)\n",
    "                for j in range(len(sentence)):\n",
    "                    \n",
    "                    # randomly pick a small subset of the words as our target labels (including the focus word itself)\n",
    "                    target_label_indices = [sentence[j]] + (concatenated[np.random.randint(0, len(concatenated),size=target_size).tolist()]).tolist()    \n",
    "\n",
    "                    # input is the list of words in the phrase with the missing word removed (missing word is in the middle word of our phrase)\n",
    "                    lo = max(0, j-phrase_half_length)\n",
    "                    hi = min(j+phrase_half_length, len(sentence))\n",
    "                    phrase_words_indices = sentence[lo:j]  + sentence[j+1:hi] \n",
    "\n",
    "                    #print(f\"j: {j}, lo: {lo}, hi: {hi-1}\")\n",
    "                    #print(f\"Missing word: {sentence[j]}\")\n",
    "                    #print(f\"Phrase words: {phrase_words_indices}\")\n",
    "\n",
    "                    # the target vector is just a vector of length equal to the number of target labels, with 1 at the zeroth position (corresponding to the missing/focus word) and zeros everywhere else\n",
    "                    y = np.zeros(shape=(1,target_size+1))\n",
    "                    y[0,0] = 1\n",
    "                    X = phrase_words_indices\n",
    "\n",
    "                    # forward propagation\n",
    "                    prediction, error = self.forward_net(X, y, target_label_indices)\n",
    "                    total_error += error\n",
    "                    train_correct_count += int(np.argmax(prediction) == np.argmax(y))\n",
    "                 \n",
    "                    # backpropagation\n",
    "                    self.backward_net()\n",
    "\n",
    "                    # weight optimization\n",
    "                    self.optimize(alpha)\n",
    "\n",
    "                    counter += 1\n",
    "\n",
    "            print(f\"Iteration# {iter+1}, Total error: {total_error}, Training accuracy: {train_correct_count/counter}\")\n",
    "\n",
    "            # dump weights to file after every two iterations\n",
    "            if(iter%2 == 0):\n",
    "                np.savez_compressed('nlp_weights.npz', net.layer_1.W, net.layer_2.W)\n",
    "                print(\"Weights saved to file.\")\n",
    "\n",
    "        print(\"Training completed!\")\n",
    "        \n",
    "    # use trained neural network weights to finds 10 words which may have similar meaning to a target word\n",
    "    def similar_words(self, target, word_index):\n",
    "        \n",
    "        # two words can be considered to have similar meaning if the set of weights connecting one word to all the hidden neurons\n",
    "        # is similar to the set of weights connecting the other word to all the hidden neurons, The weights for any specific word\n",
    "        # is just the row corresponding to that word in the W0 weights matrix. To measure similarity between two words, we will\n",
    "        # compute the euclidean distance between the corresponding two rows of W0  \n",
    "        if(target in word_index):\n",
    "            target_row_index = word_index[target]  \n",
    "            target_weights =  self.layer_1.W[target_row_index]\n",
    "            similarities = Counter()\n",
    "\n",
    "            # measure similarity with all words in the vocabulary, and find the 10 words with the higest similarities\n",
    "            for word,index in word_index.items():\n",
    "                word_weights = self.layer_1.W[index]\n",
    "                diff = target_weights - word_weights \n",
    "                squared_dist = math.sqrt(np.sum(diff * diff)) \n",
    "                \n",
    "                # define negative distance as similarity, means that most similar words will be the least negative (i.e. distance closest to zero) \n",
    "                # also makes it conveninent to extract these values from the Counter object using the most_common method\n",
    "                similarity = -squared_dist\n",
    "                similarities[word] = similarity\n",
    "            \n",
    "            most_similar_words = similarities.most_common(10)\n",
    "            return most_similar_words\n",
    "\n",
    "        else:\n",
    "            print(\"ERROR! Target does not exist in vocabulary!\")\n",
    "\n",
    "    # use trained neural network to fill in a missing word in a sentence\n",
    "    def complete_sentence(self, sentence, missing_word, word_index):\n",
    "\n",
    "        # remove missing word from sentence\n",
    "        if missing_word in sentence:\n",
    "            sentence.remove(missing_word)\n",
    "\n",
    "        # convert words in sentence to word indices\n",
    "        sentence_indices = []\n",
    "        for word in sentence:\n",
    "            sentence_indices.append(word_index[word])\n",
    "            \n",
    "        target_size = int(len(vocab))\n",
    "        np.random.seed(1)\n",
    "        #target_label_indices = np.random.randint(0, len(vocab),size=target_size).tolist()\n",
    "        target_label_indices = list(range(len(vocab)))\n",
    "        y = np.zeros(shape=(1,len(target_label_indices)))\n",
    "        # predict the missing word\n",
    "        prediction, error = self.forward_net(sentence_indices, y, target_label_indices)\n",
    "        prediction = prediction[0]\n",
    "\n",
    "        # get the top 5 predictions\n",
    "        top_pos = np.argpartition(prediction, - 5)[-5:]\n",
    "        target_label_indices = np.array(target_label_indices)\n",
    "        top_pred_indices = target_label_indices[top_pos]\n",
    "        #print(f\"Top 5 predicted word indices: {top_pred_indices}\")\n",
    "        \n",
    "        top_five_pred_words = []\n",
    "        for index in top_pred_indices:\n",
    "            top_five_pred_words.append(vocab[index])\n",
    "\n",
    "        return top_five_pred_words\n",
    "\n",
    "    # add and subtract the given words (aka word analogy)\n",
    "    def word_algebra(self,positive_words, negative_words, word_index):\n",
    "\n",
    "        # first normalize the weights\n",
    "        norm = np.sum(self.layer_1.W * self.layer_1.W, axis = 1).reshape(self.layer_1.W.shape[0],1)\n",
    "        normalized_weights = self.layer_1.W * norm\n",
    "\n",
    "        # add the rows of normalized weights corresponding to the input words\n",
    "        word_sum = np.zeros(shape=(self.layer_1.W.shape[1]))\n",
    "        for word in positive_words:\n",
    "            word_sum += normalized_weights[word_index[word]]\n",
    "        for word in negative_words:\n",
    "            word_sum -= normalized_weights[word_index[word]]\n",
    "\n",
    "        # measure similarity of the word_sum with all words in the vocabulary, and find the 10 words with the higest similarities\n",
    "        similarities = Counter()\n",
    "        for word,index in word_index.items():\n",
    "            word_weights = self.layer_1.W[index]\n",
    "            diff = word_sum - word_weights \n",
    "            squared_dist = math.sqrt(np.sum(diff * diff)) \n",
    "            \n",
    "            # define negative distance as similarity, means that most similar words will be the least negative (i.e. distance closest to zero) \n",
    "            # also makes it conveninent to extract these values from the Counter object using the most_common method\n",
    "            similarity = -squared_dist\n",
    "            similarities[word] = similarity\n",
    "        \n",
    "        most_similar_words = similarities.most_common(10)\n",
    "        return most_similar_words\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-1.0 * x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net restarted with pre-trained weights.\n",
      "Alpha: 0.025\n",
      "Training in progress...\n",
      "Iteration# 0, % completed: 5\n",
      "Iteration# 0, % completed: 10\n",
      "Iteration# 0, % completed: 15\n",
      "Iteration# 0, % completed: 20\n",
      "Iteration# 0, % completed: 25\n",
      "Iteration# 0, % completed: 30\n",
      "Iteration# 0, % completed: 35\n",
      "Iteration# 0, % completed: 40\n",
      "Iteration# 0, % completed: 45\n",
      "Iteration# 0, % completed: 50\n",
      "Iteration# 0, % completed: 55\n",
      "Iteration# 0, % completed: 60\n",
      "Iteration# 0, % completed: 65\n",
      "Iteration# 0, % completed: 70\n",
      "Iteration# 0, % completed: 75\n",
      "Iteration# 0, % completed: 80\n",
      "Iteration# 0, % completed: 85\n",
      "Iteration# 0, % completed: 90\n",
      "Iteration# 0, % completed: 95\n",
      "Iteration# 1, Total error: 4977193.592801002, Training accuracy: 0.39376855269955313\n",
      "Weights saved to file.\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# initialize a three layer network object\n",
    "input_neurons = len(vocab)\n",
    "hidden_neurons = 50\n",
    "output_neurons = len(vocab) \n",
    "net = three_layer_net(input_neurons, hidden_neurons, output_neurons, True)\n",
    "\n",
    "# train the network with the reviews dataset\n",
    "net.train(input_dataset, concatenated, word_index, 7, 4, alpha = 0.025, niters = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add and subtract the given words (aka word analogy)\n",
    "def word_algebra(positive_words, negative_words, word_index):\n",
    "\n",
    "    # first normalize the weights\n",
    "    norm = np.sum(net.layer_1.W * net.layer_1.W, axis = 1).reshape(net.layer_1.W.shape[0],1)\n",
    "    normalized_weights = net.layer_1.W * norm\n",
    "\n",
    "    # add the rows of normalized weights corresponding to the input words\n",
    "    word_sum = np.zeros(shape=(net.layer_1.W.shape[1]))\n",
    "    for word in positive_words:\n",
    "        word_sum += normalized_weights[word_index[word]]\n",
    "    for word in negative_words:\n",
    "        word_sum -= normalized_weights[word_index[word]]\n",
    "\n",
    "    # measure similarity of the word_sum with all words in the vocabulary, and find the 10 words with the higest similarities\n",
    "    similarities = Counter()\n",
    "    for word,index in word_index.items():\n",
    "        word_weights = net.layer_1.W[index]\n",
    "        diff = word_sum - word_weights \n",
    "        squared_dist = math.sqrt(np.sum(diff * diff)) \n",
    "        \n",
    "        # define negative distance as similarity, means that most similar words will be the least negative (i.e. distance closest to zero) \n",
    "        # also makes it conveninent to extract these values from the Counter object using the most_common method\n",
    "        similarity = -squared_dist\n",
    "        similarities[word] = similarity\n",
    "    \n",
    "    most_similar_words = similarities.most_common(10)\n",
    "    return most_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence:\n",
      "\n",
      "\t\ti am wearing a ______ shirt\n",
      "\n",
      "Top 5 predictions for missing word: ['mwahaha', 'torn', 'ticklish', 'groener', 'grates']\n",
      "\n",
      "Actual missing word: red\n"
     ]
    }
   ],
   "source": [
    "test_sentence = ['i', 'am' ,'wearing', 'a', 'red', 'shirt']\n",
    "missing_word='red'\n",
    "predictions = net.complete_sentence(test_sentence.copy(), missing_word, word_index)\n",
    "test_sentence[test_sentence.index(missing_word)] = \"______\"\n",
    "print('\\nTest sentence:\\n')\n",
    "print('\\t\\t'+ ' '.join(test_sentence)+'\\n')\n",
    "print(f\"Top 5 predictions for missing word: {predictions}\")\n",
    "print(f\"\\nActual missing word: {missing_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence:\n",
      "\n",
      "\t\tthe dog was ______ at me\n",
      "\n",
      "Top 5 predictions for missing word: ['struthers', 'polay', 'miltonesque', 'peril', 'innate']\n",
      "\n",
      "Actual missing word: barking\n"
     ]
    }
   ],
   "source": [
    "test_sentence = ['the', 'dog' ,'was', 'barking', 'at', 'me']\n",
    "missing_word='barking'\n",
    "predictions = net.complete_sentence(test_sentence.copy(), missing_word, word_index)\n",
    "test_sentence[test_sentence.index(missing_word)] = \"______\"\n",
    "print('\\nTest sentence:\\n')\n",
    "print('\\t\\t'+ ' '.join(test_sentence)+'\\n')\n",
    "print(f\"Top 5 predictions for missing word: {predictions}\")\n",
    "print(f\"\\nActual missing word: {missing_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', -36.26687268900622),\n",
       " ('tragedies', -36.90188368441733),\n",
       " ('klugman', -37.05513813585838),\n",
       " ('winamp', -37.18944830348514),\n",
       " ('heckle', -37.24214967083068),\n",
       " ('natascha', -37.498309466907905),\n",
       " ('oneself', -37.50223207012535),\n",
       " ('unwelcomed', -37.561593863633334),\n",
       " ('sends', -37.581841763468965),\n",
       " ('communities', -37.59451499204579)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.word_algebra(['terrible','good'],['bad'],word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', -0.0),\n",
       " ('enid', -1.6571838192934838),\n",
       " ('kilmer', -1.6759964880539258),\n",
       " ('rehibilitation', -1.6817594529683793),\n",
       " ('withholds', -1.7080757195358722),\n",
       " ('treason', -1.7250651899236749),\n",
       " ('rho', -1.7458010846253016),\n",
       " ('hardness', -1.7471793344933528),\n",
       " ('strivings', -1.749981252717309),\n",
       " ('seboipepe', -1.751881208088541)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.similar_words('woman', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amazing', -0.0),\n",
       " ('incredible', -1.0583268338046543),\n",
       " ('excellent', -1.198017297217095),\n",
       " ('awesome', -1.3299989299668475),\n",
       " ('outstanding', -1.3821164495577636),\n",
       " ('exceptional', -1.5083247701548774),\n",
       " ('extraordinary', -1.6097012692867936),\n",
       " ('astonishing', -1.7085799856837172),\n",
       " ('interesting', -1.7532179810511248),\n",
       " ('admirable', -1.7686110465717226)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.similar_words('amazing', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beautiful', -0.0),\n",
       " ('lovely', -1.3854902070497324),\n",
       " ('gorgeous', -1.4999506403988139),\n",
       " ('vibrant', -1.5413974028249746),\n",
       " ('fantastic', -1.5595608665653098),\n",
       " ('stunning', -1.610193789412003),\n",
       " ('marvelous', -1.671694917057171),\n",
       " ('wonderful', -1.68151737172721),\n",
       " ('bright', -1.708524991453973),\n",
       " ('lush', -1.7213965425013187)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.similar_words('beautiful', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('awful', -0.0),\n",
       " ('abysmal', -1.5808150077587555),\n",
       " ('appalling', -1.5943662703737012),\n",
       " ('awesome', -1.619191395636564),\n",
       " ('ok', -1.6742914967443405),\n",
       " ('dreadful', -1.6791055579462923),\n",
       " ('terrible', -1.6894983350141908),\n",
       " ('laughable', -1.727787534898245),\n",
       " ('okay', -1.7392721192672178),\n",
       " ('incredible', -1.7517186863862413)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.similar_words('awful', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', -0.0),\n",
       " ('boogeyman', -2.078429776894064),\n",
       " ('hawking', -2.1883476971125586),\n",
       " ('bigwig', -2.2379397972456565),\n",
       " ('pinter', -2.2584340769642273),\n",
       " ('gospel', -2.25894116850655),\n",
       " ('blackmailer', -2.2635661618814256),\n",
       " ('occupant', -2.2779849405068626),\n",
       " ('aristocracy', -2.2791282687631047),\n",
       " ('boorish', -2.286695419905272)]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.similar_words('king', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('person', -0.0),\n",
       " ('woman', -1.4941693747963491),\n",
       " ('man', -1.5708201411400173),\n",
       " ('girl', -1.8768443914560258),\n",
       " ('kid', -2.018671045086497),\n",
       " ('guy', -2.0505786002915714),\n",
       " ('priest', -2.062187302557605),\n",
       " ('child', -2.09051224909826),\n",
       " ('soldier', -2.097332338682669),\n",
       " ('farmer', -2.155770412193974)]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.similar_words('person', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('room', -0.0),\n",
       " ('country', -1.8644998078960804),\n",
       " ('carriage', -1.9502230344301816),\n",
       " ('house', -1.969706606249577),\n",
       " ('hallway', -1.988736398822634),\n",
       " ('protest', -1.994633146040767),\n",
       " ('competition', -1.9991688666128418),\n",
       " ('caves', -2.0064093198065165),\n",
       " ('choir', -2.0111217937520167),\n",
       " ('bedroom', -2.0653696196424107)]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.similar_words('room', word_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
