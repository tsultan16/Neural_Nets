{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Simple Neural Network Implementations`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1:** We are given a set of instances with numerical attributes, and a numerical label/target value (i.e. ground truth) for each instance. POur goal is to create a neural network that can predict the label for any given instance. In our simplest neural network model, the prediction is just a linear combination of the attributes. The network is trained by optimizing the weights (i.e. constant co-efficients) of this linear combination using gradient descent.\n",
    "\n",
    "To demo this model, we will use the \"traffic lights\" example, where we have three traffic lights, the state of each light is either `on` or `off` (i.e. 1 or 0) and the corresponding label is either walk or stop (1 or 0). The training dataset is contrived such that there is a strong correlation between the second attribute/light and the target. We would therefore expect the second weight to be much larger than the other two weights after the model has been trained sufficiently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: [0.51785724 1.13697674 0.09572198]\n",
      "Iteration# 1, Updated weights: [ 0.26293672  1.0433111  -0.14888024], Total error: 0.6615609879896762\n",
      "Iteration# 2, Updated weights: [ 0.18739697  1.05547619 -0.14459176], Total error: 0.10844138636634067\n",
      "Iteration# 3, Updated weights: [ 0.14318657  1.06375359 -0.12441057], Total error: 0.0600211648479321\n",
      "Iteration# 4, Updated weights: [ 0.11252447  1.06489327 -0.10610819], Total error: 0.037484116369458226\n",
      "Iteration# 5, Updated weights: [ 0.09023603  1.06187453 -0.09053287], Total error: 0.02505888033029876\n",
      "Iteration# 6, Updated weights: [ 0.07353676  1.05682564 -0.07731293], Total error: 0.017469992762678667\n",
      "Iteration# 7, Updated weights: [ 0.06069012  1.05099845 -0.06606951], Total error: 0.012471551424589018\n",
      "Iteration# 8, Updated weights: [ 0.05057574  1.04509218 -0.05648967], Total error: 0.009017246075032721\n",
      "Iteration# 9, Updated weights: [ 0.04245559  1.03947643 -0.04831628], Total error: 0.006563097145226049\n",
      "Iteration# 10, Updated weights: [ 0.03583245  1.03432777 -0.04133613], Total error: 0.004793170921884578\n",
      "Iteration# 11, Updated weights: [ 0.0303627   1.02971213 -0.03537087], Total error: 0.0035066265606595003\n",
      "Iteration# 12, Updated weights: [ 0.02580221  1.02563397 -0.03027042], Total error: 0.00256765532172254\n",
      "Iteration# 13, Updated weights: [ 0.02197248  1.02206544 -0.02590787], Total error: 0.0018809424713557036\n",
      "Iteration# 14, Updated weights: [ 0.01873931  1.01896337 -0.02217552], Total error: 0.0013781936819920644\n",
      "Iteration# 15, Updated weights: [ 0.01599911  1.01627902 -0.01898176], Total error: 0.0010099334393356345\n",
      "Iteration# 16, Updated weights: [ 0.01367015  1.01396348 -0.01624851], Total error: 0.0007401143259343849\n",
      "Iteration# 17, Updated weights: [ 0.01168666  1.01197052 -0.01390917], Total error: 0.0005423958352716494\n",
      "Iteration# 18, Updated weights: [ 0.00999491  1.01025787 -0.01190684], Total error: 0.00039750204308331224\n",
      "Iteration# 19, Updated weights: [ 0.00855045  1.00878774 -0.01019288], Total error: 0.00029131639352601284\n",
      "Iteration# 20, Updated weights: [ 0.00731622  1.00752678 -0.00872571], Total error: 0.00021349691385193732\n",
      "Iteration# 21, Updated weights: [ 0.00626104  1.00644582 -0.00746978], Total error: 0.00015646554707253197\n",
      "Iteration# 22, Updated weights: [ 0.00535858  1.00551953 -0.00639465], Total error: 0.00011466898883669474\n",
      "Iteration# 23, Updated weights: [ 0.00458654  1.00472601 -0.00547428], Total error: 8.403752455464298e-05\n",
      "Iteration# 24, Updated weights: [ 0.00392594  1.00404635 -0.00468638], Total error: 6.158862092454569e-05\n",
      "Iteration# 25, Updated weights: [ 0.0033606   1.00346431 -0.00401189], Total error: 4.513647679064614e-05\n",
      "Iteration# 26, Updated weights: [ 0.00287675  1.00296592 -0.00343449], Total error: 3.3079183764533524e-05\n",
      "Iteration# 27, Updated weights: [ 0.00246261  1.00253918 -0.00294018], Total error: 2.4242749416297827e-05\n",
      "Iteration# 28, Updated weights: [ 0.00210812  1.00217381 -0.00251702], Total error: 1.7766788330288532e-05\n",
      "Iteration# 29, Updated weights: [ 0.00180467  1.00186099 -0.00215477], Total error: 1.302074878221326e-05\n",
      "Iteration# 30, Updated weights: [ 0.00154491  1.00159318 -0.00184465], Total error: 9.542517682334737e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# traffic lights dataset (each row is and instance, the first three coulumns are the attributes and the last column is the label)\n",
    "traffic_lights = np.array([ [1, 0, 1, 0], \n",
    "                            [0, 1, 1, 1],\n",
    "                            [0, 0, 1, 0],\n",
    "                            [1, 1, 1, 1],\n",
    "                            [0, 1, 1, 1],\n",
    "                            [1, 0, 1, 0]] )\n",
    "\n",
    "# number of gradient descent iterations\n",
    "niters = 30\n",
    "\n",
    "# learning rate (i.e gradient descent step-size)\n",
    "alpha = 0.1\n",
    "\n",
    "# initialize random weights\n",
    "weights = np.random.randn(3) \n",
    "print(f\"Initial weights: {weights}\")\n",
    "\n",
    "# train the network\n",
    "for i in range(niters):\n",
    "\n",
    "    total_error = 0.0\n",
    "    for j in range(traffic_lights.shape[0]):\n",
    "        \n",
    "        input = traffic_lights[j, :-1]\n",
    "        target = traffic_lights[j, -1]\n",
    "         \n",
    "        # compute prediction\n",
    "        prediction = np.dot(weights, input) \n",
    "        \n",
    "        # compute squared error\n",
    "        error = (prediction - target)**2\n",
    "        total_error += error\n",
    "\n",
    "        # compute gradient of error w.r.t. weights\n",
    "        grad = 2 * (prediction - target) * input\n",
    "\n",
    "        # optimize weights using gradient descent\n",
    "        weights -= alpha * grad\n",
    "\n",
    "    print(f\"Iteration# {i+1}, Updated weights: {weights}, Total error: {total_error}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2:** We will now build a neural network with one hidden-layer, between the input and output layers, and introduce non-linearity via a relu activation function. This three layer network has two sets of weights, both of which are optimized during training. That training phase has two separate stages: a forward propagation and a backward propagation. Forward propagation involves computing the output at the end of each layer and sending them forward to be the inputs for the next layer. Backward propagatiuon involves computing the derivatives w.r.t. the inputs of the operation performed at each layer, composing these derivatives with those obtained from the next layer, and then sending these back to the previous layer. This composition of derivatives from the current layer with derivatives from the next layer is simply the application of the chain-rule for derivatives.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "    Input layer class: Input layer does not perform any operations\n",
    "'''\n",
    "class input_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    ''' \n",
    "        Input layer forward pass\n",
    "    '''\n",
    "    def forward(self, L_0):\n",
    "        self.L_0 = L_0\n",
    "        return self.L_0\n",
    "    \n",
    "''' \n",
    "    Hidden layer class: Hidden layer performs 2 operations. First it performs matrix multiplication\n",
    "                        of inputs L_0 with weights W_0. Then it operates on this result with the Relu\n",
    "                        function.\n",
    "'''    \n",
    "class hidden_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self, W, activation) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "        self.activation = activation\n",
    "        print(f\"Hidden layer activation function: {activation}\")\n",
    "\n",
    "    ''' \n",
    "        Hidden layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L, dropout): \n",
    "        self.L = L\n",
    "        return self.forward_matrix_mult(dropout)\n",
    "\n",
    "    def forward_matrix_mult(self, dropout):\n",
    "        self.Z =  np.dot(self.L, self.W)\n",
    "        self.dropout = dropout \n",
    "        if(self.dropout):\n",
    "            # generate a random dropout mask with rougly equal numbers of 0s and 1s\n",
    "            self.dropout_mask = np.random.randint(0,2,size=(self.Z.shape))\n",
    "           \n",
    "        if(self.activation == \"relu\"):\n",
    "            if(self.dropout):\n",
    "              # multiply by a factor of 2 to compensate for rougly 1/2 the neurons being turned off by the masking\n",
    "              return 2 * self.dropout_mask * self.forward_relu()\n",
    "            else:\n",
    "                return self.forward_relu()\n",
    "    \n",
    "        elif(self.activation == \"sigmoid\"):\n",
    "            if(self.dropout):\n",
    "              return 2 * self.dropout_mask * self.forward_sigmoid()\n",
    "            else:\n",
    "                return self.forward_sigmoid()\n",
    "    \n",
    "        elif(self.activation == \"tanh\"):\n",
    "            if(self.dropout):\n",
    "              return 2 * self.dropout_mask * self.forward_tanh()\n",
    "            else:\n",
    "                return self.forward_tanh()\n",
    "    \n",
    "    def forward_relu(self):\n",
    "        return Relu(self.Z)\n",
    "    \n",
    "    def forward_sigmoid(self):\n",
    "        return sigmoid(self.Z)\n",
    "   \n",
    "    def forward_tanh(self):\n",
    "        return tanh(self.Z)\n",
    "    \n",
    "    ''' \n",
    "        Hidden layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self, D):\n",
    "        if(self.activation == \"relu\"):\n",
    "           self.backward_relu(D)\n",
    "        elif(self.activation == \"sigmoid\"):\n",
    "           self.backward_sigmoid(D)\n",
    "        elif(self.activation == \"tanh\"):\n",
    "           self.backward_tanh(D)\n",
    "\n",
    "    def backward_relu(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * Relu_deriv(self.Z) \n",
    "        self.backward_matrix_mult(dE_dZ)\n",
    "    \n",
    "    def backward_sigmoid(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * sigmoid_deriv(self.Z) \n",
    "        self.backward_matrix_mult(dE_dZ)\n",
    "    \n",
    "    def backward_tanh(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * tanh_deriv(self.Z) \n",
    "        self.backward_matrix_mult(dE_dZ)\n",
    "    \n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW0\n",
    "        if(self.dropout):\n",
    "            self.W_grad = np.dot((self.L).T, self.dropout_mask * D)\n",
    "        else:\n",
    "            self.W_grad = np.dot((self.L).T, D)\n",
    "\n",
    "    ''' \n",
    "        Gradient descent optimization of hidden layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        self.W -= alpha * self.W_grad\n",
    "\n",
    "       \n",
    "\n",
    "''' \n",
    "    Ouput layer class: Performs two operations, first matrix multiplication of inputs L_1 with weights\n",
    "                       W_1. This result is then operated on by squared error function.  \n",
    "'''\n",
    "class output_layer(object):\n",
    "    \n",
    "    ''' \n",
    "        class constructor\n",
    "    '''\n",
    "\n",
    "    def __init__(self, W) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "\n",
    "    ''' \n",
    "        Output layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L, Y):\n",
    "        self.L = L\n",
    "        self.Y = Y\n",
    "        return self.forward_matrix_mult()\n",
    "\n",
    "    def forward_matrix_mult(self):\n",
    "        self.P = np.dot(self.L, self.W) \n",
    "        return self.P, self.forward_error()\n",
    " \n",
    "    def forward_error(self):\n",
    "        return np.sum((self.P - self.Y)**2) / self.P.shape[0]\n",
    "\n",
    "    '''     \n",
    "        Output layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self):\n",
    "        return self.backward_error()\n",
    "\n",
    "    def backward_error(self):\n",
    "        # dE/dP\n",
    "        dE_dP = 2*(self.P - self.Y) / self.P.shape[0]\n",
    "        return self.backward_matrix_mult(dE_dP)\n",
    "\n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW1\n",
    "        self.W_grad = np.dot((self.L).T, D)\n",
    "        # dE/dL1\n",
    "        dE_dL = np.dot(D, (self.W).T)\n",
    "        return dE_dL\n",
    "    \n",
    "    ''' \n",
    "        Gradient descent optimization of output layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        self.W -= alpha * self.W_grad\n",
    "\n",
    "'''\n",
    "    A 3-layer neural network class\n",
    "'''\n",
    "class three_layer_net(object):\n",
    "    ''' \n",
    "        class constructor: Takes in the following parameters- number of neurons in input layer (which is the number of feature attributes for each instance), number of hidden layers (has to be at least 1 and can be arbitrarily large), number of neurons in the output layer (which is the number of target attributes) and gradient descent step-size (alpha)\n",
    "    '''\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons, alpha, hidden_layer_activation = \"relu\") -> None:\n",
    "        self.input_neurons  = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        np.random.seed(1)\n",
    "        # initialize weights W0 between input layer and hidden layer \n",
    "        W0 = 0.2*np.random.random(size=(input_neurons, hidden_neurons)) - 0.1\n",
    "        # initialize weights W1 between hidden layer and output layer\n",
    "        W1 = 0.2*np.random.random(size=(hidden_neurons, output_neurons)) - 0.1 \n",
    "\n",
    "        # initialize layer objects\n",
    "        self.layer_0 = input_layer()\n",
    "        self.layer_1 = hidden_layer(W0, activation=hidden_layer_activation)\n",
    "        self.layer_2 = output_layer(W1)\n",
    "\n",
    "    ''' \n",
    "        neural network forward pass\n",
    "    '''\n",
    "    def forward_net(self, L0, Y, dropout):\n",
    "        # input layer forward pass\n",
    "        self.L0 = self.layer_0.forward(L0) \n",
    "        # hidden layer forward pass \n",
    "        self.L1 = self.layer_1.forward(self.L0, dropout) \n",
    "        # output layer forward pass\n",
    "        self.L2, error = self.layer_2.forward(self.L1, Y) \n",
    "\n",
    "        return self.L2, error\n",
    "\n",
    "    ''' \n",
    "        neural network backward pass\n",
    "    ''' \n",
    "    def backward_net(self):\n",
    "       # output layer backpropagation\n",
    "       D = self.layer_2.backward() \n",
    "       # hidden layer backpropagation\n",
    "       self.layer_1.backward(D) \n",
    "\n",
    "    '''     \n",
    "        weight optimization\n",
    "    '''\n",
    "    def optimize(self):\n",
    "        # update output layer weights\n",
    "        self.layer_2.update_weights(self.alpha)\n",
    "        # update hidden layer weights\n",
    "        self.layer_1.update_weights(self.alpha)\n",
    "\n",
    "    '''     \n",
    "        train the network\n",
    "    ''' \n",
    "    def train(self, X_train, y_train, X_test, y_test, batch_size=1, niters=1, dropout=False):\n",
    "        print(f\"Dropout Enabled: {dropout}\")\n",
    "        print(\"Training in progress...\")\n",
    "        #training iterations\n",
    "        for i in range(niters):\n",
    "            total_error = 0.0\n",
    "            train_correct_count = 0\n",
    "            # train using batch of instances\n",
    "            for j in range(int(X_train.shape[0]/batch_size)):\n",
    "\n",
    "                lo = j * batch_size\n",
    "                hi = min((j+1) * batch_size, X_train.shape[0])\n",
    "\n",
    "                X = X_train[lo:hi]\n",
    "                y = y_train[lo:hi]\n",
    "\n",
    "                # forward propagation\n",
    "                prediction, error = self.forward_net(X, y, dropout)\n",
    "                total_error += error\n",
    "                \n",
    "                for k in range(hi-lo):\n",
    "                    train_correct_count += int(np.argmax(prediction[k]) == np.argmax(y[k]))\n",
    "                \n",
    "                #if(i == (niters-1)):\n",
    "                #    print(f\"Instance# {j+1}, Target: {y}, Prediction: {prediction}\")\n",
    "\n",
    "                # backpropagation\n",
    "                self.backward_net()\n",
    "\n",
    "                # weight optimization\n",
    "                self.optimize()\n",
    "\n",
    "            # predict using test instances\n",
    "            test_correct_count = 0\n",
    "            for j in range(X_test.shape[0]):\n",
    "                X = X_test[j:j+1]\n",
    "                y = y_test[j:j+1]\n",
    "\n",
    "                # forward propagation\n",
    "                prediction, error = self.forward_net(X, y, dropout=False)\n",
    "                test_correct_count += int(np.argmax(prediction) == np.argmax(y))\n",
    "\n",
    "            print(f\"Iteration# {i+1}, Total error: {total_error}, Training accuracy: {train_correct_count/len(y_train)}, Testing accuracy: {test_correct_count/len(y_test)}\")\n",
    "\n",
    "# Relu function\n",
    "def Relu(x):\n",
    "    return x*(x > 0)\n",
    "\n",
    "# Relu derivative function\n",
    "def Relu_deriv(x):\n",
    "    return (x > 0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return (1.0 - np.tanh(x)**2)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-1.0 * x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x)) \n",
    "\n",
    "def softmax(x): \n",
    "    ex = np.exp(x)\n",
    "    return ex/np.sum(ex, axis = 1, keepdims = True)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this 3 layer network using the same traffic lights dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 2 required positional arguments: 'X_test' and 'y_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14320\\330548311.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# train the net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mthree_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mniters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 2 required positional arguments: 'X_test' and 'y_test'"
     ]
    }
   ],
   "source": [
    "# traffic lights dataset (each row is and instance, the first three coulumns are the attributes and the last column is the label)\n",
    "traffic_lights = np.array([ [1, 0, 1, 0], \n",
    "                            [0, 1, 1, 1],\n",
    "                            [0, 0, 1, 0],\n",
    "                            [1, 1, 1, 1],\n",
    "                            [0, 1, 1, 1],\n",
    "                            [1, 0, 1, 0]] )\n",
    "\n",
    "# dataset preprocessing\n",
    "X_train = traffic_lights[:,:-1]\n",
    "y_train = traffic_lights[:,-1:]\n",
    "\n",
    "# initialize a three layer neural net object\n",
    "three_net = three_layer_net(input_neurons=X_train.shape[1], hidden_neurons=4, output_neurons=1, alpha=0.5)\n",
    "\n",
    "# train the net\n",
    "three_net.train(X_train, y_train, batch_size=2, niters=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our 3 layer model with the `MNIST dataset` of handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (60000, 28, 28)\n",
      "y_train shape = (60000,)\n",
      "X_test shape = (10000, 28, 28)\n",
      "y_test shape = (10000,)\n",
      "Hidden layer activation function: sigmoid\n",
      "Dropout Enabled: True\n",
      "Training in progress...\n",
      "Iteration# 1, Total error: 27.892238970332603, Training accuracy: 0.101, Testing accuracy: 0.1535\n",
      "Iteration# 2, Total error: 24.68310967788722, Training accuracy: 0.109, Testing accuracy: 0.155\n",
      "Iteration# 3, Total error: 23.519606153535314, Training accuracy: 0.11, Testing accuracy: 0.1613\n",
      "Iteration# 4, Total error: 23.120623936091757, Training accuracy: 0.111, Testing accuracy: 0.1715\n",
      "Iteration# 5, Total error: 22.407531944266072, Training accuracy: 0.105, Testing accuracy: 0.1903\n",
      "Iteration# 6, Total error: 22.027456418336023, Training accuracy: 0.108, Testing accuracy: 0.2088\n",
      "Iteration# 7, Total error: 21.4193372072587, Training accuracy: 0.116, Testing accuracy: 0.2305\n",
      "Iteration# 8, Total error: 21.014369525574264, Training accuracy: 0.118, Testing accuracy: 0.2562\n",
      "Iteration# 9, Total error: 20.553302630265527, Training accuracy: 0.136, Testing accuracy: 0.2602\n",
      "Iteration# 10, Total error: 20.236411852153775, Training accuracy: 0.14, Testing accuracy: 0.2932\n",
      "Iteration# 11, Total error: 20.058342502121604, Training accuracy: 0.123, Testing accuracy: 0.3313\n",
      "Iteration# 12, Total error: 19.72171090434433, Training accuracy: 0.122, Testing accuracy: 0.3215\n",
      "Iteration# 13, Total error: 19.385630943540296, Training accuracy: 0.15, Testing accuracy: 0.326\n",
      "Iteration# 14, Total error: 19.356496913694365, Training accuracy: 0.125, Testing accuracy: 0.3638\n",
      "Iteration# 15, Total error: 19.09781260708388, Training accuracy: 0.152, Testing accuracy: 0.3402\n",
      "Iteration# 16, Total error: 18.987858388014747, Training accuracy: 0.153, Testing accuracy: 0.3679\n",
      "Iteration# 17, Total error: 18.66595320433646, Training accuracy: 0.161, Testing accuracy: 0.3727\n",
      "Iteration# 18, Total error: 18.43468571464573, Training accuracy: 0.162, Testing accuracy: 0.3717\n",
      "Iteration# 19, Total error: 18.366364779558346, Training accuracy: 0.182, Testing accuracy: 0.3429\n",
      "Iteration# 20, Total error: 18.365594749692125, Training accuracy: 0.17, Testing accuracy: 0.372\n",
      "Iteration# 21, Total error: 18.239480917095914, Training accuracy: 0.184, Testing accuracy: 0.4246\n",
      "Iteration# 22, Total error: 18.166440312676627, Training accuracy: 0.169, Testing accuracy: 0.3565\n",
      "Iteration# 23, Total error: 17.886006673592174, Training accuracy: 0.207, Testing accuracy: 0.3935\n",
      "Iteration# 24, Total error: 18.056566505134718, Training accuracy: 0.179, Testing accuracy: 0.4213\n",
      "Iteration# 25, Total error: 17.92886517584628, Training accuracy: 0.19, Testing accuracy: 0.429\n",
      "Iteration# 26, Total error: 17.87275911252392, Training accuracy: 0.184, Testing accuracy: 0.4272\n",
      "Iteration# 27, Total error: 17.80612851660043, Training accuracy: 0.198, Testing accuracy: 0.4298\n",
      "Iteration# 28, Total error: 17.685824818263782, Training accuracy: 0.202, Testing accuracy: 0.4194\n",
      "Iteration# 29, Total error: 17.63705119394382, Training accuracy: 0.201, Testing accuracy: 0.4321\n",
      "Iteration# 30, Total error: 17.71139670123319, Training accuracy: 0.206, Testing accuracy: 0.4229\n",
      "Iteration# 31, Total error: 17.683608399694798, Training accuracy: 0.214, Testing accuracy: 0.4295\n",
      "Iteration# 32, Total error: 17.47257829788386, Training accuracy: 0.237, Testing accuracy: 0.4078\n",
      "Iteration# 33, Total error: 17.39346414057798, Training accuracy: 0.238, Testing accuracy: 0.4685\n",
      "Iteration# 34, Total error: 17.53632637027484, Training accuracy: 0.231, Testing accuracy: 0.451\n",
      "Iteration# 35, Total error: 17.451456541950396, Training accuracy: 0.24, Testing accuracy: 0.4449\n",
      "Iteration# 36, Total error: 17.348195466146166, Training accuracy: 0.241, Testing accuracy: 0.4134\n",
      "Iteration# 37, Total error: 17.33295890722543, Training accuracy: 0.22, Testing accuracy: 0.4739\n",
      "Iteration# 38, Total error: 17.33191436091879, Training accuracy: 0.24, Testing accuracy: 0.4664\n",
      "Iteration# 39, Total error: 17.50469123737052, Training accuracy: 0.226, Testing accuracy: 0.4521\n",
      "Iteration# 40, Total error: 17.44407083559203, Training accuracy: 0.227, Testing accuracy: 0.4047\n",
      "Iteration# 41, Total error: 17.386099155834366, Training accuracy: 0.24, Testing accuracy: 0.445\n",
      "Iteration# 42, Total error: 17.343954581794016, Training accuracy: 0.244, Testing accuracy: 0.4518\n",
      "Iteration# 43, Total error: 17.123803939715472, Training accuracy: 0.262, Testing accuracy: 0.4703\n",
      "Iteration# 44, Total error: 17.325125060058486, Training accuracy: 0.241, Testing accuracy: 0.4584\n",
      "Iteration# 45, Total error: 17.281347764408807, Training accuracy: 0.229, Testing accuracy: 0.4299\n",
      "Iteration# 46, Total error: 17.25133507047802, Training accuracy: 0.244, Testing accuracy: 0.4771\n",
      "Iteration# 47, Total error: 17.247679398910243, Training accuracy: 0.261, Testing accuracy: 0.4619\n",
      "Iteration# 48, Total error: 17.352419487296544, Training accuracy: 0.249, Testing accuracy: 0.4682\n",
      "Iteration# 49, Total error: 17.105447600709006, Training accuracy: 0.257, Testing accuracy: 0.4669\n",
      "Iteration# 50, Total error: 17.151633075296232, Training accuracy: 0.248, Testing accuracy: 0.4581\n",
      "Iteration# 51, Total error: 17.398473490246683, Training accuracy: 0.238, Testing accuracy: 0.4744\n",
      "Iteration# 52, Total error: 17.239463610761, Training accuracy: 0.238, Testing accuracy: 0.4649\n",
      "Iteration# 53, Total error: 17.20080221593883, Training accuracy: 0.253, Testing accuracy: 0.4674\n",
      "Iteration# 54, Total error: 17.107019872548562, Training accuracy: 0.263, Testing accuracy: 0.4691\n",
      "Iteration# 55, Total error: 17.203861572938187, Training accuracy: 0.249, Testing accuracy: 0.4635\n",
      "Iteration# 56, Total error: 17.24021412111508, Training accuracy: 0.242, Testing accuracy: 0.4716\n",
      "Iteration# 57, Total error: 17.007985149434823, Training accuracy: 0.26, Testing accuracy: 0.4639\n",
      "Iteration# 58, Total error: 17.099123873938243, Training accuracy: 0.252, Testing accuracy: 0.4797\n",
      "Iteration# 59, Total error: 17.04353003700908, Training accuracy: 0.286, Testing accuracy: 0.4695\n",
      "Iteration# 60, Total error: 17.135737748423598, Training accuracy: 0.268, Testing accuracy: 0.4977\n",
      "Iteration# 61, Total error: 17.18114898910896, Training accuracy: 0.236, Testing accuracy: 0.4768\n",
      "Iteration# 62, Total error: 17.242018944619844, Training accuracy: 0.251, Testing accuracy: 0.4837\n",
      "Iteration# 63, Total error: 16.974043641076662, Training accuracy: 0.261, Testing accuracy: 0.4678\n",
      "Iteration# 64, Total error: 17.07565283497726, Training accuracy: 0.255, Testing accuracy: 0.4733\n",
      "Iteration# 65, Total error: 17.157031348990884, Training accuracy: 0.256, Testing accuracy: 0.4859\n",
      "Iteration# 66, Total error: 17.07408322006731, Training accuracy: 0.257, Testing accuracy: 0.4998\n",
      "Iteration# 67, Total error: 17.06017694050309, Training accuracy: 0.268, Testing accuracy: 0.4774\n",
      "Iteration# 68, Total error: 17.065190965869434, Training accuracy: 0.269, Testing accuracy: 0.5025\n",
      "Iteration# 69, Total error: 16.902930182446305, Training accuracy: 0.29, Testing accuracy: 0.4896\n",
      "Iteration# 70, Total error: 17.035895223859782, Training accuracy: 0.284, Testing accuracy: 0.4572\n",
      "Iteration# 71, Total error: 17.00811523116614, Training accuracy: 0.278, Testing accuracy: 0.5\n",
      "Iteration# 72, Total error: 17.045267649706606, Training accuracy: 0.277, Testing accuracy: 0.4815\n",
      "Iteration# 73, Total error: 16.85671415080699, Training accuracy: 0.275, Testing accuracy: 0.4883\n",
      "Iteration# 74, Total error: 17.083640493600853, Training accuracy: 0.274, Testing accuracy: 0.4847\n",
      "Iteration# 75, Total error: 16.935005069126124, Training accuracy: 0.277, Testing accuracy: 0.501\n",
      "Iteration# 76, Total error: 16.872637946659882, Training accuracy: 0.278, Testing accuracy: 0.4938\n",
      "Iteration# 77, Total error: 16.954787647435495, Training accuracy: 0.283, Testing accuracy: 0.4748\n",
      "Iteration# 78, Total error: 16.982361100666285, Training accuracy: 0.27, Testing accuracy: 0.4807\n",
      "Iteration# 79, Total error: 16.97761953546832, Training accuracy: 0.247, Testing accuracy: 0.5148\n",
      "Iteration# 80, Total error: 16.932185893611994, Training accuracy: 0.291, Testing accuracy: 0.4927\n",
      "Iteration# 81, Total error: 16.854437157291258, Training accuracy: 0.293, Testing accuracy: 0.5031\n",
      "Iteration# 82, Total error: 16.94333357295161, Training accuracy: 0.266, Testing accuracy: 0.4938\n",
      "Iteration# 83, Total error: 16.847698548824987, Training accuracy: 0.298, Testing accuracy: 0.4964\n",
      "Iteration# 84, Total error: 16.807854491393957, Training accuracy: 0.292, Testing accuracy: 0.4886\n",
      "Iteration# 85, Total error: 16.933748738824622, Training accuracy: 0.269, Testing accuracy: 0.4963\n",
      "Iteration# 86, Total error: 16.88263104651929, Training accuracy: 0.287, Testing accuracy: 0.5002\n",
      "Iteration# 87, Total error: 16.702407236185856, Training accuracy: 0.3, Testing accuracy: 0.4877\n",
      "Iteration# 88, Total error: 16.984124044503428, Training accuracy: 0.272, Testing accuracy: 0.4937\n",
      "Iteration# 89, Total error: 16.80081733280675, Training accuracy: 0.28, Testing accuracy: 0.5062\n",
      "Iteration# 90, Total error: 16.955381434230496, Training accuracy: 0.27, Testing accuracy: 0.5089\n",
      "Iteration# 91, Total error: 16.795509459526553, Training accuracy: 0.306, Testing accuracy: 0.4997\n",
      "Iteration# 92, Total error: 16.763731934243893, Training accuracy: 0.281, Testing accuracy: 0.5078\n",
      "Iteration# 93, Total error: 16.783841775799083, Training accuracy: 0.304, Testing accuracy: 0.4725\n",
      "Iteration# 94, Total error: 16.71827175869273, Training accuracy: 0.288, Testing accuracy: 0.4814\n",
      "Iteration# 95, Total error: 16.825083171123048, Training accuracy: 0.295, Testing accuracy: 0.5116\n",
      "Iteration# 96, Total error: 16.761476991023063, Training accuracy: 0.295, Testing accuracy: 0.4989\n",
      "Iteration# 97, Total error: 16.859833674345918, Training accuracy: 0.289, Testing accuracy: 0.498\n",
      "Iteration# 98, Total error: 16.91396668708457, Training accuracy: 0.28, Testing accuracy: 0.4953\n",
      "Iteration# 99, Total error: 16.79734809764069, Training accuracy: 0.3, Testing accuracy: 0.5018\n",
      "Iteration# 100, Total error: 16.783183579235562, Training accuracy: 0.306, Testing accuracy: 0.5053\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    MNIST dataset of handwritten digits:\n",
    "\n",
    "    Each observation is an image. The input values per image are 28x28 pixels (i.e. 784 features/inputs per observation).\n",
    "'''\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # x values contain image pixels (i.e. features) and y vaues are the corresponding labels\n",
    "\n",
    "print(f\"X_train shape = {X_train.shape}\")\n",
    "print(f\"y_train shape = {y_train.shape}\")\n",
    "print(f\"X_test shape = {X_test.shape}\")\n",
    "print(f\"y_test shape = {y_test.shape}\")\n",
    "\n",
    "# flatten image pixels array\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]) \n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]) \n",
    "\n",
    "# normalize of pixel values from (0,255) to (0,1)\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one-hot encode the labels\n",
    "y_train_onehot = np.zeros(shape=(y_train.shape[0], 10))\n",
    "y_test_onehot = np.zeros(shape=(y_test.shape[0], 10))\n",
    "\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_train_onehot[i, y_train[i]] = 1\n",
    "\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_onehot[i, y_test[i]] = 1\n",
    "\n",
    "# training dataset preparation\n",
    "training_images = X_train[0:1000]\n",
    "training_labels = y_train_onehot[0:1000]\n",
    "testing_images = X_test\n",
    "testing_labels = y_test_onehot\n",
    "\n",
    "# initialize a three layer neural net object\n",
    "three_net = three_layer_net(input_neurons=training_images.shape[1], hidden_neurons=50, output_neurons=training_labels.shape[1], alpha=0.005, hidden_layer_activation=\"tanh\")\n",
    "\n",
    "# train the net\n",
    "three_net.train(training_images, training_labels, testing_images, testing_labels, batch_size=50, niters=100, dropout=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
