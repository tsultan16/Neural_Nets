{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor Neural Network Framework** \n",
    "\n",
    "In this framework, vectors and matrices are represened by a generalized `Tensor` class. A tensor object contains the data for the vector/matrix, a unique identifier and a list of Tensor operation methods. It also includes information pertaining to how the tensor was created, e.g. if it was created by a tensor operation from other tensors, then we would call it a `child` of those `parent tensors`. (So these Tensors can be considered to form the `nodes` of a `tree`-like hierarchical structure, with data being transmitted across the node edges during forward and backward propagation). Finally, the tensor object also contains a method for computing and backpropagating deriviatives, this feature can be turned on by setting the `autograd` property to `True`. The backpropagtion occurs recursively over all the ancestors of that Tensor and stops when a Tensor which does not have any parents is reached. Any given tensor will wait until it has recieved and accumulated the backpropagted derivatives from all it's children and then it will backpropagate it's gradient to it's parents.   \n",
    "\n",
    "In addition to this Tensor class, we also create a base `Layer` class, and define a `Linear` Layer sub-class which represents a linear layer in a neural network, i.e. it has a matrix of weights and it takes a vector of input neurons and multiplies it to the weights matrix resulting in a vector of output neurons.\n",
    "\n",
    "We also create sub-classes for `non-lineararity layers` which take a vector of input neurons and operates on this vector with a non-linear function such as `sigmoid` or `relu`. Similarly, we also have a `loss function layer` for computing error/loss for a given target and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data, creators=None, creation_op=None, autograd=False, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        if(id == None):\n",
    "            id = np.random.randint(0,100000)\n",
    "        self.id = id\n",
    "        self.children = {}\n",
    "        if(creators is not None):\n",
    "            for creator in creators:\n",
    "                if self.id not in creator.children:\n",
    "                    creator.children[self.id] = 1\n",
    "                else:\n",
    "                    creator.children[self.id] += 1    \n",
    "\n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            if(grad_origin is not None):\n",
    "                # if waiting to receive gradient, decrement counter\n",
    "                if(self.children[grad_origin.id] != 0):\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "                else:\n",
    "                    raise Exception(\"Same child cannot backpropagate more than once!\")\n",
    "\n",
    "            # if this is the beginning of the backpropagtion chain\n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            # accumulate gradients from all the children \n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad    \n",
    "\n",
    "            # backpropagate to creators if all gradients from children have been received or if gradients did not originate from another node\n",
    "            if((self.creators is not None) and (self.received_grads_from_all_children() or (grad_origin is None))):\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    new_grad = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    new_grad = self.grad.__neg__()\n",
    "                    self.creators[0].backward(new_grad, self)    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    new_grad = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = self.grad.__neg__()\n",
    "                    self.creators[1].backward(new_grad, self)    \n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new_grad = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = self.creators[0] * self.grad\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    new_grad = self.grad.mm(self.creators[1].transpose())\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = (self.creators[0].transpose()).mm(self.grad)\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    new_grad = self.grad.transpose()\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    # sigmoid derivative\n",
    "                    new_grad = self.grad * (self * (ones - self))\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    # tanh derivative\n",
    "                    new_grad = self.grad * (ones - self*self)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"relu\"):\n",
    "                    # relu derivative\n",
    "                    new_grad = self.grad * (self.creators[0].data > 0)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim,ds))\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "\n",
    "\n",
    "    # check to see if this tensor has recieved gradients from all children, which is indicated by all children counts being zero\n",
    "    def received_grads_from_all_children(self):\n",
    "        for id,count in self.children.items():\n",
    "            if (count != 0):\n",
    "                return False\n",
    "        return True     \n",
    "\n",
    "    # Note: operations always return a new tensor object \n",
    "\n",
    "    # element-wise addition\n",
    "    def __add__(self, other):\n",
    "        # return a new tensor object containing the sum\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data, creators=[self,other], creation_op =\"add\", autograd=True)\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    # element-wise negation\n",
    "    def __neg__(self):\n",
    "        # return a new tensor object containing the negation\n",
    "        if(self.autograd):\n",
    "            return Tensor(-1 * self.data, creators=[self], creation_op =\"neg\", autograd=True)\n",
    "        return Tensor(-1 * self.data)\n",
    "\n",
    "    # element-wise subtraction\n",
    "    def __sub__(self, other):\n",
    "        # return a new tensor object containing the subtraction\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data, creators=[self,other], creation_op =\"sub\", autograd=True)\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    # element-wise multiplication\n",
    "    def __mul__(self, other):\n",
    "        # return a new tensor object containing the multiplication\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data, creators=[self,other], creation_op =\"mul\", autograd=True)\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    # sum over all elements along given axis\n",
    "    def sum(self, axis):\n",
    "        # return a new tensor object containing the sum\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(axis), creators=[self], creation_op =\"sum_\"+str(axis), autograd=True)\n",
    "        return Tensor(self.data.sum(axis))\n",
    "    \n",
    "    # expands the tensor along the given axis\n",
    "    def expand(self, axis, copies):\n",
    "        \n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(axis, len(self.data.shape))\n",
    "        \n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data, autograd=True, creators=[self], creation_op=\"expand_\"+str(axis))\n",
    "        return Tensor(new_data)\n",
    "\n",
    "    # transpose of matrix \n",
    "    def transpose(self):\n",
    "        # return a new tensor object with the transposed tensor\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(), creators=[self], creation_op =\"transpose\", autograd=True)\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    # matrix multiplication\n",
    "    def mm(self, other):\n",
    "        # return a new tensor object containing the multiplication\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(np.dot(self.data, other.data), creators=[self,other], creation_op =\"mm\", autograd=True)\n",
    "        return Tensor(np.dot(self.data, other.data))\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    # Non-linearity functions\n",
    "\n",
    "    # sigmoid function\n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1.0 / (1.0 + np.exp(-self.data)), creators=[self], creation_op=\"sigmoid\", autograd=True)\n",
    "        return Tensor(1.0 / (1.0 + np.exp(self.data)))\n",
    "\n",
    "    # tanh function\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data), creators=[self], creation_op=\"sigmoid\", autograd=True)\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    # relu function\n",
    "    def relu(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * (self.data > 0), creators=[self], creation_op=\"sigmoid\", autograd=True)\n",
    "        return Tensor(self.data * (self.data > 0))\n",
    "    \n",
    "# stochastic gradient descent optimizer    \n",
    "class SGD_Optimizer(object):\n",
    "\n",
    "    def __init__(self, parameters, alpha) -> None:\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha    \n",
    "\n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "\n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= self.alpha * p.grad.data\n",
    "\n",
    "            if(zero):\n",
    "                p.grad.data *= 0\n",
    "\n",
    "# layer base class\n",
    "class Layer(object):   \n",
    "    def __init__(self) -> None:\n",
    "        self.parameters = []\n",
    "\n",
    "    def get_parameters(self):                     \n",
    "        return self.parameters\n",
    "    \n",
    "# layer inherited classes\n",
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs) -> None:\n",
    "        super().__init__()\n",
    "        # initilize the weights\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/n_inputs)\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "\n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.mm(self.weight) + self.bias.expand(0,len(input.data))   \n",
    "\n",
    "# a class for a senquence of layer, i.e. a neral network model\n",
    "class Sequential(Layer):\n",
    "    def __init__(self, layers = []) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.get_parameters()\n",
    "\n",
    "        return params    \n",
    "    \n",
    "# means squared error loss function layer    \n",
    "class MSELoss(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return ((pred-target) * (pred-target)).sum(0)\n",
    "\n",
    "# nonlinearity layers\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "class Relu(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.relu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([3,3,3,3,3], autograd=True)\n",
    "d = a + (-b)\n",
    "e = (-b) + c\n",
    "f = d + e\n",
    "\n",
    "print(f\"node(a), id: {a.id}, children: {a.children}, creators: {a.creators}\")\n",
    "print(f\"node(b), id: {b.id}, children: {b.children}, creators: {b.creators}\")\n",
    "print(f\"node(c), id: {c.id}, children: {c.children}, creators: {c.creators}\")\n",
    "print(f\"node(d), id: {d.id}, children: {d.children}, creators: {d.creators}\")\n",
    "print(f\"node(e), id: {e.id}, children: {e.children}, creators: {e.creators}\")\n",
    "print(f\"node(f), id: {f.id}, children: {f.children}, creators: {f.creators}\")\n",
    "\n",
    "D = Tensor([1,1,1,1,1])\n",
    "f.backward(grad = D)\n",
    "\n",
    "print(f\"f grad: {f.grad}\")\n",
    "print(f\"e grad: {e.grad}\")\n",
    "print(f\"d grad: {d.grad}\")\n",
    "print(f\"c grad: {c.grad}\")\n",
    "print(f\"b grad: {b.grad}\")\n",
    "print(f\"a grad: {a.grad}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: Using the tensor object and autograd to train a simple two layer linear network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "input_data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True) \n",
    "\n",
    "input_neurons = input_data.data.shape[1]\n",
    "hidden_neurons = 3\n",
    "output_neurons = target.data.shape[1]\n",
    "\n",
    "# initialize neural net layers\n",
    "model = Sequential(layers=[Linear(input_neurons, hidden_neurons), Linear(hidden_neurons, output_neurons)])\n",
    "loss_layer = MSELoss()\n",
    "\n",
    "# initialize optimizer\n",
    "optim = SGD_Optimizer(parameters=model.get_parameters(), alpha = 0.05) \n",
    "\n",
    "# training iterations\n",
    "niters = 10\n",
    "for iter in range(niters):\n",
    "\n",
    "    # forward pass\n",
    "    pred = model.forward(input_data)\n",
    "\n",
    "    # compute loss\n",
    "    loss = loss_layer.forward(pred, target)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization of weights\n",
    "    optim.step()\n",
    "\n",
    "    print(f\"Iteration# {iter+1}, Loss: {loss}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: Using the tensor object and autograd to train a network with non-linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "input_data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True) \n",
    "\n",
    "input_neurons = input_data.data.shape[1]\n",
    "hidden_neurons = 3\n",
    "output_neurons = target.data.shape[1]\n",
    "\n",
    "# initialize neural net layers\n",
    "model = Sequential(layers=[Linear(input_neurons, hidden_neurons), Tanh(),Linear(hidden_neurons, output_neurons), Sigmoid()])\n",
    "loss_layer = MSELoss()\n",
    "\n",
    "# initialize optimizer\n",
    "optim = SGD_Optimizer(parameters=model.get_parameters(), alpha = 1) \n",
    "\n",
    "# training iterations\n",
    "niters = 10\n",
    "for iter in range(niters):\n",
    "\n",
    "    # forward pass\n",
    "    pred = model.forward(input_data)\n",
    "\n",
    "    # compute loss\n",
    "    loss = loss_layer.forward(pred, target)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization of weights\n",
    "    optim.step()\n",
    "\n",
    "    print(f\"Iteration# {iter+1}, Loss: {loss}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding support for language processing:**\n",
    "\n",
    "Previously we had a `linear layer` which had a matrix of weights and forward propagatopn involved computing the vector-matrix multiplication of the inputs with the weights. We will now create a similar `embedding layer` for natural language processing. The `embedding layer` will also have a weights matrix, in this case each row of the matrix will correspond to an embedding for a word from the vocabulary, and the number of rows should be set equal to the total number of words in the vocabulary. The number of columns on the other hand will be set equal to the desired number of hidden neurons.\n",
    "\n",
    "During forward propagation, the input vector is going to be a list of word indices and the output will be specific rows (corresponding to the input word indices) selected from the weights matrix. To do this, we will add an `index_select` operation into our tensor object. During backpropagation, the gradients accociated with only those specific rows will be computed, and so a copy of the input word indices will be stored in the tensor containing the selected word rows and utilized during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data, creators=None, creation_op=None, autograd=False, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        if(id == None):\n",
    "            id = np.random.randint(0,100000)\n",
    "        self.id = id\n",
    "        self.children = {}\n",
    "        if(creators is not None):\n",
    "            for creator in creators:\n",
    "                if self.id not in creator.children:\n",
    "                    creator.children[self.id] = 1\n",
    "                else:\n",
    "                    creator.children[self.id] += 1    \n",
    "\n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            if(grad_origin is not None):\n",
    "                # if waiting to receive gradient, decrement counter\n",
    "                if(self.children[grad_origin.id] != 0):\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "                else:\n",
    "                    raise Exception(\"Same child cannot backpropagate more than once!\")\n",
    "\n",
    "            # if this is the beginning of the backpropagtion chain\n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            # accumulate gradients from all the children \n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad    \n",
    "\n",
    "            # backpropagate to creators if all gradients from children have been received or if gradients did not originate from another node\n",
    "            if((self.creators is not None) and (self.received_grads_from_all_children() or (grad_origin is None))):\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    new_grad = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    new_grad = self.grad.__neg__()\n",
    "                    self.creators[0].backward(new_grad, self)    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    new_grad = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = self.grad.__neg__()\n",
    "                    self.creators[1].backward(new_grad, self)    \n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new_grad = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = self.creators[0] * self.grad\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    new_grad = self.grad.mm(self.creators[1].transpose())\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = (self.creators[0].transpose()).mm(self.grad)\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    new_grad = self.grad.transpose()\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    # sigmoid derivative\n",
    "                    new_grad = self.grad * (self * (ones - self))\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    # tanh derivative\n",
    "                    new_grad = self.grad * (ones - self*self)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"relu\"):\n",
    "                    # relu derivative\n",
    "                    new_grad = self.grad * (self.creators[0].data > 0)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    # cross entropy derivative\n",
    "                    new_grad = Tensor(self.softmax_output - self.target_dist)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    # gradient of the weights matrix of word embeddings\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    # we only add gradients to the specific rows corresponding to the selected words \n",
    "                    indices_ = self.index_select_indices.data.flatten() \n",
    "                    grad_ = self.grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad), self)       \n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim,ds))\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "\n",
    "\n",
    "    # check to see if this tensor has recieved gradients from all children, which is indicated by all children counts being zero\n",
    "    def received_grads_from_all_children(self):\n",
    "        for id,count in self.children.items():\n",
    "            if (count != 0):\n",
    "                return False\n",
    "        return True     \n",
    "\n",
    "    # Note: operations always return a new tensor object \n",
    "\n",
    "    # element-wise addition\n",
    "    def __add__(self, other):\n",
    "        # return a new tensor object containing the sum\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data, creators=[self,other], creation_op =\"add\", autograd=True)\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    # element-wise negation\n",
    "    def __neg__(self):\n",
    "        # return a new tensor object containing the negation\n",
    "        if(self.autograd):\n",
    "            return Tensor(-1 * self.data, creators=[self], creation_op =\"neg\", autograd=True)\n",
    "        return Tensor(-1 * self.data)\n",
    "\n",
    "    # element-wise subtraction\n",
    "    def __sub__(self, other):\n",
    "        # return a new tensor object containing the subtraction\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data, creators=[self,other], creation_op =\"sub\", autograd=True)\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    # element-wise multiplication\n",
    "    def __mul__(self, other):\n",
    "        # return a new tensor object containing the multiplication\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data, creators=[self,other], creation_op =\"mul\", autograd=True)\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    # sum over all elements along given axis\n",
    "    def sum(self, axis):\n",
    "        # return a new tensor object containing the sum\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(axis), creators=[self], creation_op =\"sum_\"+str(axis), autograd=True)\n",
    "        return Tensor(self.data.sum(axis))\n",
    "    \n",
    "    # expands the tensor along the given axis\n",
    "    def expand(self, axis, copies):\n",
    "        \n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(axis, len(self.data.shape))\n",
    "        \n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data, autograd=True, creators=[self], creation_op=\"expand_\"+str(axis))\n",
    "        return Tensor(new_data)\n",
    "\n",
    "    # transpose of matrix \n",
    "    def transpose(self):\n",
    "        # return a new tensor object with the transposed tensor\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(), creators=[self], creation_op =\"transpose\", autograd=True)\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    # matrix multiplication\n",
    "    def mm(self, other):\n",
    "        # return a new tensor object containing the multiplication\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(np.dot(self.data, other.data), creators=[self,other], creation_op =\"mm\", autograd=True)\n",
    "        return Tensor(np.dot(self.data, other.data))\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    # Non-linearity functions\n",
    "\n",
    "    # sigmoid function\n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1.0 / (1.0 + np.exp(-self.data)), creators=[self], creation_op=\"sigmoid\", autograd=True)\n",
    "        return Tensor(1.0 / (1.0 + np.exp(-self.data)))\n",
    "\n",
    "    # tanh function\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data), creators=[self], creation_op=\"tanh\", autograd=True)\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    # relu function\n",
    "    def relu(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * (self.data > 0), creators=[self], creation_op=\"relu\", autograd=True)\n",
    "        return Tensor(self.data * (self.data > 0))\n",
    "    \n",
    "    \n",
    "    def softmax(self):\n",
    "\n",
    "        ex = np.exp(self.data)\n",
    "        softmax_output = ex/np.sum(ex, axis = len(self.data.shape)-1, keepdims = True) \n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(softmax_output, creators = [self], creation_op = \"softmax\", autograd=True)    \n",
    "        return Tensor(loss) \n",
    "\n",
    "\n",
    "    def cross_entropy(self, target_indices):\n",
    "\n",
    "        ex = np.exp(self.data)\n",
    "        softmax_output = ex/np.sum(ex, axis = len(self.data.shape)-1, keepdims = True) \n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t), -1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "\n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss, creators = [self], creation_op = \"cross_entropy\", autograd=True)\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out \n",
    "        return Tensor(loss) \n",
    "\n",
    "\n",
    "    # word embedding operations (the input 'indices' are just word a vector of indices, i.e. specifix row numbers that are to be selected and returned)\n",
    "    def index_select(self, indices):\n",
    "        if(self.autograd):\n",
    "            selected_rows =  Tensor(self.data[indices.data], creators=[self], creation_op=\"index_select\", autograd=True)\n",
    "            selected_rows.index_select_indices = indices \n",
    "            return selected_rows \n",
    "        return Tensor(self.data[indices.data])\n",
    "\n",
    "# stochastic gradient descent optimizer    \n",
    "class SGD_Optimizer(object):\n",
    "\n",
    "    def __init__(self, parameters, alpha) -> None:\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha    \n",
    "\n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "\n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= self.alpha * p.grad.data\n",
    "\n",
    "            if(zero):\n",
    "                p.grad.data *= 0\n",
    "\n",
    "# layer base class\n",
    "class Layer(object):   \n",
    "    def __init__(self) -> None:\n",
    "        self.parameters = []\n",
    "\n",
    "    def get_parameters(self):                     \n",
    "        return self.parameters\n",
    "    \n",
    "# layer inherited classes\n",
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs) -> None:\n",
    "        super().__init__()\n",
    "        # initilize the weights\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/n_inputs)\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "\n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.mm(self.weight) + self.bias.expand(0,len(input.data))   \n",
    "\n",
    "# embedding layer inherited class\n",
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, hidden_neurons) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "\n",
    "        # initialize the weights matrix of word embeddings \n",
    "        weight = (np.random.rand(vocab_size, hidden_neurons)-0.5)/hidden_neurons\n",
    "        self.weight = Tensor(weight, autograd=True)\n",
    "        self.parameters.append(self.weight)   \n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)    \n",
    "        \n",
    "\n",
    "# a class for a senquence of layer, i.e. a neral network model\n",
    "class Sequential(Layer):\n",
    "    def __init__(self, layers = []) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.get_parameters()\n",
    "\n",
    "        return params    \n",
    "    \n",
    "# means squared error loss function layer    \n",
    "class MSELoss(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return ((pred-target) * (pred-target)).sum(0)\n",
    "\n",
    "# cross entropy loss function layer    \n",
    "class CrossEntropyLoss(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "\n",
    "\n",
    "# nonlinearity layers\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "class Relu(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.relu()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of word embedding forward pass and backprop in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a weights matrix for a vocabulary of 5 words and 5 hidden neurons\n",
    "w = Tensor(np.eye(5), autograd=True)\n",
    "print(\"weights matrix:\")\n",
    "print(w)\n",
    "\n",
    "# forward pass for an input containing two sentence vectors with three words each\n",
    "input_indices = Tensor(np.array([[1,2,3], [2,3,4]]))\n",
    "selected_rows = w.index_select(input_indices)\n",
    "print(\"Selected rows:\")\n",
    "print(selected_rows)\n",
    "\n",
    "# compute gradient of weights for the given input\n",
    "selected_rows.backward()\n",
    "print(\"weights gradient:\")\n",
    "print(w.grad)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a training network with embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "input_data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True) \n",
    "vocab_size = 5\n",
    "hidden_neurons = 3\n",
    "output_neurons = target.data.shape[1]\n",
    "\n",
    "# initialize neural net layers\n",
    "model = Sequential(layers=[Embedding(vocab_size, hidden_neurons),Tanh(),Linear(hidden_neurons, output_neurons), Sigmoid()])\n",
    "loss_layer = MSELoss()\n",
    "\n",
    "# initialize optimizer\n",
    "optim = SGD_Optimizer(parameters=model.get_parameters(), alpha = 0.5) \n",
    "\n",
    "# training iterations\n",
    "niters = 10\n",
    "for iter in range(niters):\n",
    "\n",
    "    # forward pass\n",
    "    pred = model.forward(input_data)\n",
    "\n",
    "    # compute loss\n",
    "    loss = loss_layer.forward(pred, target)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization of weights\n",
    "    optim.step()\n",
    "\n",
    "    print(f\"Iteration# {iter+1}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# input data indices\n",
    "input_data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "# target indices\n",
    "target = Tensor(np.array([0,1,0,1]), autograd=True) \n",
    "\n",
    "vocab_size = 3\n",
    "hidden_neurons = 3\n",
    "output_neurons = len(target.data)\n",
    "\n",
    "# initialize neural net layers\n",
    "model = Sequential(layers=[Embedding(vocab_size, hidden_neurons),Tanh(),Linear(hidden_neurons, output_neurons)])\n",
    "loss_layer = CrossEntropyLoss()\n",
    "\n",
    "# initialize optimizer\n",
    "optim = SGD_Optimizer(parameters=model.get_parameters(), alpha = 0.1) \n",
    "\n",
    "# training iterations\n",
    "niters = 10\n",
    "for iter in range(niters):\n",
    "\n",
    "    # forward pass\n",
    "    pred = model.forward(input_data)\n",
    "\n",
    "    # compute loss\n",
    "    loss = loss_layer.forward(pred, target)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization of weights\n",
    "    optim.step()\n",
    "\n",
    "    print(f\"Iteration# {iter+1}, Loss: {loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a `Recurrent Layer` to handle sequenced inputs**\n",
    "\n",
    "A recurrent layer is made up of several linear and non-linear sublayers, each of these is called an `RNN cell` so that the `recurrent neural network is a chain of these RNN cells`. Then given a `sequence of input vectors`, the first vector in the sequence is fed into the first RNN cell, and a vector called the `hidden state` is computed. This hidden state is simply the input vector multiplied by a weight matrix (`W_ih`) added to the vector obtained from multiplying another weight matrix (`W_hh`) to the hidden state computed in the previous RNN cell, and the combined result is passed through a non-linearity layer (containing an activation function). Then this hiddent state is multiplied by a final weight matrix (`W_ho`) to compute a prediction. So each vector in the input sequence is fed into it's corresponding RNN cell and a hidden state and prediction are computed in its RNN cell. The `hidden state` is the key component here that contains information about the ordering of the items in the input sequence. The first RNN cell requires a hidden state to be initialized so that constitutes as an extra set of parameters.    \n",
    "\n",
    "For natural language processing, the inputs and prediction vectors are word embeddings, so the size of these vectors (i.e. number of input and output neurons) will be the size of the vocabulary. And we're free to choose any number of hidden neurons we want.\n",
    "\n",
    "For the following examples, we will constructs models that can only handle input sequences of fixed size (i.e. 6 words per sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data, creators=None, creation_op=None, autograd=False, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        if(id == None):\n",
    "            id = np.random.randint(0,100000)\n",
    "        self.id = id\n",
    "        self.children = {}\n",
    "        if(creators is not None):\n",
    "            for creator in creators:\n",
    "                if self.id not in creator.children:\n",
    "                    creator.children[self.id] = 1\n",
    "                else:\n",
    "                    creator.children[self.id] += 1    \n",
    "\n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            if(grad_origin is not None):\n",
    "                # if waiting to receive gradient, decrement counter\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    return\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "                \n",
    "                #else:\n",
    "                #    raise Exception(\"Same child cannot backpropagate more than once!\")\n",
    "\n",
    "            # if this is the beginning of the backpropagtion chain\n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            # accumulate gradients from all the children \n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad    \n",
    "\n",
    "            # make sure grads don't have their own grads\n",
    "            assert grad.autograd == False\n",
    "\n",
    "            # backpropagate to creators if all gradients from children have been received or if gradients did not originate from another node\n",
    "            if((self.creators is not None) and (self.received_grads_from_all_children() or (grad_origin is None))):\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    new_grad = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "\n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    new_grad = self.grad.__neg__()\n",
    "                    self.creators[0].backward(new_grad)    \n",
    "                \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    new_grad = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = Tensor(self.grad.__neg__().data)\n",
    "                    self.creators[1].backward(new_grad, self)    \n",
    "                \n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new_grad = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = self.creators[0] * self.grad\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "\n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    new_grad = self.grad.mm(self.creators[1].transpose())\n",
    "                    self.creators[0].backward(new_grad)\n",
    "                    new_grad = (self.creators[0].transpose()).mm(self.grad)\n",
    "                    self.creators[1].backward(new_grad)\n",
    "                \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    new_grad = self.grad.transpose()\n",
    "                    self.creators[0].backward(new_grad)\n",
    "                \n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    # sigmoid derivative\n",
    "                    new_grad = self.grad * (self * (ones - self))\n",
    "                    self.creators[0].backward(new_grad)\n",
    "                \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    # tanh derivative\n",
    "                    new_grad = self.grad * (ones - self*self)\n",
    "                    self.creators[0].backward(new_grad)\n",
    "                \n",
    "                if(self.creation_op == \"relu\"):\n",
    "                    # relu derivative\n",
    "                    new_grad = self.grad * (self.creators[0].data > 0)\n",
    "                    self.creators[0].backward(new_grad)\n",
    "                \n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    # cross entropy derivative\n",
    "                    new_grad = Tensor(self.softmax_output - self.target_dist)\n",
    "                    self.creators[0].backward(new_grad)\n",
    "                \n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    # gradient of the weights matrix of word embeddings\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    # we only add gradients to the specific rows corresponding to the selected words \n",
    "                    indices_ = self.index_select_indices.data.flatten() \n",
    "                    grad_ = self.grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))       \n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim,ds))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "\n",
    "\n",
    "    # check to see if this tensor has recieved gradients from all children, which is indicated by all children counts being zero\n",
    "    def received_grads_from_all_children(self):\n",
    "        for id,count in self.children.items():\n",
    "            if (count != 0):\n",
    "                return False\n",
    "        return True     \n",
    "\n",
    "    # Note: operations always return a new tensor object \n",
    "\n",
    "    # element-wise addition\n",
    "    def __add__(self, other):\n",
    "        # return a new tensor object containing the sum\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data, creators=[self,other], creation_op =\"add\", autograd=True)\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    # element-wise negation\n",
    "    def __neg__(self):\n",
    "        # return a new tensor object containing the negation\n",
    "        if(self.autograd):\n",
    "            return Tensor(-1 * self.data, creators=[self], creation_op =\"neg\", autograd=True)\n",
    "        return Tensor(-1 * self.data)\n",
    "\n",
    "    # element-wise subtraction\n",
    "    def __sub__(self, other):\n",
    "        # return a new tensor object containing the subtraction\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data, creators=[self,other], creation_op =\"sub\", autograd=True)\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    # element-wise multiplication\n",
    "    def __mul__(self, other):\n",
    "        # return a new tensor object containing the multiplication\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data, creators=[self,other], creation_op =\"mul\", autograd=True)\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    # sum over all elements along given axis\n",
    "    def sum(self, axis):\n",
    "        # return a new tensor object containing the sum\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(axis), creators=[self], creation_op =\"sum_\"+str(axis), autograd=True)\n",
    "        return Tensor(self.data.sum(axis))\n",
    "    \n",
    "    # expands the tensor along the given axis\n",
    "    def expand(self, axis, copies):\n",
    "        \n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(axis, len(self.data.shape))\n",
    "        \n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data, autograd=True, creators=[self], creation_op=\"expand_\"+str(axis))\n",
    "        return Tensor(new_data)\n",
    "\n",
    "    # transpose of matrix \n",
    "    def transpose(self):\n",
    "        # return a new tensor object with the transposed tensor\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(), creators=[self], creation_op =\"transpose\", autograd=True)\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    # matrix multiplication\n",
    "    def mm(self, other):\n",
    "        # return a new tensor object containing the multiplication\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(np.dot(self.data, other.data), creators=[self,other], creation_op =\"mm\", autograd=True)\n",
    "        return Tensor(np.dot(self.data, other.data))\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    # Non-linearity functions\n",
    "\n",
    "    # sigmoid function\n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1.0 / (1.0 + np.exp(-self.data)), creators=[self], creation_op=\"sigmoid\", autograd=True)\n",
    "        return Tensor(1.0 / (1.0 + np.exp(-self.data)))\n",
    "\n",
    "    # tanh function\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data), creators=[self], creation_op=\"tanh\", autograd=True)\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    # relu function\n",
    "    def relu(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * (self.data > 0), creators=[self], creation_op=\"relu\", autograd=True)\n",
    "        return Tensor(self.data * (self.data > 0))\n",
    "    \n",
    "    \n",
    "    def softmax(self):\n",
    "\n",
    "        ex = np.exp(self.data)\n",
    "        softmax_output = ex/np.sum(ex, axis = len(self.data.shape)-1, keepdims = True) \n",
    "        return softmax_output \n",
    "    \n",
    "\n",
    "    def cross_entropy(self, target_indices):\n",
    "\n",
    "        ex = np.exp(self.data)\n",
    "        softmax_output = ex/np.sum(ex, axis = len(self.data.shape)-1, keepdims = True) \n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t), -1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "\n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss, creators = [self], creation_op = \"cross_entropy\", autograd=True)\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out \n",
    "        return Tensor(loss) \n",
    "\n",
    "\n",
    "    # word embedding operations (the input 'indices' are just word a vector of indices, i.e. specifix row numbers that are to be selected and returned)\n",
    "    def index_select(self, indices):\n",
    "        if(self.autograd):\n",
    "            selected_rows =  Tensor(self.data[indices.data], creators=[self], creation_op=\"index_select\", autograd=True)\n",
    "            selected_rows.index_select_indices = indices \n",
    "            return selected_rows \n",
    "        return Tensor(self.data[indices.data])\n",
    "\n",
    "# stochastic gradient descent optimizer    \n",
    "class SGD_Optimizer(object):\n",
    "\n",
    "    def __init__(self, parameters, alpha) -> None:\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha    \n",
    "\n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "\n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= self.alpha * p.grad.data\n",
    "\n",
    "            if(zero):\n",
    "                p.grad.data *= 0\n",
    "\n",
    "# layer base class\n",
    "class Layer(object):   \n",
    "    def __init__(self) -> None:\n",
    "        self.parameters = []\n",
    "\n",
    "    def get_parameters(self):                     \n",
    "        return self.parameters\n",
    "    \n",
    "# layer inherited classes\n",
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs, bias=True) -> None:\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        # initilize the weights\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/n_inputs)\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.parameters.append(self.weight)\n",
    "\n",
    "        if(bias):\n",
    "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        if(bias):\n",
    "            self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if(self.bias):\n",
    "            return input.mm(self.weight) + self.bias.expand(0,len(input.data))   \n",
    "        else:\n",
    "            return input.mm(self.weight)\n",
    "\n",
    "\n",
    "# embedding layer inherited class\n",
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, hidden_neurons) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "\n",
    "        # initialize the weights matrix of word embeddings \n",
    "        weight = (np.random.rand(vocab_size, hidden_neurons)-0.5)/hidden_neurons\n",
    "        self.weight = Tensor(weight, autograd=True)\n",
    "        self.parameters.append(self.weight)   \n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)    \n",
    "        \n",
    "\n",
    "class RNNcell(Layer):\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons, activation = \"sigmoid\") -> None:\n",
    "        super().__init__()\n",
    "        self.input_neurons = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        \n",
    "        # initialize the nonlinearity layer\n",
    "        if(activation == \"sigmoid\"):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == \"tanh\"):\n",
    "            self.activation = Tanh()\n",
    "        elif(activation == \"relu\"):\n",
    "            self.activation = Relu()\n",
    "        else:\n",
    "            raise Exception(\"ERROR: Non-linearity function not found!\")\n",
    "\n",
    "        # initialize the wieghts\n",
    "        self.w_ih = Linear(input_neurons, hidden_neurons)\n",
    "        self.w_hh = Linear(hidden_neurons, hidden_neurons)\n",
    "        self.w_ho = Linear(hidden_neurons, output_neurons)\n",
    "\n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "\n",
    "    def forward(self, input, prev_hidden):\n",
    "\n",
    "        # compute hidden state for this RNN cell\n",
    "        input_times_weight = self.w_ih.forward(input) \n",
    "        combined = input_times_weight + self.w_hh.forward(prev_hidden)   \n",
    "        hidden = self.activation.forward(combined)\n",
    "        #compute prediction\n",
    "        pred = self.w_ho.forward(hidden)\n",
    "       \n",
    "        return pred, hidden\n",
    "     \n",
    "    def init_hidden(self, batch_size = 1):\n",
    "        # initialize the hidden state\n",
    "        return Tensor(np.zeros(shape=(batch_size, self.hidden_neurons)), autograd=True) \n",
    "  \n",
    "        \n",
    "# a class for a senquence of layer, i.e. a neral network model\n",
    "class Sequential(Layer):\n",
    "    def __init__(self, layers = []) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.get_parameters()\n",
    "\n",
    "        return params    \n",
    "    \n",
    "# means squared error loss function layer    \n",
    "class MSELoss(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return ((pred-target) * (pred-target)).sum(0)\n",
    "\n",
    "# cross entropy loss function layer    \n",
    "class CrossEntropyLoss(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "\n",
    "\n",
    "# nonlinearity layers\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "class Relu(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.relu()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an RNN with the Babi text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data from file\n",
    "f = open('tasksv11/en/qa1_single-supporting-fact_train.txt', 'r')\n",
    "raw = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the first 1000 senteneces (remove numbers, newline characters and punctuations)\n",
    "tokens= []\n",
    "for i, sentence in enumerate(raw[0:1000]):\n",
    "    tokenized_sent = sentence.lower().replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"?\",\"\").replace(\".\",\"\").split(\" \")[1:] \n",
    "    if((i+1)%3 == 0):\n",
    "        # get rid of number from the last word\n",
    "        last_word = tokenized_sent[-1]\n",
    "        tokenized_sent[-1] = \"\".join([char for char in last_word if not char.isnumeric()])\n",
    "    # pad the sentence at the beginning with '-' characters to make it 6 words long\n",
    "    padded_sent = ['-'] * (6 - len(tokenized_sent)) + tokenized_sent  \n",
    "    tokens.append(padded_sent)\n",
    "\n",
    "# create a vocabulary from the data\n",
    "vocab = set()\n",
    "for sentence in tokens:\n",
    "    for word in sentence:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# create a dictionary of vocab word indices\n",
    "word_index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word_index[word] = i    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting a list of words into a list of word indices\n",
    "def words_to_indices(words):\n",
    "    indices = [word_index[word] for word in words]\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the input data, i.e. list of word indices\n",
    "indices = []\n",
    "for sentence in tokens:\n",
    "    indices.append(words_to_indices(sentence))\n",
    "\n",
    "data = np.array(indices)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration# 1, Loss: 174.91233356158114, Accuracy: 0.0\n",
      "Iteration# 2, Loss: 173.6856930251804, Accuracy: 0.0\n",
      "Iteration# 3, Loss: 173.04619442751138, Accuracy: 0.0\n",
      "Iteration# 4, Loss: 172.84155725320647, Accuracy: 0.0\n",
      "Iteration# 5, Loss: 172.96326217494772, Accuracy: 0.0\n",
      "Iteration# 6, Loss: 173.33188314017048, Accuracy: 0.0\n",
      "Iteration# 7, Loss: 173.88811975168045, Accuracy: 0.137\n",
      "Iteration# 8, Loss: 174.58703443150947, Accuracy: 0.182\n",
      "Iteration# 9, Loss: 175.39418267463245, Accuracy: 0.182\n",
      "Iteration# 10, Loss: 176.28292115969154, Accuracy: 0.182\n",
      "Iteration# 11, Loss: 177.23248471207654, Accuracy: 0.182\n",
      "Iteration# 12, Loss: 178.22658554155396, Accuracy: 0.182\n",
      "Iteration# 13, Loss: 179.25237820894438, Accuracy: 0.182\n",
      "Iteration# 14, Loss: 180.29968635980245, Accuracy: 0.182\n",
      "Iteration# 15, Loss: 181.36041970498397, Accuracy: 0.182\n",
      "Iteration# 16, Loss: 182.42813078162183, Accuracy: 0.182\n",
      "Iteration# 17, Loss: 183.49767526724804, Accuracy: 0.182\n",
      "Iteration# 18, Loss: 184.56494954652524, Accuracy: 0.182\n",
      "Iteration# 19, Loss: 185.62668629255808, Accuracy: 0.182\n",
      "Iteration# 20, Loss: 186.68029391379585, Accuracy: 0.182\n",
      "Iteration# 21, Loss: 187.72372941234747, Accuracy: 0.182\n",
      "Iteration# 22, Loss: 188.75539689403573, Accuracy: 0.182\n",
      "Iteration# 23, Loss: 189.7740659410669, Accuracy: 0.182\n",
      "Iteration# 24, Loss: 190.77880550254557, Accuracy: 0.182\n",
      "Iteration# 25, Loss: 191.76893001930904, Accuracy: 0.182\n",
      "Iteration# 26, Loss: 192.74395528168682, Accuracy: 0.182\n",
      "Iteration# 27, Loss: 193.7035620975983, Accuracy: 0.182\n",
      "Iteration# 28, Loss: 194.6475662790199, Accuracy: 0.182\n",
      "Iteration# 29, Loss: 195.57589377737366, Accuracy: 0.182\n",
      "Iteration# 30, Loss: 196.48856004189915, Accuracy: 0.182\n",
      "Iteration# 31, Loss: 197.38565286065216, Accuracy: 0.182\n",
      "Iteration# 32, Loss: 198.26731808665917, Accuracy: 0.182\n",
      "Iteration# 33, Loss: 199.13374776300407, Accuracy: 0.182\n",
      "Iteration# 34, Loss: 199.98517024820785, Accuracy: 0.182\n",
      "Iteration# 35, Loss: 200.8218420129901, Accuracy: 0.182\n",
      "Iteration# 36, Loss: 201.64404083560368, Accuracy: 0.182\n",
      "Iteration# 37, Loss: 202.45206016850608, Accuracy: 0.182\n",
      "Iteration# 38, Loss: 203.2462044864731, Accuracy: 0.182\n",
      "Iteration# 39, Loss: 204.02678545706934, Accuracy: 0.182\n",
      "Iteration# 40, Loss: 204.79411879997235, Accuracy: 0.182\n",
      "Iteration# 41, Loss: 205.548521722989, Accuracy: 0.182\n",
      "Iteration# 42, Loss: 206.2903108404598, Accuracy: 0.182\n",
      "Iteration# 43, Loss: 207.01980049475017, Accuracy: 0.182\n",
      "Iteration# 44, Loss: 207.7373014141347, Accuracy: 0.182\n",
      "Iteration# 45, Loss: 208.4431196510013, Accuracy: 0.182\n",
      "Iteration# 46, Loss: 209.1375557532498, Accuracy: 0.182\n",
      "Iteration# 47, Loss: 209.82090412929972, Accuracy: 0.182\n",
      "Iteration# 48, Loss: 210.49345257347997, Accuracy: 0.182\n",
      "Iteration# 49, Loss: 211.1554819239251, Accuracy: 0.182\n",
      "Iteration# 50, Loss: 211.80726582961395, Accuracy: 0.182\n"
     ]
    }
   ],
   "source": [
    "niters = 50\n",
    "batch_size = 100\n",
    "hidden_neurons = 16\n",
    "\n",
    "# initialize the RNN layers\n",
    "embed = Embedding(len(vocab), hidden_neurons)\n",
    "# Note: since we're going to feed in outputs from the embedding layer into the RNN cell, the input neurons size needs to be equal to the the length of the embedding vectors, which is the hidden neurons size\n",
    "model = RNNcell(hidden_neurons, hidden_neurons, len(vocab))\n",
    "loss_layer = CrossEntropyLoss()\n",
    "params = embed.get_parameters() + model.get_parameters() \n",
    "optim = SGD_Optimizer(params, alpha=0.0001)\n",
    "\n",
    "\n",
    "\n",
    "# train the network to predict the last word (i.e the 6th word) in every sentence in the input set\n",
    "for iter in range(niters):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    # train in batches\n",
    "    for j in range(int(len(data)/batch_size)):\n",
    "    \n",
    "        batch_lo = j * batch_size \n",
    "        batch_hi = min((j+1) * batch_size, len(data)) \n",
    "        batch = data[batch_lo:batch_hi]\n",
    "\n",
    "        sent = []\n",
    "        for ix in range(6):\n",
    "            sent.append(vocab[batch[0,ix]])\n",
    "        #if(iter == 9):\n",
    "        #    print(f\"Sentence: {sent}\")\n",
    "\n",
    "        # initilaize hidden state\n",
    "        hidden = model.init_hidden(batch_size) \n",
    "\n",
    "        # forward pass through RNN cells (5 word input sequence so 5 RNN cell passes)\n",
    "        for k in range(5):\n",
    "            input = Tensor(batch[:, k], autograd=True)\n",
    "            # create the word embedding from the input word\n",
    "            rnn_input = embed.forward(input)\n",
    "            # feed the word embedding into the RNN cell\n",
    "            prediction, hidden = model.forward(rnn_input, hidden)\n",
    " \n",
    "            # compute loss (i.e. compare predicted word from the last RNN cell to last word in the sentence)\n",
    "            target = Tensor(batch[:, k+1], autograd=True)\n",
    "            loss = loss_layer.forward(prediction, target)\n",
    "            \n",
    "            total_loss += loss.data\n",
    "\n",
    "    \n",
    "        # compute prediction accuracy\n",
    "        for ix in range(batch_size):\n",
    "            if(np.argmax(prediction.data[ix]) == target.data[ix]):\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1    \n",
    "\n",
    "        \n",
    "        #    print(f\"Actual word: {vocab[target.data[ix]]}, Predicted word: {vocab[np.argmax(prediction.data[ix])]} \")\n",
    "       \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "            \n",
    "        # weights optimization\n",
    "        optim.step()\n",
    "\n",
    "    if(iter % 1 == 0):\n",
    "        print(f\"Iteration# {iter+1}, Loss: {total_loss}, Accuracy: {float(correct)/(float(correct + incorrect))}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the network to predict every next word in the sentence starting from the first word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration# 1, Loss: 139.20131257111737, Accuracy: 0.2052\n",
      "Iteration# 2, Loss: 91.66085931127233, Accuracy: 0.4924\n",
      "Iteration# 3, Loss: 69.48780187348744, Accuracy: 0.5248\n",
      "Iteration# 4, Loss: 60.37685940819524, Accuracy: 0.548\n",
      "Iteration# 5, Loss: 55.30365899018569, Accuracy: 0.5838\n",
      "Iteration# 6, Loss: 51.906851470236894, Accuracy: 0.6006\n",
      "Iteration# 7, Loss: 49.599056463675076, Accuracy: 0.6016\n",
      "Iteration# 8, Loss: 48.04471509259804, Accuracy: 0.6006\n",
      "Iteration# 9, Loss: 46.96582850151995, Accuracy: 0.601\n",
      "Iteration# 10, Loss: 46.1809290623813, Accuracy: 0.6016\n"
     ]
    }
   ],
   "source": [
    "niters = 10\n",
    "batch_size = 100\n",
    "hidden_neurons = 16\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize the RNN layers\n",
    "embed = Embedding(len(vocab), hidden_neurons)\n",
    "# Note: since we're going to feed in outputs from the embedding layer into the RNN cell, the input neurons size needs to be equal to the the length of the embedding vectors, which is the hidden neurons size\n",
    "model = RNNcell(hidden_neurons, hidden_neurons, len(vocab), activation=\"tanh\")\n",
    "\n",
    "# initialize loss layers for predictions at each RNN cell\n",
    "loss_layers = [CrossEntropyLoss()]*5\n",
    "\n",
    "params = embed.get_parameters() + model.get_parameters() \n",
    "optim = SGD_Optimizer(params, alpha=0.001)\n",
    "\n",
    "\n",
    "# make sure input data set is divisible by batch size\n",
    "if(len(data)%batch_size != 0):\n",
    "    raise Exception(\"ERROR! Input dataset needs to be divisible by batch_size\")\n",
    "\n",
    "# train the network to predict the next word in the given input sequence\n",
    "for iter in range(niters):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    # initilaize hidden state\n",
    "    hidden = model.init_hidden(batch_size) \n",
    "\n",
    "    # train in batches\n",
    "    for j in range(int(len(data)/batch_size)):\n",
    "    \n",
    "        batch_lo = j * batch_size \n",
    "        batch_hi = min((j+1) * batch_size, len(data)) \n",
    "        batch = data[batch_lo:batch_hi]\n",
    "        \n",
    "        # forward pass through RNN cells (5 word input sequence so 5 RNN cell passes)\n",
    "        for k in range(5):\n",
    "            \n",
    "            input = Tensor(batch[:, k], autograd=True)\n",
    "            # create the word embedding from the input word\n",
    "            rnn_input = embed.forward(input)\n",
    "            # feed the word embedding into the RNN cell to predict the next word\n",
    "            prediction, hidden = model.forward(rnn_input, hidden)\n",
    "    \n",
    "            # compute loss (i.e. compare predicted word from the last RNN cell to last word in the sentence)\n",
    "            target = Tensor(batch[:, k+1], autograd=True)\n",
    "            loss = loss_layers[k].forward(prediction, target)\n",
    "            total_loss += loss.data\n",
    "        \n",
    "            # backpropagate the loss gadients\n",
    "            loss.backward()\n",
    "\n",
    "                    \n",
    "            # compute prediction accuracy\n",
    "            for ix in range(batch_size):\n",
    "                if(np.argmax(prediction.data[ix]) == target.data[ix]):\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    incorrect += 1    \n",
    "            \n",
    "\n",
    "        # compute prediction accuracy\n",
    "        #for ix in range(batch_size):\n",
    "        #    if(np.argmax(prediction.data[ix]) == target.data[ix]):\n",
    "        #        correct += 1\n",
    "        #    else:\n",
    "        #        incorrect += 1   \n",
    "\n",
    "        # weights optimization\n",
    "        optim.step()\n",
    "\n",
    "    if(iter%1 == 0):\n",
    "        print(f\"Iteration# {iter+1}, Loss: {total_loss}, Accuracy: {float(correct)/(float(correct + incorrect))}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Truncated Backpropagation:`** In the previous example, we processed input sequences of fixed size, i.e. 6 word long sequences, so the number of RNNcell forward passes that we needed were 5 since we trained the RNN to predict the next word after every word, starting from the first word. And the losses are backpropagated all teh way up to the input of the first RNN cell in the sequence. In practice, we can have arbitrarily long input sequences and all we would need to do is increase the number of iterations in the RNN cells forward pass loop, but with the backpropapagtions also become very long. It turns out that we can still optimize the weights reasonably well without backpropagating the gradients all the way to the first RNN cell's input. For this kind of truncated backpopagation, we break up the input sequence into smaller equal-sized chunks and we only backpropagate upto the beginning of each chunk. This is demonstarted in the example below where we break up the 6 word inputs into two chunks of 3 words.\n",
    "\n",
    "Since the input to the first RNNcell is the initial hidden state, to implement this kind of truncated backpropagation, all we need to do is initialize a new hidden state tensor at the beginning of each chunk and autograd will make sure that the gradients propagate up to the beginning of that chunk only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "ERROR! Input dataset needs to be divisible by batch_size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16736\\2647396067.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# make sure input data set is divisible by batch size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ERROR! Input dataset needs to be divisible by batch_size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# train the network to predict the next word in the given input sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: ERROR! Input dataset needs to be divisible by batch_size"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "niters = 1000\n",
    "batch_size = 100\n",
    "hidden_neurons = 16\n",
    "chunk_size = 3\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize the RNN layers\n",
    "embed = Embedding(len(vocab), hidden_neurons)\n",
    "# Note: since we're going to feed in outputs from the embedding layer into the RNN cell, the input neurons size needs to be equal to the the length of the embedding vectors, which is the hidden neurons size\n",
    "model = RNNcell(hidden_neurons, hidden_neurons, len(vocab), activation=\"tanh\")\n",
    "\n",
    "# initialize loss layers for predictions at each RNN cell\n",
    "loss_layers = [CrossEntropyLoss()]*5\n",
    "\n",
    "params = embed.get_parameters() + model.get_parameters() \n",
    "optim = SGD_Optimizer(params, alpha=0.001)\n",
    "\n",
    "\n",
    "# make sure input data set is divisible by batch size\n",
    "if(len(data)%batch_size != 0):\n",
    "    raise Exception(\"ERROR! Input dataset needs to be divisible by batch_size\")\n",
    "\n",
    "# train the network to predict the next word in the given input sequence\n",
    "for iter in range(niters):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    # initilaize hidden state\n",
    "    hidden = model.init_hidden(batch_size) \n",
    "\n",
    "    # train in batches\n",
    "    for j in range(int(len(data)/batch_size)):\n",
    "    \n",
    "        batch_lo = j * batch_size \n",
    "        batch_hi = min((j+1) * batch_size, len(data)) \n",
    "        batch = data[batch_lo:batch_hi]\n",
    "        \n",
    "        # forward pass through RNN cells \n",
    "        for k in range(batch.shape[1]-1):\n",
    "\n",
    "            # initialize a new hidden state at the beginning of each chunk\n",
    "            if(k%chunk_size == 0):\n",
    "                hidden = Tensor(hidden.data, autograd=True)\n",
    "            \n",
    "            input = Tensor(batch[:, k], autograd=True)\n",
    "\n",
    "            # create the word embedding from the input word\n",
    "            rnn_input = embed.forward(input)\n",
    "            # feed the word embedding into the RNN cell to predict the next word\n",
    "            prediction, hidden = model.forward(rnn_input, hidden)\n",
    "    \n",
    "            # compute loss (i.e. compare predicted word from the last RNN cell to last word in the sentence)\n",
    "            target = Tensor(batch[:, k+1], autograd=True)\n",
    "            loss = loss_layers[k].forward(prediction, target)\n",
    "            total_loss += loss.data\n",
    "        \n",
    "            # compute prediction accuracy\n",
    "            for ix in range(batch_size):\n",
    "                if(np.argmax(prediction.data[ix]) == target.data[ix]):\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    incorrect += 1    \n",
    "\n",
    "            # backpropagate the loss gadients\n",
    "            loss.backward()\n",
    "        \n",
    "        # weights optimization\n",
    "        optim.step()\n",
    "\n",
    "    if(iter%5 == 0):\n",
    "        print(f\"Iteration# {iter+1}, Loss: {total_loss}, Accuracy: {float(correct)/(float(correct + incorrect))}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Predicting using sequences of characters:`** In the following example, we will train a neural network to predict the next character in a sequence of characters. The training dateset will be text from Shakespear plays. For this example, we again use truncated backpropagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data from file\n",
    "f = open('shakespear.txt', 'r')\n",
    "raw = f.read() # read character by character\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a vocabulary of all characters in the dataset\n",
    "vocab= list(set(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of vocab character indices\n",
    "char_index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    char_index[word] = i    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the entire dataset into character indices\n",
    "indices = []\n",
    "for char in raw:\n",
    "    indices.append(char_index[char])\n",
    "data = np.array(indices).reshape(-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (6250, 16)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "niters = 500\n",
    "batch_size = 50\n",
    "hidden_neurons = 512\n",
    "chunk_size = 16\n",
    "input_size = 16\n",
    "\n",
    "# since the data is a single very long string of characters, we split it up into smallers sections, each section containing input_size number of characters\n",
    "num_inputs = math.ceil(len(data)/input_size)\n",
    "inputs = np.zeros(shape=(num_inputs, input_size))\n",
    "for i in range(num_inputs):\n",
    "    lo = i * input_size\n",
    "    hi = min((i + 1) * input_size, len(data))\n",
    "    inputs[i,0:hi-lo] = data[lo:hi]\n",
    "inputs = inputs.astype(int)\n",
    "\n",
    "print(f\"inputs shape: {inputs.shape}\")\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "# initialize the RNN layers\n",
    "embed = Embedding(len(vocab), hidden_neurons)\n",
    "# Note: since we're going to feed in outputs from the embedding layer into the RNN cell, the input neurons size needs to be equal to the the length of the embedding vectors, which is the hidden neurons size\n",
    "model = RNNcell(hidden_neurons, hidden_neurons, len(vocab), activation=\"sigmoid\")\n",
    "\n",
    "# initialize loss layers for predictions at each RNN cell\n",
    "loss_layer = CrossEntropyLoss() \n",
    "\n",
    "params = embed.get_parameters() + model.get_parameters() \n",
    "optim = SGD_Optimizer(params, alpha=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration# 1, Loss: 33.235664198492316, Accuracy: 0.90944\n",
      "Iteration# 2, Loss: 34.79418371735777, Accuracy: 0.89936\n",
      "Iteration# 3, Loss: 34.38671587787573, Accuracy: 0.90144\n",
      "Iteration# 4, Loss: 28.06674885239116, Accuracy: 0.9192\n",
      "Iteration# 5, Loss: 29.526388860420205, Accuracy: 0.91472\n",
      "Iteration# 6, Loss: 30.085732485037514, Accuracy: 0.9176\n",
      "Iteration# 7, Loss: 33.862207284008626, Accuracy: 0.90592\n",
      "Iteration# 8, Loss: 33.09242533082873, Accuracy: 0.90736\n",
      "Iteration# 9, Loss: 30.2421906758337, Accuracy: 0.91616\n",
      "Iteration# 10, Loss: 28.27710566066971, Accuracy: 0.91648\n",
      "Iteration# 11, Loss: 28.945244196481674, Accuracy: 0.92016\n",
      "Iteration# 12, Loss: 26.477695921367157, Accuracy: 0.92432\n",
      "Iteration# 13, Loss: 24.05393471026353, Accuracy: 0.92976\n",
      "Iteration# 14, Loss: 21.53344949483074, Accuracy: 0.93888\n",
      "Iteration# 15, Loss: 21.6049184845462, Accuracy: 0.93712\n",
      "Iteration# 16, Loss: 25.024112225507462, Accuracy: 0.92768\n",
      "Iteration# 17, Loss: 22.176474851168177, Accuracy: 0.93744\n",
      "Iteration# 18, Loss: 19.758019997055182, Accuracy: 0.9448\n",
      "Iteration# 19, Loss: 19.53507411817893, Accuracy: 0.94608\n",
      "Iteration# 20, Loss: 19.438853878513623, Accuracy: 0.94416\n",
      "Iteration# 21, Loss: 20.010494987742202, Accuracy: 0.94224\n",
      "Iteration# 22, Loss: 18.635121207335953, Accuracy: 0.9472\n",
      "Iteration# 23, Loss: 18.374062464277134, Accuracy: 0.94768\n",
      "Iteration# 24, Loss: 20.15813923546675, Accuracy: 0.9456\n",
      "Iteration# 25, Loss: 17.202770076378602, Accuracy: 0.95344\n",
      "Iteration# 26, Loss: 19.35007312124202, Accuracy: 0.94704\n",
      "Iteration# 27, Loss: 16.897717956498248, Accuracy: 0.95424\n",
      "Iteration# 28, Loss: 17.44213300697096, Accuracy: 0.95168\n",
      "Iteration# 29, Loss: 17.336646982911244, Accuracy: 0.95056\n",
      "Iteration# 30, Loss: 18.304560653173823, Accuracy: 0.94976\n",
      "Iteration# 31, Loss: 15.092942338884843, Accuracy: 0.95968\n",
      "Iteration# 32, Loss: 14.267744628315675, Accuracy: 0.96144\n",
      "Iteration# 33, Loss: 15.137279183405692, Accuracy: 0.95952\n",
      "Iteration# 34, Loss: 14.83353258002721, Accuracy: 0.9624\n",
      "Iteration# 35, Loss: 15.029784736086922, Accuracy: 0.96112\n",
      "Iteration# 36, Loss: 17.16785935422463, Accuracy: 0.95408\n",
      "Iteration# 37, Loss: 18.047557325112926, Accuracy: 0.94976\n",
      "Iteration# 38, Loss: 16.952137860948675, Accuracy: 0.95488\n",
      "Iteration# 39, Loss: 19.979076447594075, Accuracy: 0.94336\n",
      "Iteration# 40, Loss: 17.733119321761034, Accuracy: 0.94944\n",
      "Iteration# 41, Loss: 16.528951980013577, Accuracy: 0.9536\n",
      "Iteration# 42, Loss: 16.57602044440648, Accuracy: 0.9568\n",
      "Iteration# 43, Loss: 16.216804883712605, Accuracy: 0.95712\n",
      "Iteration# 44, Loss: 15.55264467914017, Accuracy: 0.96128\n",
      "Iteration# 45, Loss: 16.89953211652056, Accuracy: 0.95376\n",
      "Iteration# 46, Loss: 16.63430159841705, Accuracy: 0.95584\n",
      "Iteration# 47, Loss: 16.005801802622592, Accuracy: 0.95632\n",
      "Iteration# 48, Loss: 14.505646996230185, Accuracy: 0.96192\n",
      "Iteration# 49, Loss: 15.630857823000069, Accuracy: 0.9584\n",
      "Iteration# 50, Loss: 15.93702023031935, Accuracy: 0.95952\n",
      "Iteration# 51, Loss: 15.221685510386145, Accuracy: 0.96128\n",
      "Iteration# 52, Loss: 14.525130503528201, Accuracy: 0.96144\n",
      "Iteration# 53, Loss: 14.747661112905593, Accuracy: 0.96192\n",
      "Iteration# 54, Loss: 13.609954924388314, Accuracy: 0.96288\n",
      "Iteration# 55, Loss: 13.34941866745463, Accuracy: 0.96464\n",
      "Iteration# 56, Loss: 13.424687916071584, Accuracy: 0.9632\n",
      "Iteration# 57, Loss: 14.332690764602745, Accuracy: 0.96432\n",
      "Iteration# 58, Loss: 14.087629149290615, Accuracy: 0.95968\n",
      "Iteration# 59, Loss: 14.106248932509658, Accuracy: 0.96384\n",
      "Iteration# 60, Loss: 15.035061431534185, Accuracy: 0.96016\n",
      "Iteration# 61, Loss: 14.8052724124399, Accuracy: 0.95904\n",
      "Iteration# 62, Loss: 15.717988992851827, Accuracy: 0.95824\n",
      "Iteration# 63, Loss: 14.37041507194256, Accuracy: 0.96256\n",
      "Iteration# 64, Loss: 14.06712262311199, Accuracy: 0.96608\n",
      "Iteration# 65, Loss: 13.154179959948364, Accuracy: 0.96608\n",
      "Iteration# 66, Loss: 13.022126632486742, Accuracy: 0.96736\n",
      "Iteration# 67, Loss: 14.62060905470927, Accuracy: 0.96352\n",
      "Iteration# 68, Loss: 13.056596150445689, Accuracy: 0.96512\n",
      "Iteration# 69, Loss: 14.271639688056116, Accuracy: 0.96304\n",
      "Iteration# 70, Loss: 13.59652681772435, Accuracy: 0.96688\n",
      "Iteration# 71, Loss: 15.74451150315317, Accuracy: 0.96112\n",
      "Iteration# 72, Loss: 13.425999475396742, Accuracy: 0.96656\n",
      "Iteration# 73, Loss: 13.859701047087704, Accuracy: 0.96352\n",
      "Iteration# 74, Loss: 13.17583792715091, Accuracy: 0.96496\n",
      "Iteration# 75, Loss: 14.264302703429195, Accuracy: 0.96224\n",
      "Iteration# 76, Loss: 14.084878271237065, Accuracy: 0.96496\n",
      "Iteration# 77, Loss: 14.56889561499925, Accuracy: 0.96272\n",
      "Iteration# 78, Loss: 14.872354435002299, Accuracy: 0.96032\n",
      "Iteration# 79, Loss: 13.983679013116403, Accuracy: 0.96304\n",
      "Iteration# 80, Loss: 14.025445053839865, Accuracy: 0.96512\n",
      "Iteration# 81, Loss: 14.184797647644565, Accuracy: 0.96496\n",
      "Iteration# 82, Loss: 13.735182302907129, Accuracy: 0.96272\n",
      "Iteration# 83, Loss: 13.069541156349054, Accuracy: 0.9672\n",
      "Iteration# 84, Loss: 13.537633000727572, Accuracy: 0.96656\n",
      "Iteration# 85, Loss: 14.180080426089923, Accuracy: 0.96528\n",
      "Iteration# 86, Loss: 13.66313746177652, Accuracy: 0.9656\n",
      "Iteration# 87, Loss: 13.77702772819238, Accuracy: 0.96304\n",
      "Iteration# 88, Loss: 13.589119873095486, Accuracy: 0.9664\n",
      "Iteration# 89, Loss: 14.499023439889033, Accuracy: 0.96512\n",
      "Iteration# 90, Loss: 12.573574740461275, Accuracy: 0.96992\n",
      "Iteration# 91, Loss: 12.435121708446008, Accuracy: 0.97056\n",
      "Iteration# 92, Loss: 11.964725116990001, Accuracy: 0.97088\n",
      "Iteration# 93, Loss: 12.922851667470269, Accuracy: 0.96864\n",
      "Iteration# 94, Loss: 13.22802579749995, Accuracy: 0.96768\n",
      "Iteration# 95, Loss: 12.106763569696284, Accuracy: 0.97248\n",
      "Iteration# 96, Loss: 11.534456703363754, Accuracy: 0.9744\n",
      "Iteration# 97, Loss: 12.752497161742381, Accuracy: 0.9688\n",
      "Iteration# 98, Loss: 13.324394918963732, Accuracy: 0.96928\n",
      "Iteration# 99, Loss: 11.500531469111094, Accuracy: 0.9752\n",
      "Iteration# 100, Loss: 11.120449242671214, Accuracy: 0.97472\n",
      "Iteration# 101, Loss: 11.08791305224574, Accuracy: 0.97376\n",
      "Iteration# 102, Loss: 11.624247995052892, Accuracy: 0.97408\n",
      "Iteration# 103, Loss: 12.124920125340632, Accuracy: 0.97376\n",
      "Iteration# 104, Loss: 11.741507353076873, Accuracy: 0.97376\n",
      "Iteration# 105, Loss: 12.198249782294694, Accuracy: 0.97232\n",
      "Iteration# 106, Loss: 12.338915666709498, Accuracy: 0.97056\n",
      "Iteration# 107, Loss: 12.445255551792414, Accuracy: 0.97104\n",
      "Iteration# 108, Loss: 12.082292887776777, Accuracy: 0.97328\n",
      "Iteration# 109, Loss: 13.37702043304487, Accuracy: 0.97072\n",
      "Iteration# 110, Loss: 13.403632158017022, Accuracy: 0.968\n",
      "Iteration# 111, Loss: 12.989086252892003, Accuracy: 0.96992\n",
      "Iteration# 112, Loss: 12.726370899863163, Accuracy: 0.97136\n",
      "Iteration# 113, Loss: 11.782337369321425, Accuracy: 0.97392\n",
      "Iteration# 114, Loss: 11.043063402216792, Accuracy: 0.97504\n",
      "Iteration# 115, Loss: 12.334381011926919, Accuracy: 0.9696\n",
      "Iteration# 116, Loss: 11.70477376295779, Accuracy: 0.9728\n",
      "Iteration# 117, Loss: 12.178522148655174, Accuracy: 0.97328\n",
      "Iteration# 118, Loss: 12.627875830069831, Accuracy: 0.96864\n",
      "Iteration# 119, Loss: 11.451284224487692, Accuracy: 0.97328\n",
      "Iteration# 120, Loss: 11.726739146182858, Accuracy: 0.97472\n",
      "Iteration# 121, Loss: 11.580455839181207, Accuracy: 0.9752\n",
      "Iteration# 122, Loss: 12.382904937601207, Accuracy: 0.97312\n",
      "Iteration# 123, Loss: 12.23261600481115, Accuracy: 0.9744\n",
      "Iteration# 124, Loss: 11.539152934876947, Accuracy: 0.976\n",
      "Iteration# 125, Loss: 12.006375089419917, Accuracy: 0.97312\n",
      "Iteration# 126, Loss: 11.225141416910942, Accuracy: 0.97648\n",
      "Iteration# 127, Loss: 11.355703354734485, Accuracy: 0.97568\n",
      "Iteration# 128, Loss: 11.519850878117264, Accuracy: 0.9768\n",
      "Iteration# 129, Loss: 11.586347465191968, Accuracy: 0.97616\n",
      "Iteration# 130, Loss: 11.904675322291343, Accuracy: 0.97328\n",
      "Iteration# 131, Loss: 11.766057197505654, Accuracy: 0.97456\n",
      "Iteration# 132, Loss: 11.761557993822915, Accuracy: 0.97376\n",
      "Iteration# 133, Loss: 12.186726997058427, Accuracy: 0.97264\n",
      "Iteration# 134, Loss: 11.602392761283628, Accuracy: 0.97312\n",
      "Iteration# 135, Loss: 11.52893003962308, Accuracy: 0.9752\n",
      "Iteration# 136, Loss: 10.934592345949453, Accuracy: 0.97648\n",
      "Iteration# 137, Loss: 10.682024644956, Accuracy: 0.97728\n",
      "Iteration# 138, Loss: 10.779948275332934, Accuracy: 0.97792\n",
      "Iteration# 139, Loss: 9.742223526828305, Accuracy: 0.98288\n",
      "Iteration# 140, Loss: 9.792250477590915, Accuracy: 0.98224\n",
      "Iteration# 141, Loss: 9.883587067877553, Accuracy: 0.9816\n",
      "Iteration# 142, Loss: 9.81070074785307, Accuracy: 0.9792\n",
      "Iteration# 143, Loss: 9.604361245576737, Accuracy: 0.9832\n",
      "Iteration# 144, Loss: 9.934652834745835, Accuracy: 0.98272\n",
      "Iteration# 145, Loss: 9.875286753601719, Accuracy: 0.98176\n",
      "Iteration# 146, Loss: 10.518703832699419, Accuracy: 0.97952\n",
      "Iteration# 147, Loss: 10.949857910112975, Accuracy: 0.97904\n",
      "Iteration# 148, Loss: 10.933728655737468, Accuracy: 0.97872\n",
      "Iteration# 149, Loss: 10.483037005171584, Accuracy: 0.98224\n",
      "Iteration# 150, Loss: 9.821085551532578, Accuracy: 0.98464\n",
      "Iteration# 151, Loss: 9.056031814504632, Accuracy: 0.98688\n",
      "Iteration# 152, Loss: 9.001210194122088, Accuracy: 0.98608\n",
      "Iteration# 153, Loss: 9.513816046208769, Accuracy: 0.98512\n",
      "Iteration# 154, Loss: 10.330182136363561, Accuracy: 0.98032\n",
      "Iteration# 155, Loss: 9.818260528434573, Accuracy: 0.98192\n",
      "Iteration# 156, Loss: 9.927029305412136, Accuracy: 0.98176\n",
      "Iteration# 157, Loss: 9.724860535313683, Accuracy: 0.98368\n",
      "Iteration# 158, Loss: 9.225460176780471, Accuracy: 0.98656\n",
      "Iteration# 159, Loss: 9.513968861293957, Accuracy: 0.98384\n",
      "Iteration# 160, Loss: 9.084095984769144, Accuracy: 0.98464\n",
      "Iteration# 161, Loss: 8.977435618122229, Accuracy: 0.9856\n",
      "Iteration# 162, Loss: 9.253587760349946, Accuracy: 0.98544\n",
      "Iteration# 163, Loss: 9.5362480299072, Accuracy: 0.98384\n",
      "Iteration# 164, Loss: 9.180361681585582, Accuracy: 0.9824\n",
      "Iteration# 165, Loss: 8.976301151356777, Accuracy: 0.98592\n",
      "Iteration# 166, Loss: 9.042049084214979, Accuracy: 0.98752\n",
      "Iteration# 167, Loss: 9.19270533781257, Accuracy: 0.98448\n",
      "Iteration# 168, Loss: 8.764600472300419, Accuracy: 0.98672\n",
      "Iteration# 169, Loss: 8.693499025714722, Accuracy: 0.9856\n",
      "Iteration# 170, Loss: 8.661384090051243, Accuracy: 0.9864\n",
      "Iteration# 171, Loss: 8.956441261865796, Accuracy: 0.98448\n",
      "Iteration# 172, Loss: 9.272743996339152, Accuracy: 0.984\n",
      "Iteration# 173, Loss: 9.169779487235694, Accuracy: 0.98352\n",
      "Iteration# 174, Loss: 9.392498715614922, Accuracy: 0.98432\n",
      "Iteration# 175, Loss: 9.69465358720821, Accuracy: 0.98288\n",
      "Iteration# 176, Loss: 9.334265652891785, Accuracy: 0.98432\n",
      "Iteration# 177, Loss: 9.461235955313901, Accuracy: 0.98144\n",
      "Iteration# 178, Loss: 9.212406485778805, Accuracy: 0.9832\n",
      "Iteration# 179, Loss: 8.820367852152328, Accuracy: 0.98592\n",
      "Iteration# 180, Loss: 8.497959983360676, Accuracy: 0.98688\n",
      "Iteration# 181, Loss: 8.967186929372641, Accuracy: 0.98544\n",
      "Iteration# 182, Loss: 9.061659450064095, Accuracy: 0.98416\n",
      "Iteration# 183, Loss: 9.083954995635766, Accuracy: 0.98416\n",
      "Iteration# 184, Loss: 8.939528450997427, Accuracy: 0.98448\n",
      "Iteration# 185, Loss: 8.684918876417912, Accuracy: 0.9864\n",
      "Iteration# 186, Loss: 8.608163591325846, Accuracy: 0.9872\n",
      "Iteration# 187, Loss: 8.933641514786085, Accuracy: 0.98464\n",
      "Iteration# 188, Loss: 8.869388893488313, Accuracy: 0.9856\n",
      "Iteration# 189, Loss: 8.352618282835087, Accuracy: 0.988\n",
      "Iteration# 190, Loss: 7.983388706875797, Accuracy: 0.98896\n",
      "Iteration# 191, Loss: 7.902029258167576, Accuracy: 0.98848\n",
      "Iteration# 192, Loss: 7.826036219138439, Accuracy: 0.9888\n",
      "Iteration# 193, Loss: 8.181004290889192, Accuracy: 0.9872\n",
      "Iteration# 194, Loss: 8.289298037537073, Accuracy: 0.98704\n",
      "Iteration# 195, Loss: 8.499243038297355, Accuracy: 0.98592\n",
      "Iteration# 196, Loss: 8.237118526170123, Accuracy: 0.98736\n",
      "Iteration# 197, Loss: 8.176793968417538, Accuracy: 0.98672\n",
      "Iteration# 198, Loss: 8.271513751189193, Accuracy: 0.98768\n",
      "Iteration# 199, Loss: 8.369788435428651, Accuracy: 0.9872\n",
      "Iteration# 200, Loss: 8.406780386039587, Accuracy: 0.98576\n",
      "Iteration# 201, Loss: 8.508038146341624, Accuracy: 0.98704\n",
      "Iteration# 202, Loss: 8.336307655789058, Accuracy: 0.98816\n",
      "Iteration# 203, Loss: 8.97126678426957, Accuracy: 0.9864\n",
      "Iteration# 204, Loss: 9.136251370478918, Accuracy: 0.98528\n",
      "Iteration# 205, Loss: 9.031963091308697, Accuracy: 0.98672\n",
      "Iteration# 206, Loss: 8.813282940244134, Accuracy: 0.9872\n",
      "Iteration# 207, Loss: 8.982097994885585, Accuracy: 0.98672\n",
      "Iteration# 208, Loss: 8.976395910489899, Accuracy: 0.98496\n",
      "Iteration# 209, Loss: 8.606355344925799, Accuracy: 0.9856\n",
      "Iteration# 210, Loss: 8.459111580797137, Accuracy: 0.98656\n",
      "Iteration# 211, Loss: 8.516035037713, Accuracy: 0.98656\n",
      "Iteration# 212, Loss: 8.52734024412251, Accuracy: 0.98624\n",
      "Iteration# 213, Loss: 8.560489992831242, Accuracy: 0.98768\n",
      "Iteration# 214, Loss: 8.551187908512023, Accuracy: 0.9872\n",
      "Iteration# 215, Loss: 8.398991225737134, Accuracy: 0.988\n",
      "Iteration# 216, Loss: 8.490223895371932, Accuracy: 0.9856\n",
      "Iteration# 217, Loss: 8.424176620001248, Accuracy: 0.98608\n",
      "Iteration# 218, Loss: 8.22578332603482, Accuracy: 0.98672\n",
      "Iteration# 219, Loss: 8.39752112732444, Accuracy: 0.98672\n",
      "Iteration# 220, Loss: 8.21575217807318, Accuracy: 0.98656\n",
      "Iteration# 221, Loss: 8.421071780338899, Accuracy: 0.98608\n",
      "Iteration# 222, Loss: 8.23122181470671, Accuracy: 0.98624\n",
      "Iteration# 223, Loss: 8.054065911108562, Accuracy: 0.98736\n",
      "Iteration# 224, Loss: 7.686889682172207, Accuracy: 0.98832\n",
      "Iteration# 225, Loss: 7.630021532589682, Accuracy: 0.98832\n",
      "Iteration# 226, Loss: 7.543542287348906, Accuracy: 0.9896\n",
      "Iteration# 227, Loss: 7.549270063620981, Accuracy: 0.98944\n",
      "Iteration# 228, Loss: 7.416866091465237, Accuracy: 0.98832\n",
      "Iteration# 229, Loss: 7.342114398548393, Accuracy: 0.9896\n",
      "Iteration# 230, Loss: 7.2240128644739405, Accuracy: 0.99088\n",
      "Iteration# 231, Loss: 7.265040336313292, Accuracy: 0.98928\n",
      "Iteration# 232, Loss: 7.180859550167705, Accuracy: 0.98976\n",
      "Iteration# 233, Loss: 7.091146139967449, Accuracy: 0.98976\n",
      "Iteration# 234, Loss: 7.022314004851233, Accuracy: 0.99184\n",
      "Iteration# 235, Loss: 7.009879628352758, Accuracy: 0.99184\n",
      "Iteration# 236, Loss: 7.098029100644512, Accuracy: 0.99152\n",
      "Iteration# 237, Loss: 7.1555350673493, Accuracy: 0.99024\n",
      "Iteration# 238, Loss: 7.054213009061632, Accuracy: 0.99072\n",
      "Iteration# 239, Loss: 6.959408752587409, Accuracy: 0.99184\n",
      "Iteration# 240, Loss: 7.154975133988123, Accuracy: 0.99088\n",
      "Iteration# 241, Loss: 7.256746804176755, Accuracy: 0.99088\n",
      "Iteration# 242, Loss: 7.213918649979285, Accuracy: 0.99136\n",
      "Iteration# 243, Loss: 7.439469251011773, Accuracy: 0.98992\n",
      "Iteration# 244, Loss: 7.562952306704933, Accuracy: 0.98896\n",
      "Iteration# 245, Loss: 7.65000348989756, Accuracy: 0.98832\n",
      "Iteration# 246, Loss: 7.660156120969141, Accuracy: 0.98752\n",
      "Iteration# 247, Loss: 7.498832664150686, Accuracy: 0.98928\n",
      "Iteration# 248, Loss: 7.398406842795079, Accuracy: 0.98928\n",
      "Iteration# 249, Loss: 7.301347842691793, Accuracy: 0.98928\n",
      "Iteration# 250, Loss: 7.233811331958884, Accuracy: 0.98944\n",
      "Iteration# 251, Loss: 7.421961992867994, Accuracy: 0.99008\n",
      "Iteration# 252, Loss: 7.341716923053445, Accuracy: 0.98976\n",
      "Iteration# 253, Loss: 7.264268744392143, Accuracy: 0.9904\n",
      "Iteration# 254, Loss: 7.024547373698082, Accuracy: 0.99088\n",
      "Iteration# 255, Loss: 6.933037007500522, Accuracy: 0.99136\n",
      "Iteration# 256, Loss: 6.869428649941263, Accuracy: 0.992\n",
      "Iteration# 257, Loss: 6.772847149667211, Accuracy: 0.99168\n",
      "Iteration# 258, Loss: 6.649297282053911, Accuracy: 0.99264\n",
      "Iteration# 259, Loss: 6.5864131705646445, Accuracy: 0.99232\n",
      "Iteration# 260, Loss: 6.610135296463477, Accuracy: 0.99232\n",
      "Iteration# 261, Loss: 6.520860112518285, Accuracy: 0.992\n",
      "Iteration# 262, Loss: 6.518446759787357, Accuracy: 0.99264\n",
      "Iteration# 263, Loss: 6.495980751288956, Accuracy: 0.99296\n",
      "Iteration# 264, Loss: 6.471730641182741, Accuracy: 0.99248\n",
      "Iteration# 265, Loss: 6.4834318110621005, Accuracy: 0.99312\n",
      "Iteration# 266, Loss: 6.543460919689388, Accuracy: 0.99312\n",
      "Iteration# 267, Loss: 6.535891074063969, Accuracy: 0.992\n",
      "Iteration# 268, Loss: 6.511521499033719, Accuracy: 0.99312\n",
      "Iteration# 269, Loss: 6.567651639760438, Accuracy: 0.99296\n",
      "Iteration# 270, Loss: 6.558722891041573, Accuracy: 0.99392\n",
      "Iteration# 271, Loss: 6.599873581802175, Accuracy: 0.99344\n",
      "Iteration# 272, Loss: 6.705848024956496, Accuracy: 0.992\n",
      "Iteration# 273, Loss: 6.7592003801359235, Accuracy: 0.99216\n",
      "Iteration# 274, Loss: 6.717575932514443, Accuracy: 0.99136\n",
      "Iteration# 275, Loss: 6.691967707617088, Accuracy: 0.99104\n",
      "Iteration# 276, Loss: 6.643428236950858, Accuracy: 0.99248\n",
      "Iteration# 277, Loss: 6.7816959474929, Accuracy: 0.9912\n",
      "Iteration# 278, Loss: 6.740163243737493, Accuracy: 0.99152\n",
      "Iteration# 279, Loss: 6.785085439257562, Accuracy: 0.99168\n",
      "Iteration# 280, Loss: 6.7951218544976495, Accuracy: 0.99232\n",
      "Iteration# 281, Loss: 6.827467694927165, Accuracy: 0.992\n",
      "Iteration# 282, Loss: 6.848604564342817, Accuracy: 0.99104\n",
      "Iteration# 283, Loss: 6.827410787681246, Accuracy: 0.99136\n",
      "Iteration# 284, Loss: 6.702283491603579, Accuracy: 0.99024\n",
      "Iteration# 285, Loss: 6.696271387223293, Accuracy: 0.99008\n",
      "Iteration# 286, Loss: 6.6649237364773946, Accuracy: 0.9904\n",
      "Iteration# 287, Loss: 6.692198010155408, Accuracy: 0.99088\n",
      "Iteration# 288, Loss: 6.803138890480558, Accuracy: 0.99152\n",
      "Iteration# 289, Loss: 6.851460044041996, Accuracy: 0.99056\n",
      "Iteration# 290, Loss: 6.803351525139307, Accuracy: 0.99152\n",
      "Iteration# 291, Loss: 6.7404898617850595, Accuracy: 0.99088\n",
      "Iteration# 292, Loss: 6.668759569349394, Accuracy: 0.99088\n",
      "Iteration# 293, Loss: 6.661613405574677, Accuracy: 0.99136\n",
      "Iteration# 294, Loss: 6.573561379033018, Accuracy: 0.99136\n",
      "Iteration# 295, Loss: 6.559804207044152, Accuracy: 0.99104\n",
      "Iteration# 296, Loss: 6.564111085343406, Accuracy: 0.9912\n",
      "Iteration# 297, Loss: 6.583890440002817, Accuracy: 0.99136\n",
      "Iteration# 298, Loss: 6.603333878078229, Accuracy: 0.99232\n",
      "Iteration# 299, Loss: 6.596169075604004, Accuracy: 0.99248\n",
      "Iteration# 300, Loss: 6.545512824366812, Accuracy: 0.99216\n",
      "Iteration# 301, Loss: 6.4622615474687235, Accuracy: 0.99232\n",
      "Iteration# 302, Loss: 6.509562742010036, Accuracy: 0.99168\n",
      "Iteration# 303, Loss: 6.444191216446029, Accuracy: 0.99184\n",
      "Iteration# 304, Loss: 6.437842126945262, Accuracy: 0.99216\n",
      "Iteration# 305, Loss: 6.505286750490195, Accuracy: 0.99168\n",
      "Iteration# 306, Loss: 6.501981762290561, Accuracy: 0.99184\n",
      "Iteration# 307, Loss: 6.535216246200139, Accuracy: 0.9912\n",
      "Iteration# 308, Loss: 6.508172922863281, Accuracy: 0.99152\n",
      "Iteration# 309, Loss: 6.554853636552646, Accuracy: 0.9912\n",
      "Iteration# 310, Loss: 6.611976318623041, Accuracy: 0.99024\n",
      "Iteration# 311, Loss: 6.669638913773812, Accuracy: 0.9904\n",
      "Iteration# 312, Loss: 6.79238428970772, Accuracy: 0.99072\n",
      "Iteration# 313, Loss: 6.77132811612918, Accuracy: 0.9904\n",
      "Iteration# 314, Loss: 6.91067625370987, Accuracy: 0.98976\n",
      "Iteration# 315, Loss: 7.034994996200902, Accuracy: 0.99056\n",
      "Iteration# 316, Loss: 7.047133931343935, Accuracy: 0.99056\n",
      "Iteration# 317, Loss: 7.013472584836994, Accuracy: 0.99088\n",
      "Iteration# 318, Loss: 6.997802002471567, Accuracy: 0.99072\n",
      "Iteration# 319, Loss: 7.011554095781054, Accuracy: 0.98976\n",
      "Iteration# 320, Loss: 6.997968203218839, Accuracy: 0.98928\n",
      "Iteration# 321, Loss: 6.937973164911988, Accuracy: 0.98944\n",
      "Iteration# 322, Loss: 6.9737410761198095, Accuracy: 0.98928\n",
      "Iteration# 323, Loss: 7.052848202673383, Accuracy: 0.98896\n",
      "Iteration# 324, Loss: 7.107092092953144, Accuracy: 0.98928\n",
      "Iteration# 325, Loss: 7.17868723600583, Accuracy: 0.98992\n",
      "Iteration# 326, Loss: 7.149398754772962, Accuracy: 0.9904\n",
      "Iteration# 327, Loss: 7.043996619508061, Accuracy: 0.99072\n",
      "Iteration# 328, Loss: 7.0380669984048385, Accuracy: 0.99136\n",
      "Iteration# 329, Loss: 6.938624771065766, Accuracy: 0.99104\n",
      "Iteration# 330, Loss: 6.903518221702042, Accuracy: 0.99152\n",
      "Iteration# 331, Loss: 6.821872334193345, Accuracy: 0.992\n",
      "Iteration# 332, Loss: 6.713823489055011, Accuracy: 0.99184\n",
      "Iteration# 333, Loss: 6.703049100993881, Accuracy: 0.99152\n",
      "Iteration# 334, Loss: 6.667781353855055, Accuracy: 0.992\n",
      "Iteration# 335, Loss: 6.632234445000329, Accuracy: 0.99232\n",
      "Iteration# 336, Loss: 6.655495050294902, Accuracy: 0.992\n",
      "Iteration# 337, Loss: 6.663368760023614, Accuracy: 0.99264\n",
      "Iteration# 338, Loss: 6.630372204541474, Accuracy: 0.99216\n",
      "Iteration# 339, Loss: 6.586416632122719, Accuracy: 0.992\n",
      "Iteration# 340, Loss: 6.552738316855186, Accuracy: 0.99232\n",
      "Iteration# 341, Loss: 6.564918013401598, Accuracy: 0.99216\n",
      "Iteration# 342, Loss: 6.626048272530353, Accuracy: 0.99216\n",
      "Iteration# 343, Loss: 6.684635899709998, Accuracy: 0.99184\n",
      "Iteration# 344, Loss: 6.76472666579562, Accuracy: 0.99168\n",
      "Iteration# 345, Loss: 6.7530716698523, Accuracy: 0.99152\n",
      "Iteration# 346, Loss: 6.655519413086512, Accuracy: 0.99248\n",
      "Iteration# 347, Loss: 6.585228830094753, Accuracy: 0.9928\n",
      "Iteration# 348, Loss: 6.559574622438886, Accuracy: 0.99248\n",
      "Iteration# 349, Loss: 6.4999570237723185, Accuracy: 0.99232\n",
      "Iteration# 350, Loss: 6.406203749432972, Accuracy: 0.99216\n",
      "Iteration# 351, Loss: 6.280770353301975, Accuracy: 0.99296\n",
      "Iteration# 352, Loss: 6.159915787542744, Accuracy: 0.99328\n",
      "Iteration# 353, Loss: 6.064109295044416, Accuracy: 0.9928\n",
      "Iteration# 354, Loss: 5.995391712322856, Accuracy: 0.99344\n",
      "Iteration# 355, Loss: 6.014628445707903, Accuracy: 0.9928\n",
      "Iteration# 356, Loss: 6.02539838648066, Accuracy: 0.99248\n",
      "Iteration# 357, Loss: 5.934548216269791, Accuracy: 0.99264\n",
      "Iteration# 358, Loss: 5.896009441657829, Accuracy: 0.99296\n",
      "Iteration# 359, Loss: 5.908283011778105, Accuracy: 0.99312\n",
      "Iteration# 360, Loss: 5.9186980575072585, Accuracy: 0.99264\n",
      "Iteration# 361, Loss: 5.877556095374844, Accuracy: 0.99232\n",
      "Iteration# 362, Loss: 5.856368897256315, Accuracy: 0.99248\n",
      "Iteration# 363, Loss: 5.849863653325766, Accuracy: 0.99264\n",
      "Iteration# 364, Loss: 5.828496278950656, Accuracy: 0.99264\n",
      "Iteration# 365, Loss: 5.820699854821407, Accuracy: 0.99296\n",
      "Iteration# 366, Loss: 5.8166857287639475, Accuracy: 0.9936\n",
      "Iteration# 367, Loss: 5.8195763579042135, Accuracy: 0.99392\n",
      "Iteration# 368, Loss: 5.822291291195554, Accuracy: 0.99424\n",
      "Iteration# 369, Loss: 5.828135639521063, Accuracy: 0.99408\n",
      "Iteration# 370, Loss: 5.813926904778359, Accuracy: 0.9944\n",
      "Iteration# 371, Loss: 5.817353355514087, Accuracy: 0.99456\n",
      "Iteration# 372, Loss: 5.826868895484681, Accuracy: 0.99488\n",
      "Iteration# 373, Loss: 5.820344809368177, Accuracy: 0.99456\n",
      "Iteration# 374, Loss: 5.845706421239461, Accuracy: 0.99424\n",
      "Iteration# 375, Loss: 5.877632086553497, Accuracy: 0.99376\n",
      "Iteration# 376, Loss: 5.901338726279065, Accuracy: 0.9936\n",
      "Iteration# 377, Loss: 5.931077717267245, Accuracy: 0.99376\n",
      "Iteration# 378, Loss: 5.956424573688355, Accuracy: 0.99376\n",
      "Iteration# 379, Loss: 5.964754232705895, Accuracy: 0.99376\n",
      "Iteration# 380, Loss: 5.97372270153782, Accuracy: 0.99408\n",
      "Iteration# 381, Loss: 5.985569990985652, Accuracy: 0.99408\n",
      "Iteration# 382, Loss: 6.019376061471036, Accuracy: 0.99296\n",
      "Iteration# 383, Loss: 6.026629052057181, Accuracy: 0.9928\n",
      "Iteration# 384, Loss: 6.053618681650421, Accuracy: 0.9928\n",
      "Iteration# 385, Loss: 6.095598322512529, Accuracy: 0.9928\n",
      "Iteration# 386, Loss: 6.135740966453723, Accuracy: 0.99216\n",
      "Iteration# 387, Loss: 6.14788318977242, Accuracy: 0.99232\n",
      "Iteration# 388, Loss: 6.127858517607575, Accuracy: 0.99232\n",
      "Iteration# 389, Loss: 6.105262302811643, Accuracy: 0.9928\n",
      "Iteration# 390, Loss: 6.098686300614682, Accuracy: 0.99296\n",
      "Iteration# 391, Loss: 6.11705162595975, Accuracy: 0.99264\n",
      "Iteration# 392, Loss: 6.131884091695862, Accuracy: 0.99184\n",
      "Iteration# 393, Loss: 6.16029931533028, Accuracy: 0.99264\n",
      "Iteration# 394, Loss: 6.188165508322238, Accuracy: 0.99264\n",
      "Iteration# 395, Loss: 6.228114067115212, Accuracy: 0.99312\n",
      "Iteration# 396, Loss: 6.213741974231335, Accuracy: 0.99312\n",
      "Iteration# 397, Loss: 6.181613968437284, Accuracy: 0.99328\n",
      "Iteration# 398, Loss: 6.2001492304025705, Accuracy: 0.99328\n",
      "Iteration# 399, Loss: 6.224597410995511, Accuracy: 0.99296\n",
      "Iteration# 400, Loss: 6.188617315383124, Accuracy: 0.99312\n",
      "Iteration# 401, Loss: 6.1595493957336345, Accuracy: 0.99328\n",
      "Iteration# 402, Loss: 6.15808195589602, Accuracy: 0.9928\n",
      "Iteration# 403, Loss: 6.170474787523064, Accuracy: 0.99248\n",
      "Iteration# 404, Loss: 6.183359304214661, Accuracy: 0.99216\n",
      "Iteration# 405, Loss: 6.173152566194492, Accuracy: 0.99216\n",
      "Iteration# 406, Loss: 6.16337149522708, Accuracy: 0.99264\n",
      "Iteration# 407, Loss: 6.158470173144295, Accuracy: 0.99296\n",
      "Iteration# 408, Loss: 6.170361150978433, Accuracy: 0.9928\n",
      "Iteration# 409, Loss: 6.189370216167262, Accuracy: 0.99296\n",
      "Iteration# 410, Loss: 6.212544891561953, Accuracy: 0.99248\n",
      "Iteration# 411, Loss: 6.22344662977343, Accuracy: 0.99264\n",
      "Iteration# 412, Loss: 6.210685427028078, Accuracy: 0.99312\n",
      "Iteration# 413, Loss: 6.205092063002398, Accuracy: 0.99296\n",
      "Iteration# 414, Loss: 6.203320957711475, Accuracy: 0.9928\n",
      "Iteration# 415, Loss: 6.206096243815626, Accuracy: 0.99248\n",
      "Iteration# 416, Loss: 6.171459337546014, Accuracy: 0.99344\n",
      "Iteration# 417, Loss: 6.13037564110094, Accuracy: 0.9936\n",
      "Iteration# 418, Loss: 6.111529556659011, Accuracy: 0.99328\n",
      "Iteration# 419, Loss: 6.0801455722218956, Accuracy: 0.99296\n",
      "Iteration# 420, Loss: 6.040356582809922, Accuracy: 0.99344\n",
      "Iteration# 421, Loss: 6.006281616815141, Accuracy: 0.99344\n",
      "Iteration# 422, Loss: 5.97797453326879, Accuracy: 0.9936\n",
      "Iteration# 423, Loss: 5.957586089276598, Accuracy: 0.9936\n",
      "Iteration# 424, Loss: 5.9315560085745425, Accuracy: 0.9936\n",
      "Iteration# 425, Loss: 5.903481185713843, Accuracy: 0.99376\n",
      "Iteration# 426, Loss: 5.884763631205267, Accuracy: 0.99424\n",
      "Iteration# 427, Loss: 5.8680789180722055, Accuracy: 0.99408\n",
      "Iteration# 428, Loss: 5.842340051775907, Accuracy: 0.99424\n",
      "Iteration# 429, Loss: 5.820028741961817, Accuracy: 0.9944\n",
      "Iteration# 430, Loss: 5.804340487584583, Accuracy: 0.99488\n",
      "Iteration# 431, Loss: 5.806609139227607, Accuracy: 0.99504\n",
      "Iteration# 432, Loss: 5.804819506801531, Accuracy: 0.99472\n",
      "Iteration# 433, Loss: 5.7994807794908105, Accuracy: 0.99424\n",
      "Iteration# 434, Loss: 5.789314744356384, Accuracy: 0.99408\n",
      "Iteration# 435, Loss: 5.775619644918776, Accuracy: 0.99392\n",
      "Iteration# 436, Loss: 5.773183877171997, Accuracy: 0.99408\n",
      "Iteration# 437, Loss: 5.7600449811053185, Accuracy: 0.9944\n",
      "Iteration# 438, Loss: 5.805209753820953, Accuracy: 0.99472\n",
      "Iteration# 439, Loss: 5.81506181017149, Accuracy: 0.99488\n",
      "Iteration# 440, Loss: 5.7380936096076525, Accuracy: 0.9952\n",
      "Iteration# 441, Loss: 5.73290826480508, Accuracy: 0.9952\n",
      "Iteration# 442, Loss: 5.719665650997924, Accuracy: 0.99552\n",
      "Iteration# 443, Loss: 5.705076451773648, Accuracy: 0.99536\n",
      "Iteration# 444, Loss: 5.695287049379719, Accuracy: 0.99536\n",
      "Iteration# 445, Loss: 5.694416046537886, Accuracy: 0.99536\n",
      "Iteration# 446, Loss: 5.696616546624132, Accuracy: 0.99536\n",
      "Iteration# 447, Loss: 5.7003621163720055, Accuracy: 0.9952\n",
      "Iteration# 448, Loss: 5.7001381204806965, Accuracy: 0.99536\n",
      "Iteration# 449, Loss: 5.721009972106569, Accuracy: 0.99504\n",
      "Iteration# 450, Loss: 5.72071270011543, Accuracy: 0.99504\n",
      "Iteration# 451, Loss: 5.7041650682733795, Accuracy: 0.99504\n",
      "Iteration# 452, Loss: 5.685106389322808, Accuracy: 0.9952\n",
      "Iteration# 453, Loss: 5.680024011405641, Accuracy: 0.99536\n",
      "Iteration# 454, Loss: 5.6845727616001085, Accuracy: 0.99536\n",
      "Iteration# 455, Loss: 5.685813348308245, Accuracy: 0.9952\n",
      "Iteration# 456, Loss: 5.68326134146432, Accuracy: 0.99552\n",
      "Iteration# 457, Loss: 5.685295835185757, Accuracy: 0.99552\n",
      "Iteration# 458, Loss: 5.691166752100833, Accuracy: 0.99552\n",
      "Iteration# 459, Loss: 5.692785216574241, Accuracy: 0.99536\n",
      "Iteration# 460, Loss: 5.680415238745072, Accuracy: 0.99552\n",
      "Iteration# 461, Loss: 5.67805141072306, Accuracy: 0.99568\n",
      "Iteration# 462, Loss: 5.677576573374215, Accuracy: 0.99584\n",
      "Iteration# 463, Loss: 5.681020170071778, Accuracy: 0.99568\n",
      "Iteration# 464, Loss: 5.67040407953999, Accuracy: 0.99584\n",
      "Iteration# 465, Loss: 5.678313590529533, Accuracy: 0.99568\n",
      "Iteration# 466, Loss: 5.673798216271842, Accuracy: 0.9952\n",
      "Iteration# 467, Loss: 5.672515985920071, Accuracy: 0.99504\n",
      "Iteration# 468, Loss: 5.671392312274207, Accuracy: 0.99504\n",
      "Iteration# 469, Loss: 5.669640609797461, Accuracy: 0.9952\n",
      "Iteration# 470, Loss: 5.664189891945842, Accuracy: 0.9952\n",
      "Iteration# 471, Loss: 5.656868478674988, Accuracy: 0.99504\n",
      "Iteration# 472, Loss: 5.645132797261265, Accuracy: 0.99488\n",
      "Iteration# 473, Loss: 5.641888894387738, Accuracy: 0.99456\n",
      "Iteration# 474, Loss: 5.6418733035838216, Accuracy: 0.99472\n",
      "Iteration# 475, Loss: 5.638717826462782, Accuracy: 0.9944\n",
      "Iteration# 476, Loss: 5.636874242944784, Accuracy: 0.9944\n",
      "Iteration# 477, Loss: 5.633896168177686, Accuracy: 0.99408\n",
      "Iteration# 478, Loss: 5.635185713360518, Accuracy: 0.9944\n",
      "Iteration# 479, Loss: 5.6445717496608125, Accuracy: 0.99456\n",
      "Iteration# 480, Loss: 5.654573087155151, Accuracy: 0.99424\n",
      "Iteration# 481, Loss: 5.66364799123665, Accuracy: 0.99408\n",
      "Iteration# 482, Loss: 5.668791460104236, Accuracy: 0.99392\n",
      "Iteration# 483, Loss: 5.671245319739892, Accuracy: 0.99376\n",
      "Iteration# 484, Loss: 5.667980589153579, Accuracy: 0.99408\n",
      "Iteration# 485, Loss: 5.6738136521709714, Accuracy: 0.99424\n",
      "Iteration# 486, Loss: 5.686092407271725, Accuracy: 0.99408\n",
      "Iteration# 487, Loss: 5.698405090871691, Accuracy: 0.99424\n",
      "Iteration# 488, Loss: 5.704433514956415, Accuracy: 0.99424\n",
      "Iteration# 489, Loss: 5.70542861882254, Accuracy: 0.9944\n",
      "Iteration# 490, Loss: 5.708835715091476, Accuracy: 0.9944\n",
      "Iteration# 491, Loss: 5.715374148471459, Accuracy: 0.99472\n",
      "Iteration# 492, Loss: 5.725659032325527, Accuracy: 0.9944\n",
      "Iteration# 493, Loss: 5.7317454725499015, Accuracy: 0.99424\n",
      "Iteration# 494, Loss: 5.727395645279234, Accuracy: 0.99424\n",
      "Iteration# 495, Loss: 5.722785016784395, Accuracy: 0.9944\n",
      "Iteration# 496, Loss: 5.724093755878782, Accuracy: 0.99424\n",
      "Iteration# 497, Loss: 5.723605302151456, Accuracy: 0.99408\n",
      "Iteration# 498, Loss: 5.721530861042686, Accuracy: 0.99408\n",
      "Iteration# 499, Loss: 5.723108112740101, Accuracy: 0.99408\n",
      "Iteration# 500, Loss: 5.731059819427537, Accuracy: 0.99408\n"
     ]
    }
   ],
   "source": [
    "# make sure input data set is divisible by batch size\n",
    "if(inputs.shape[0]%batch_size != 0):\n",
    "    raise Exception(\"ERROR! Input dataset needs to be divisible by batch_size\")\n",
    "\n",
    "# train the network to predict the next word in the given input sequence\n",
    "for iter in range(niters):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    # initilaize hidden state\n",
    "    hidden = model.init_hidden(batch_size) \n",
    "\n",
    "    # train in batches\n",
    "    for j in range(int(inputs.shape[0]/batch_size)):\n",
    "    \n",
    "        batch_lo = j * batch_size \n",
    "        batch_hi = min((j+1) * batch_size, inputs.shape[0]) \n",
    "        batch = inputs[batch_lo:batch_hi]\n",
    "        \n",
    "        # forward pass through RNN cells \n",
    "        for k in range(batch.shape[1]-1):\n",
    "\n",
    "            # initialize a new hidden state at the beginning of each chunk\n",
    "            if(k%chunk_size == 0):\n",
    "                hidden = Tensor(hidden.data, autograd=True)\n",
    "            \n",
    "            input = Tensor(batch[:, k], autograd=True)\n",
    "            # create the word embedding from the input word\n",
    "            rnn_input = embed.forward(input)\n",
    "            # feed the word embedding into the RNN cell to predict the next word\n",
    "            prediction, hidden = model.forward(rnn_input, hidden)\n",
    "    \n",
    "        # compute loss (i.e. compare predicted word from the last RNN cell to last word in the sentence)\n",
    "        target = Tensor(batch[:, k+1], autograd=True)\n",
    "        loss = loss_layer.forward(prediction, target)\n",
    "        total_loss += loss.data\n",
    "    \n",
    "        # compute prediction accuracy\n",
    "        for ix in range(batch_size):\n",
    "            if(np.argmax(prediction.data[ix]) == target.data[ix]):\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1    \n",
    "\n",
    "        # backpropagate the loss gadients\n",
    "        loss.backward()\n",
    "    \n",
    "        # weights optimization\n",
    "        optim.step()\n",
    "\n",
    "    # exponentially decay the learning rate\n",
    "    optim.alpha *= 0.99\n",
    "\n",
    "    if(iter%1 == 0):\n",
    "        print(f\"Iteration# {iter+1}, Loss: {total_loss}, Accuracy: {float(correct)/(float(correct + incorrect))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(num_characters=300, first_character=\" \", use_temperature=False):\n",
    "    \n",
    "    # initilaize hidden state and input character\n",
    "    hidden = model.init_hidden(batch_size= 1) \n",
    "    input = Tensor(np.array([char_index[first_character]]), autograd=True)\n",
    "    input_indices = [input]\n",
    "    s = \"\"+first_character\n",
    "\n",
    "    # forward pass through RNN cells to predict the next characters\n",
    "    for i in range(num_characters):\n",
    "\n",
    "        # create the word embedding from the input word\n",
    "        rnn_input = embed.forward(input)\n",
    "        # feed the word embedding into the RNN cell to predict the next word\n",
    "        prediction, hidden = model.forward(rnn_input, hidden)\n",
    "\n",
    "        if(use_temperature):\n",
    "            # instead of choosing the character with the highest predicted probability, we create a \"temperature distribution\"\n",
    "            # which is just the prediction amplified by a constant and passed through the softmax function \n",
    "            prediction.data *= (1.0/0.099)\n",
    "            temperature_dist = prediction.softmax()\n",
    "            temperature_dist.data = temperature_dist.data * (1/temperature_dist.sum(1).data)\n",
    " \n",
    "            # then we randomly sample this temperature distribution to get the next character in the sequence\n",
    "            next_char_index = (temperature_dist.data > np.random.rand()).argmax()\n",
    "        else:\n",
    "             next_char_index = np.argmax(prediction.data[0])\n",
    "               \n",
    "        next_char = vocab[next_char_index]\n",
    "        input = Tensor(np.array([next_char_index]), autograd=True)\n",
    "        s += next_char\n",
    "    return s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boak thice of el whither the tides coun,\n",
      "Whice grear you in eng mays:\n",
      "No. ffet even the juil, see cound to spoind:\n",
      "What like as that his aep and thut, for head;\n",
      "eoy and with the subll this caur seace;\n",
      "\n",
      "CRosell, but you and I most heart, with me see be call whither the dies fear.\n",
      "Mers all tar to the gut with the subll thich ghat his amprey with my down?\n",
      "And see the couner gractis par\n",
      "Dour fatheem thee, I will racity, ety my parl with veleg cour port is father, and, and flain'd them gracity\n",
      "your grtace geghtws ound:\n",
      "What I have\n",
      "Crest not my lord, my Lord:\n",
      "What gis thou gracitle,\n",
      "When our awstace grow now all, 'frited the gutms uport in ever me see our aws a botion wishall the masters.\n",
      "\n",
      "HING PUS:\n",
      "Whice of eace gesh name weet the bract be calleaplu,\n",
      "Master! even the juil, soul,\n",
      "They put them themstre: how by tescoundem-wroaghtre: villair orrouse? I way him,\n",
      "Byour gash with veles 'd storn the stace grself him:\n",
      "Come, sten dy mentlems rot for heart, had to the heart, hand, my lord. of the juds\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(num_characters=1000, first_character=\"B\",use_temperature=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JARLASLO:\n",
      "Go bad light row now shall name wets of thy domeep these mastersGis came gondemonw him,\n",
      "Whith a then but a GovGA:\n",
      "Detted, that did so poin my lord, my Lord:\n",
      "What gis threakes of theGoless if his to hath with and thut, for head;\n",
      "Det not Toayss more\n",
      "Inghemorrobeh cound he the heart, hand, my lord, my lord.\n",
      "\n",
      "KING HEN:\n",
      "Mentlems all that wife of ever me see him,\n",
      "And with the subll thich gritle, storiGy'look oG storn therefore, wnaGoined.\n",
      "\n",
      "KING HENGLOGHLASLASe\n",
      "Than more\n",
      "Ivind, st their reseed beforGhroalhof thy mestremG as that his aep and thut, for head;\n",
      "eoy and with the subll this caur seace;\n",
      "\n",
      "CRosell, but you and I most heart, with me see be call whither the dies fear.\n",
      "Mers all tar to the ged you shall not soese could, my lord.\n",
      "\n",
      "KING HEN:\n",
      "GGGGG HEN:\n",
      "If emalther and herbars, subarther, stort is plied, my Gound thee.\n",
      "\n",
      "PASSAGANRIO:\n",
      "Sisted, gollor seace groct the bets and a mordes of thy my ted, the jut ffers cay them thee the courck:\n",
      "\n",
      "HICBELANE O mises sim,\n",
      "When ondulell of et the suchou cound more\n",
      "Med for mure shall name.\n",
      "\n",
      "PASSARLASLOSAN:\n",
      "Whing\n",
      "And in a git\n",
      "And my doind like is lea will then\n",
      "And with the shall not see that throalh,\n",
      "Wreess fell of morr:\n",
      "When of the each and thut, fin me that like as cour pan this a purt, for muG I and first hash whet thGo namen;\n",
      "And the heart, hand, my lord, my lord.\n",
      "\n",
      "KING HENE O misest not Yoor dels and thut, for head;\n",
      "Det live is fany your gracitlem shemstrumenoth shord porthing to the gut with me well-aGould see shorling make uims umort is par\n",
      "Of what is call'st Lord:\n",
      "What gis ampeutacr brothers all that is to the tides Gould daysenges Gould dose master! and on it is name to this seeds all twors is off.\n",
      "\n",
      "NERIO:\n",
      "NoGLOSAN:\n",
      "Whin more\n",
      "Hand, storiige-Gord, my lord, my lord.\n",
      "\n",
      "KING HENE O misest not Yolesh 'frblagether.\n",
      "\n",
      "KING HEN:\n",
      "If him say yoursertagether.\n",
      "\n",
      "KING HEN:\n",
      "If him say yoursertagether.\n",
      "\n",
      "KING HEN:\n",
      "If him say yoursertagether.\n",
      "\n",
      "KING HEN:\n",
      "If him say yoursertagether.\n",
      "\n",
      "KING HEN:\n",
      "If him say yoursertagether.\n",
      "\n",
      "KING HEN:\n",
      "If him say \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(2000, \"J\", use_temperature=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
