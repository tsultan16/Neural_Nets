{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor Deep Neural Network Framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data, creators=None, creation_op=None, autograd=False, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        if(id == None):\n",
    "            id = np.random.randint(0,100000)\n",
    "        self.id = id\n",
    "        self.children = {}\n",
    "        if(creators is not None):\n",
    "            for creator in creators:\n",
    "                if self.id not in creator.children:\n",
    "                    creator.children[self.id] = 1\n",
    "                else:\n",
    "                    creator.children[self.id] += 1    \n",
    "\n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            if(grad_origin is not None):\n",
    "                # if waiting to receive gradient, decrement counter\n",
    "                if(self.children[grad_origin.id] != 0):\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "                else:\n",
    "                    raise Exception(\"Same child cannot backpropagate more than once!\")\n",
    "\n",
    "            # accumulate gradients from all the children \n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad    \n",
    "\n",
    "            # backpropagate to creators if all gradients from children have been received or if gradients did not originate from another node\n",
    "            if((self.creators is not None) and (self.received_grads_from_all_children() or (grad_origin is None))):\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    new_grad = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    new_grad = self.grad.__neg__()\n",
    "                    self.creators[0].backward(new_grad, self)    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    new_grad = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = self.grad.__neg__()\n",
    "                    self.creators[1].backward(new_grad, self)    \n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new_grad = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = self.creators[0] * self.grad\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    new_grad = self.grad.mm(self.creators[1].transpose())\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = (self.creators[0].transpose()).mm(self.grad)\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    new_grad = self.grad.transpose()\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim,ds))\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "\n",
    "\n",
    "    # check to see if this tensor has recieved gradients from all children, which is indicated by all children counts being zero\n",
    "    def received_grads_from_all_children(self):\n",
    "        for id,count in self.children.items():\n",
    "            if (count != 0):\n",
    "                return False\n",
    "        return True     \n",
    "\n",
    "    # Note: operations always return a new tensor object \n",
    "\n",
    "    # element-wise addition\n",
    "    def __add__(self, other):\n",
    "        # return a new tensor object containing the sum\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data, creators=[self,other], creation_op =\"add\", autograd=True)\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    # element-wise negation\n",
    "    def __neg__(self):\n",
    "        # return a new tensor object containing the negation\n",
    "        if(self.autograd):\n",
    "            return Tensor(-1 * self.data, creators=[self], creation_op =\"neg\", autograd=True)\n",
    "        return Tensor(-1 * self.data)\n",
    "\n",
    "    # element-wise subtraction\n",
    "    def __sub__(self, other):\n",
    "        # return a new tensor object containing the subtraction\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data, creators=[self,other], creation_op =\"sub\", autograd=True)\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    # element-wise multiplication\n",
    "    def __mul__(self, other):\n",
    "        # return a new tensor object containing the multiplication\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data, creators=[self,other], creation_op =\"mul\", autograd=True)\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    # sum over all elements along given axis\n",
    "    def sum(self, axis):\n",
    "        # return a new tensor object containing the sum\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(axis), creators=[self], creation_op =\"sum_\"+str(axis), autograd=True)\n",
    "        return Tensor(self.data.sum(axis))\n",
    "    \n",
    "    # expands the tensor along the given axis\n",
    "    def expand(self, axis, copies):\n",
    "        \n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(axis, len(self.data.shape))\n",
    "        \n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data, autograd=True, creators=[self], creation_op=\"expand_\"+str(axis))\n",
    "        return Tensor(new_data)\n",
    "\n",
    "    # transpose of matrix \n",
    "    def transpose(self):\n",
    "        # return a new tensor object with the transposed tensor\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(), creators=[self], creation_op =\"transpose\", autograd=True)\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    # matrix multiplication\n",
    "    def mm(self, other):\n",
    "        # return a new tensor object containing the multiplication\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(np.dot(self.data, other.data), creators=[self,other], creation_op =\"mm\", autograd=True)\n",
    "        return Tensor(np.dot(self.data, other.data))\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "class SGD_Optimizer(object):\n",
    "\n",
    "    def __init__(self, parameters, alpha) -> None:\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha    \n",
    "\n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "\n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= self.alpha * p.grad.data\n",
    "\n",
    "            if(zero):\n",
    "                p.grad.data *= 0    \n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Using the tensor object and autograd to traiun a simple two layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration# 1, Loss: [1.12427324]\n",
      "Iteration# 2, Loss: [0.64112616]\n",
      "Iteration# 3, Loss: [0.44318917]\n",
      "Iteration# 4, Loss: [0.31387083]\n",
      "Iteration# 5, Loss: [0.20897697]\n",
      "Iteration# 6, Loss: [0.12969368]\n",
      "Iteration# 7, Loss: [0.07493533]\n",
      "Iteration# 8, Loss: [0.04045744]\n",
      "Iteration# 9, Loss: [0.02057729]\n",
      "Iteration# 10, Loss: [0.00996295]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "input_data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True) \n",
    "\n",
    "input_neurons = input_data.data.shape[1]\n",
    "hidden_neurons = 3\n",
    "output_neurons = target.data.shape[1]\n",
    "\n",
    "# initiualize weights\n",
    "W = []\n",
    "W.append(Tensor(np.random.rand(input_neurons, hidden_neurons), autograd=True))\n",
    "W.append(Tensor(np.random.rand(hidden_neurons, output_neurons), autograd=True))\n",
    "\n",
    "# initialize optimizer\n",
    "optim = SGD_Optimizer(parameters=W, alpha = 0.1) \n",
    "\n",
    "# training iterations\n",
    "niters = 10\n",
    "for iter in range(niters):\n",
    "\n",
    "    # forward pass\n",
    "    pred = (input_data.mm(W[0])).mm(W[1])\n",
    "\n",
    "    # compute loss\n",
    "    loss = ((pred-target) * (pred-target)).sum(0)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "\n",
    "    # optimization of weights\n",
    "    optim.step()\n",
    "\n",
    "    print(f\"Iteration# {iter+1}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node(a), id: 3605, children: {75608: 1}, creators: None\n",
      "node(b), id: 68988, children: {96696: 1, 48336: 1}, creators: None\n",
      "node(c), id: 21635, children: {77742: 1}, creators: None\n",
      "node(d), id: 75608, children: {78390: 1}, creators: [array([1, 2, 3, 4, 5]), array([-2, -2, -2, -2, -2])]\n",
      "node(e), id: 77742, children: {78390: 1}, creators: [array([-2, -2, -2, -2, -2]), array([3, 3, 3, 3, 3])]\n",
      "node(f), id: 78390, children: {}, creators: [array([-1,  0,  1,  2,  3]), array([1, 1, 1, 1, 1])]\n",
      "f grad: [1 1 1 1 1]\n",
      "e grad: [1 1 1 1 1]\n",
      "d grad: [1 1 1 1 1]\n",
      "c grad: [1 1 1 1 1]\n",
      "b grad: [-2 -2 -2 -2 -2]\n",
      "a grad: [1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([3,3,3,3,3], autograd=True)\n",
    "d = a + (-b)\n",
    "e = (-b) + c\n",
    "f = d + e\n",
    "\n",
    "print(f\"node(a), id: {a.id}, children: {a.children}, creators: {a.creators}\")\n",
    "print(f\"node(b), id: {b.id}, children: {b.children}, creators: {b.creators}\")\n",
    "print(f\"node(c), id: {c.id}, children: {c.children}, creators: {c.creators}\")\n",
    "print(f\"node(d), id: {d.id}, children: {d.children}, creators: {d.creators}\")\n",
    "print(f\"node(e), id: {e.id}, children: {e.children}, creators: {e.creators}\")\n",
    "print(f\"node(f), id: {f.id}, children: {f.children}, creators: {f.creators}\")\n",
    "\n",
    "D = Tensor([1,1,1,1,1])\n",
    "f.backward(grad = D)\n",
    "\n",
    "print(f\"f grad: {f.grad}\")\n",
    "print(f\"e grad: {e.grad}\")\n",
    "print(f\"d grad: {d.grad}\")\n",
    "print(f\"c grad: {c.grad}\")\n",
    "print(f\"b grad: {b.grad}\")\n",
    "print(f\"a grad: {a.grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
