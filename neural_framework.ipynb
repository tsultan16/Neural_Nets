{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor Neural Network Framework** \n",
    "\n",
    "In this framework, vectors and matrices are represened by a generalized `Tensor` class. A tensor object contains the data for the vector/matrix, a unique identifier and a list of Tensor operation methods. It also includes information pertaining to how the tensor was created, e.g. if it was created by a tensor operation from other tensors, then we would call it a `child` of those `parent tensors`. (So these Tensors can be considered to form the `nodes` of a `tree`-like hierarchical structure, with data being transmitted across the node edges during forward and backward propagation). Finally, the tensor object also contains a method for computing and backpropagating deriviatives, this feature can be turned on by setting the `autograd` property to `True`. The backpropagtion occurs recursively over all the ancestors of that Tensor and stops when a Tensor which does not have any parents is reached. Any given tensor will wait until it has recieved and accumulated the backpropagted derivatives from all it's children and then it will backpropagate it's gradient to it's parents.   \n",
    "\n",
    "In addition to this Tensor class, we also create a base `Layer` class, and define a `Linear` Layer sub-class which represents a linear layer in a neural network, i.e. it has a matrix of weights and it takes a vector of input neurons and multiplies it to the weights matrix resulting in a vector of output neurons.\n",
    "\n",
    "We also create sub-classes for `non-lineararity layers` which take a vector of input neurons and operates on this vector with a non-linear function such as `sigmoid` or `relu`. Similarly, we also have a `loss function layer` for computing error/loss for a given target and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data, creators=None, creation_op=None, autograd=False, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        if(id == None):\n",
    "            id = np.random.randint(0,100000)\n",
    "        self.id = id\n",
    "        self.children = {}\n",
    "        if(creators is not None):\n",
    "            for creator in creators:\n",
    "                if self.id not in creator.children:\n",
    "                    creator.children[self.id] = 1\n",
    "                else:\n",
    "                    creator.children[self.id] += 1    \n",
    "\n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            if(grad_origin is not None):\n",
    "                # if waiting to receive gradient, decrement counter\n",
    "                if(self.children[grad_origin.id] != 0):\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "                else:\n",
    "                    raise Exception(\"Same child cannot backpropagate more than once!\")\n",
    "\n",
    "            # if this is the beginning of the backpropagtion chain\n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            # accumulate gradients from all the children \n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad    \n",
    "\n",
    "            # backpropagate to creators if all gradients from children have been received or if gradients did not originate from another node\n",
    "            if((self.creators is not None) and (self.received_grads_from_all_children() or (grad_origin is None))):\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    new_grad = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    new_grad = self.grad.__neg__()\n",
    "                    self.creators[0].backward(new_grad, self)    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    new_grad = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = self.grad.__neg__()\n",
    "                    self.creators[1].backward(new_grad, self)    \n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new_grad = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = self.creators[0] * self.grad\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    new_grad = self.grad.mm(self.creators[1].transpose())\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = (self.creators[0].transpose()).mm(self.grad)\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    new_grad = self.grad.transpose()\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    # sigmoid derivative\n",
    "                    new_grad = self.grad * (self * (ones - self))\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    # tanh derivative\n",
    "                    new_grad = self.grad * (ones - self*self)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"relu\"):\n",
    "                    # relu derivative\n",
    "                    new_grad = self.grad * (self.creators[0].data > 0)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim,ds))\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "\n",
    "\n",
    "    # check to see if this tensor has recieved gradients from all children, which is indicated by all children counts being zero\n",
    "    def received_grads_from_all_children(self):\n",
    "        for id,count in self.children.items():\n",
    "            if (count != 0):\n",
    "                return False\n",
    "        return True     \n",
    "\n",
    "    # Note: operations always return a new tensor object \n",
    "\n",
    "    # element-wise addition\n",
    "    def __add__(self, other):\n",
    "        # return a new tensor object containing the sum\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data, creators=[self,other], creation_op =\"add\", autograd=True)\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    # element-wise negation\n",
    "    def __neg__(self):\n",
    "        # return a new tensor object containing the negation\n",
    "        if(self.autograd):\n",
    "            return Tensor(-1 * self.data, creators=[self], creation_op =\"neg\", autograd=True)\n",
    "        return Tensor(-1 * self.data)\n",
    "\n",
    "    # element-wise subtraction\n",
    "    def __sub__(self, other):\n",
    "        # return a new tensor object containing the subtraction\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data, creators=[self,other], creation_op =\"sub\", autograd=True)\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    # element-wise multiplication\n",
    "    def __mul__(self, other):\n",
    "        # return a new tensor object containing the multiplication\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data, creators=[self,other], creation_op =\"mul\", autograd=True)\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    # sum over all elements along given axis\n",
    "    def sum(self, axis):\n",
    "        # return a new tensor object containing the sum\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(axis), creators=[self], creation_op =\"sum_\"+str(axis), autograd=True)\n",
    "        return Tensor(self.data.sum(axis))\n",
    "    \n",
    "    # expands the tensor along the given axis\n",
    "    def expand(self, axis, copies):\n",
    "        \n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(axis, len(self.data.shape))\n",
    "        \n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data, autograd=True, creators=[self], creation_op=\"expand_\"+str(axis))\n",
    "        return Tensor(new_data)\n",
    "\n",
    "    # transpose of matrix \n",
    "    def transpose(self):\n",
    "        # return a new tensor object with the transposed tensor\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(), creators=[self], creation_op =\"transpose\", autograd=True)\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    # matrix multiplication\n",
    "    def mm(self, other):\n",
    "        # return a new tensor object containing the multiplication\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(np.dot(self.data, other.data), creators=[self,other], creation_op =\"mm\", autograd=True)\n",
    "        return Tensor(np.dot(self.data, other.data))\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    # Non-linearity functions\n",
    "\n",
    "    # sigmoid function\n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1.0 / (1.0 + np.exp(-self.data)), creators=[self], creation_op=\"sigmoid\", autograd=True)\n",
    "        return Tensor(1.0 / (1.0 + np.exp(self.data)))\n",
    "\n",
    "    # tanh function\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data), creators=[self], creation_op=\"sigmoid\", autograd=True)\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    # relu function\n",
    "    def relu(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * (self.data > 0), creators=[self], creation_op=\"sigmoid\", autograd=True)\n",
    "        return Tensor(self.data * (self.data > 0))\n",
    "    \n",
    "# stochastic gradient descent optimizer    \n",
    "class SGD_Optimizer(object):\n",
    "\n",
    "    def __init__(self, parameters, alpha) -> None:\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha    \n",
    "\n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "\n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= self.alpha * p.grad.data\n",
    "\n",
    "            if(zero):\n",
    "                p.grad.data *= 0\n",
    "\n",
    "# layer base class\n",
    "class Layer(object):   \n",
    "    def __init__(self) -> None:\n",
    "        self.parameters = []\n",
    "\n",
    "    def get_parameters(self):                     \n",
    "        return self.parameters\n",
    "    \n",
    "# layer inherited classes\n",
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs) -> None:\n",
    "        super().__init__()\n",
    "        # initilize the weights\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/n_inputs)\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "\n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.mm(self.weight) + self.bias.expand(0,len(input.data))   \n",
    "\n",
    "# a class for a senquence of layer, i.e. a neral network model\n",
    "class Sequential(Layer):\n",
    "    def __init__(self, layers = []) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.get_parameters()\n",
    "\n",
    "        return params    \n",
    "    \n",
    "# means squared error loss function layer    \n",
    "class MSELoss(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return ((pred-target) * (pred-target)).sum(0)\n",
    "\n",
    "# nonlinearity layers\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "class Relu(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.relu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([3,3,3,3,3], autograd=True)\n",
    "d = a + (-b)\n",
    "e = (-b) + c\n",
    "f = d + e\n",
    "\n",
    "print(f\"node(a), id: {a.id}, children: {a.children}, creators: {a.creators}\")\n",
    "print(f\"node(b), id: {b.id}, children: {b.children}, creators: {b.creators}\")\n",
    "print(f\"node(c), id: {c.id}, children: {c.children}, creators: {c.creators}\")\n",
    "print(f\"node(d), id: {d.id}, children: {d.children}, creators: {d.creators}\")\n",
    "print(f\"node(e), id: {e.id}, children: {e.children}, creators: {e.creators}\")\n",
    "print(f\"node(f), id: {f.id}, children: {f.children}, creators: {f.creators}\")\n",
    "\n",
    "D = Tensor([1,1,1,1,1])\n",
    "f.backward(grad = D)\n",
    "\n",
    "print(f\"f grad: {f.grad}\")\n",
    "print(f\"e grad: {e.grad}\")\n",
    "print(f\"d grad: {d.grad}\")\n",
    "print(f\"c grad: {c.grad}\")\n",
    "print(f\"b grad: {b.grad}\")\n",
    "print(f\"a grad: {a.grad}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: Using the tensor object and autograd to train a simple two layer linear network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "input_data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True) \n",
    "\n",
    "input_neurons = input_data.data.shape[1]\n",
    "hidden_neurons = 3\n",
    "output_neurons = target.data.shape[1]\n",
    "\n",
    "# initialize neural net layers\n",
    "model = Sequential(layers=[Linear(input_neurons, hidden_neurons), Linear(hidden_neurons, output_neurons)])\n",
    "loss_layer = MSELoss()\n",
    "\n",
    "# initialize optimizer\n",
    "optim = SGD_Optimizer(parameters=model.get_parameters(), alpha = 0.05) \n",
    "\n",
    "# training iterations\n",
    "niters = 10\n",
    "for iter in range(niters):\n",
    "\n",
    "    # forward pass\n",
    "    pred = model.forward(input_data)\n",
    "\n",
    "    # compute loss\n",
    "    loss = loss_layer.forward(pred, target)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization of weights\n",
    "    optim.step()\n",
    "\n",
    "    print(f\"Iteration# {iter+1}, Loss: {loss}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: Using the tensor object and autograd to train a network with non-linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "input_data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True) \n",
    "\n",
    "input_neurons = input_data.data.shape[1]\n",
    "hidden_neurons = 3\n",
    "output_neurons = target.data.shape[1]\n",
    "\n",
    "# initialize neural net layers\n",
    "model = Sequential(layers=[Linear(input_neurons, hidden_neurons), Tanh(),Linear(hidden_neurons, output_neurons), Sigmoid()])\n",
    "loss_layer = MSELoss()\n",
    "\n",
    "# initialize optimizer\n",
    "optim = SGD_Optimizer(parameters=model.get_parameters(), alpha = 1) \n",
    "\n",
    "# training iterations\n",
    "niters = 10\n",
    "for iter in range(niters):\n",
    "\n",
    "    # forward pass\n",
    "    pred = model.forward(input_data)\n",
    "\n",
    "    # compute loss\n",
    "    loss = loss_layer.forward(pred, target)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization of weights\n",
    "    optim.step()\n",
    "\n",
    "    print(f\"Iteration# {iter+1}, Loss: {loss}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding support for language processing:**\n",
    "\n",
    "Previously we had a `linear layer` which had a matrix of weights and forward propagatopn involved computing the vector-matrix multiplication of the inputs with the weights. We will now create a similar `embedding layer` for natural language processing. The `embedding layer` will also have a weights matrix, in this case each row of the matrix will correspond to an embedding for a word from the vocabulary, and the number of rows should be set equal to the total number of words in the vocabulary. The number of columns on the other hand will be set equal to the desired number of hidden neurons.\n",
    "\n",
    "During forward propagation, the input vector is going to be a list of word indices and the output will be specific rows (corresponding to the input word indices) selected from the weights matrix. To do this, we will add an `index_select` operation into our tensor object. During backpropagation, the gradients accociated with only those specific rows will be computed, and so a copy of the input word indices will be stored in the tensor containing the selected word rows and utilized during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data, creators=None, creation_op=None, autograd=False, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        if(id == None):\n",
    "            id = np.random.randint(0,100000)\n",
    "        self.id = id\n",
    "        self.children = {}\n",
    "        if(creators is not None):\n",
    "            for creator in creators:\n",
    "                if self.id not in creator.children:\n",
    "                    creator.children[self.id] = 1\n",
    "                else:\n",
    "                    creator.children[self.id] += 1    \n",
    "\n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            if(grad_origin is not None):\n",
    "                # if waiting to receive gradient, decrement counter\n",
    "                if(self.children[grad_origin.id] != 0):\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "                else:\n",
    "                    raise Exception(\"Same child cannot backpropagate more than once!\")\n",
    "\n",
    "            # if this is the beginning of the backpropagtion chain\n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            # accumulate gradients from all the children \n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad    \n",
    "\n",
    "            # backpropagate to creators if all gradients from children have been received or if gradients did not originate from another node\n",
    "            if((self.creators is not None) and (self.received_grads_from_all_children() or (grad_origin is None))):\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    new_grad = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    new_grad = self.grad.__neg__()\n",
    "                    self.creators[0].backward(new_grad, self)    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    new_grad = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = self.grad.__neg__()\n",
    "                    self.creators[1].backward(new_grad, self)    \n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new_grad = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = self.creators[0] * self.grad\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    new_grad = self.grad.mm(self.creators[1].transpose())\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                    new_grad = (self.creators[0].transpose()).mm(self.grad)\n",
    "                    self.creators[1].backward(new_grad, self)\n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    new_grad = self.grad.transpose()\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    # sigmoid derivative\n",
    "                    new_grad = self.grad * (self * (ones - self))\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    # tanh derivative\n",
    "                    new_grad = self.grad * (ones - self*self)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"relu\"):\n",
    "                    # relu derivative\n",
    "                    new_grad = self.grad * (self.creators[0].data > 0)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    # cross entropy derivative\n",
    "                    new_grad = Tensor(self.softmax_output - self.target_dist)\n",
    "                    self.creators[0].backward(new_grad, self)\n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    # gradient of the weights matrix of word embeddings\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    # we only add gradients to the specific rows corresponding to the selected words \n",
    "                    indices_ = self.index_select_indices.data.flatten() \n",
    "                    grad_ = self.grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad), self)       \n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim,ds))\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "\n",
    "\n",
    "    # check to see if this tensor has recieved gradients from all children, which is indicated by all children counts being zero\n",
    "    def received_grads_from_all_children(self):\n",
    "        for id,count in self.children.items():\n",
    "            if (count != 0):\n",
    "                return False\n",
    "        return True     \n",
    "\n",
    "    # Note: operations always return a new tensor object \n",
    "\n",
    "    # element-wise addition\n",
    "    def __add__(self, other):\n",
    "        # return a new tensor object containing the sum\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data, creators=[self,other], creation_op =\"add\", autograd=True)\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    # element-wise negation\n",
    "    def __neg__(self):\n",
    "        # return a new tensor object containing the negation\n",
    "        if(self.autograd):\n",
    "            return Tensor(-1 * self.data, creators=[self], creation_op =\"neg\", autograd=True)\n",
    "        return Tensor(-1 * self.data)\n",
    "\n",
    "    # element-wise subtraction\n",
    "    def __sub__(self, other):\n",
    "        # return a new tensor object containing the subtraction\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data, creators=[self,other], creation_op =\"sub\", autograd=True)\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    # element-wise multiplication\n",
    "    def __mul__(self, other):\n",
    "        # return a new tensor object containing the multiplication\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data, creators=[self,other], creation_op =\"mul\", autograd=True)\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    # sum over all elements along given axis\n",
    "    def sum(self, axis):\n",
    "        # return a new tensor object containing the sum\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(axis), creators=[self], creation_op =\"sum_\"+str(axis), autograd=True)\n",
    "        return Tensor(self.data.sum(axis))\n",
    "    \n",
    "    # expands the tensor along the given axis\n",
    "    def expand(self, axis, copies):\n",
    "        \n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(axis, len(self.data.shape))\n",
    "        \n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data, autograd=True, creators=[self], creation_op=\"expand_\"+str(axis))\n",
    "        return Tensor(new_data)\n",
    "\n",
    "    # transpose of matrix \n",
    "    def transpose(self):\n",
    "        # return a new tensor object with the transposed tensor\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(), creators=[self], creation_op =\"transpose\", autograd=True)\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    # matrix multiplication\n",
    "    def mm(self, other):\n",
    "        # return a new tensor object containing the multiplication\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(np.dot(self.data, other.data), creators=[self,other], creation_op =\"mm\", autograd=True)\n",
    "        return Tensor(np.dot(self.data, other.data))\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    # Non-linearity functions\n",
    "\n",
    "    # sigmoid function\n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1.0 / (1.0 + np.exp(-self.data)), creators=[self], creation_op=\"sigmoid\", autograd=True)\n",
    "        return Tensor(1.0 / (1.0 + np.exp(-self.data)))\n",
    "\n",
    "    # tanh function\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data), creators=[self], creation_op=\"tanh\", autograd=True)\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    # relu function\n",
    "    def relu(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * (self.data > 0), creators=[self], creation_op=\"relu\", autograd=True)\n",
    "        return Tensor(self.data * (self.data > 0))\n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "\n",
    "        ex = np.exp(self.data)\n",
    "        softmax_output = ex/np.sum(ex, axis = len(self.data.shape)-1, keepdims = True) \n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t), -1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "\n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss, creators = [self], creation_op = \"cross_entropy\", autograd=True)\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out \n",
    "        return Tensor(loss) \n",
    "\n",
    "\n",
    "    # word embedding operations (the input 'indices' are just word a vector of indices, i.e. specifix row numbers that are to be selected and returned)\n",
    "    def index_select(self, indices):\n",
    "        if(self.autograd):\n",
    "            selected_rows =  Tensor(self.data[indices.data], creators=[self], creation_op=\"index_select\", autograd=True)\n",
    "            selected_rows.index_select_indices = indices \n",
    "            return selected_rows \n",
    "        return Tensor(self.data[indices.data])\n",
    "\n",
    "# stochastic gradient descent optimizer    \n",
    "class SGD_Optimizer(object):\n",
    "\n",
    "    def __init__(self, parameters, alpha) -> None:\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha    \n",
    "\n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "\n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= self.alpha * p.grad.data\n",
    "\n",
    "            if(zero):\n",
    "                p.grad.data *= 0\n",
    "\n",
    "# layer base class\n",
    "class Layer(object):   \n",
    "    def __init__(self) -> None:\n",
    "        self.parameters = []\n",
    "\n",
    "    def get_parameters(self):                     \n",
    "        return self.parameters\n",
    "    \n",
    "# layer inherited classes\n",
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs) -> None:\n",
    "        super().__init__()\n",
    "        # initilize the weights\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/n_inputs)\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "\n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.mm(self.weight) + self.bias.expand(0,len(input.data))   \n",
    "\n",
    "# embedding layer inherited class\n",
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, hidden_neurons) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "\n",
    "        # initialize the weights matrix of word embeddings \n",
    "        weight = (np.random.rand(vocab_size, hidden_neurons)-0.5)/hidden_neurons\n",
    "        self.weight = Tensor(weight, autograd=True)\n",
    "        self.parameters.append(self.weight)   \n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)    \n",
    "        \n",
    "\n",
    "# a class for a senquence of layer, i.e. a neral network model\n",
    "class Sequential(Layer):\n",
    "    def __init__(self, layers = []) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.get_parameters()\n",
    "\n",
    "        return params    \n",
    "    \n",
    "# means squared error loss function layer    \n",
    "class MSELoss(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return ((pred-target) * (pred-target)).sum(0)\n",
    "\n",
    "# cross entropy loss function layer    \n",
    "class CrossEntropyLoss(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "\n",
    "\n",
    "# nonlinearity layers\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "class Relu(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.relu()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of word embedding forward pass and backprop in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a weights matrix for a vocabulary of 5 words and 5 hidden neurons\n",
    "w = Tensor(np.eye(5), autograd=True)\n",
    "print(\"weights matrix:\")\n",
    "print(w)\n",
    "\n",
    "# forward pass for an input containing two sentence vectors with three words each\n",
    "input_indices = Tensor(np.array([[1,2,3], [2,3,4]]))\n",
    "selected_rows = w.index_select(input_indices)\n",
    "print(\"Selected rows:\")\n",
    "print(selected_rows)\n",
    "\n",
    "# compute gradient of weights for the given input\n",
    "selected_rows.backward()\n",
    "print(\"weights gradient:\")\n",
    "print(w.grad)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a training network with embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "input_data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True) \n",
    "vocab_size = 5\n",
    "hidden_neurons = 3\n",
    "output_neurons = target.data.shape[1]\n",
    "\n",
    "# initialize neural net layers\n",
    "model = Sequential(layers=[Embedding(vocab_size, hidden_neurons),Tanh(),Linear(hidden_neurons, output_neurons), Sigmoid()])\n",
    "loss_layer = MSELoss()\n",
    "\n",
    "# initialize optimizer\n",
    "optim = SGD_Optimizer(parameters=model.get_parameters(), alpha = 0.5) \n",
    "\n",
    "# training iterations\n",
    "niters = 10\n",
    "for iter in range(niters):\n",
    "\n",
    "    # forward pass\n",
    "    pred = model.forward(input_data)\n",
    "\n",
    "    # compute loss\n",
    "    loss = loss_layer.forward(pred, target)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization of weights\n",
    "    optim.step()\n",
    "\n",
    "    print(f\"Iteration# {iter+1}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# input data indices\n",
    "input_data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "# target indices\n",
    "target = Tensor(np.array([0,1,0,1]), autograd=True) \n",
    "\n",
    "vocab_size = 3\n",
    "hidden_neurons = 3\n",
    "output_neurons = len(target.data)\n",
    "\n",
    "# initialize neural net layers\n",
    "model = Sequential(layers=[Embedding(vocab_size, hidden_neurons),Tanh(),Linear(hidden_neurons, output_neurons)])\n",
    "loss_layer = CrossEntropyLoss()\n",
    "\n",
    "# initialize optimizer\n",
    "optim = SGD_Optimizer(parameters=model.get_parameters(), alpha = 0.1) \n",
    "\n",
    "# training iterations\n",
    "niters = 10\n",
    "for iter in range(niters):\n",
    "\n",
    "    # forward pass\n",
    "    pred = model.forward(input_data)\n",
    "\n",
    "    # compute loss\n",
    "    loss = loss_layer.forward(pred, target)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization of weights\n",
    "    optim.step()\n",
    "\n",
    "    print(f\"Iteration# {iter+1}, Loss: {loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a `Recurrent Layer` to handle sequenced inputs**\n",
    "\n",
    "A recurrent layer is made up of several linear and non-linear sublayers, each of these is called an `RNN cell` so that the `recurrent neural network is a chain of these RNN cells`. Then given a `sequence of input vectors`, the first vector in the sequence is fed into the first RNN cell, and a vector called the `hidden state` is computed. This hidden state is simply the input vector multiplied by a weight matrix (`W_ih`) added to the vector obtained from multiplying another weight matrix (`W_hh`) to the hidden state computed in the previous RNN cell, and the combined result is passed through a non-linearity layer (containing an activation function). Then this hiddent state is multiplied by a final weight matrix (`W_ho`) to compute a prediction. So each vector in the input sequence is fed into it's corresponding RNN cell and a hidden state and prediction are computed in its RNN cell. The `hidden state` is the key component here that contains information about the ordering of the items in the input sequence. The first RNN cell requires a hidden state to be initialized so that constitutes as an extra set of parameters.    \n",
    "\n",
    "For natural language processing, the inputs and prediction vectors are word embeddings, so the size of these vectors (i.e. number of input and output neurons) will be the size of the vocabulary. And we're free to choose any number of hidden neurons we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNcell(Layer):\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons, activation = \"sigmoid\") -> None:\n",
    "        super().__init__()\n",
    "        self.input_neurons = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        \n",
    "        # initialize the nonlinearity layer\n",
    "        if(activation == \"sigmoid\"):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == \"tanh\"):\n",
    "            self.activation = Tanh()\n",
    "        elif(activation == \"relu\"):\n",
    "            self.activation = Relu()\n",
    "        else:\n",
    "            raise Exception(\"ERROR: Non-linearity function not found!\")\n",
    "\n",
    "        # initialize the wieghts\n",
    "        self.w_ih = Linear(input_neurons, hidden_neurons)\n",
    "        self.w_hh = Linear(hidden_neurons, hidden_neurons)\n",
    "        self.w_ho = Linear(hidden_neurons, output_neurons)\n",
    "\n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "\n",
    "    def forward(self, input, prev_hidden):\n",
    "\n",
    "        # compute hidden state for this RNN cell\n",
    "        input_times_weight = self.w_ih.forward(input) \n",
    "        combined = input_times_weight + self.w_hh.forward(prev_hidden)   \n",
    "        hidden = self.activation.forward(combined)\n",
    "        #compute prediction\n",
    "        pred = self.w_ho.forward(hidden)\n",
    "       \n",
    "        return pred, hidden\n",
    "     \n",
    "    def init_hidden(self, batch_size = 1):\n",
    "        # initialize the hidden state\n",
    "        return Tensor(np.zeros(shape=(batch_size, self.hidden_neurons)), autograd=True) \n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an RNN with th Babi text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data from file\n",
    "f = open('tasksv11/en/qa1_single-supporting-fact_train.txt', 'r')\n",
    "raw = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the first 1000 senteneces (remove numbers, newline characters and punctuations)\n",
    "tokens= []\n",
    "for i, sentence in enumerate(raw[0:1000]):\n",
    "    tokenized_sent = sentence.lower().replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"?\",\"\").replace(\".\",\"\").split(\" \")[1:] \n",
    "    if((i+1)%3 == 0):\n",
    "        # get rid of number from the last word\n",
    "        last_word = tokenized_sent[-1]\n",
    "        tokenized_sent[-1] = \"\".join([char for char in last_word if not char.isnumeric()])\n",
    "    # pad the sentence at the beginning with '-' characters to make it 6 words long\n",
    "    padded_sent = ['-'] * (6 - len(tokenized_sent)) + tokenized_sent  \n",
    "    tokens.append(padded_sent)\n",
    "\n",
    "# create a vocabulary from the data\n",
    "vocab = set()\n",
    "for sentence in tokens:\n",
    "    for word in sentence:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# create a dictionary of vocab word indices\n",
    "word_index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word_index[word] = i    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting a list of words into a list of word indices\n",
    "def words_to_indices(words):\n",
    "    indices = [word_index[word] for word in words]\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the input data, i.e. list of word indices\n",
    "indices = []\n",
    "for sentence in tokens:\n",
    "    indices.append(words_to_indices(sentence))\n",
    "\n",
    "data = np.array(indices)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = 1000\n",
    "batch_size = 100\n",
    "hidden_neurons = 16\n",
    "\n",
    "# initialize the RNN layers\n",
    "embed = Embedding(len(vocab), hidden_neurons)\n",
    "# Note: since we're going to feed in outputs from the embedding layer into the RNN cell, the input neurons size needs to be equal to the the length of the embedding vectors, which is the hidden neurons size\n",
    "model = RNNcell(hidden_neurons, hidden_neurons, len(vocab))\n",
    "loss_layer = CrossEntropyLoss()\n",
    "params = embed.get_parameters() + model.get_parameters() \n",
    "optim = SGD_Optimizer(params, alpha=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train the network to predict the last word (i.e the 6th word) in every sentence in the input set\n",
    "for iter in range(niters):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    # train in batches\n",
    "    for j in range(int(len(data)/batch_size)):\n",
    "    \n",
    "        batch_lo = j * batch_size \n",
    "        batch_hi = min((j+1) * batch_size, len(data)) \n",
    "        batch = data[batch_lo:batch_hi]\n",
    "\n",
    "        sent = []\n",
    "        for ix in range(6):\n",
    "            sent.append(vocab[batch[0,ix]])\n",
    "        #if(iter == 9):\n",
    "        #    print(f\"Sentence: {sent}\")\n",
    "\n",
    "        # initilaize hidden state\n",
    "        hidden = model.init_hidden(batch_size) \n",
    "\n",
    "        # forward pass through RNN cells (5 word input sequence so 5 RNN cell passes)\n",
    "        for k in range(5):\n",
    "            input = Tensor(batch[:, k], autograd=True)\n",
    "            # create the word embedding from the input word\n",
    "            rnn_input = embed.forward(input)\n",
    "            # feed the word embedding into the RNN cell\n",
    "            prediction, hidden = model.forward(rnn_input, hidden)\n",
    " \n",
    "        # compute loss (i.e. compare predicted word from the last RNN cell to last word in the sentence)\n",
    "        target = Tensor(batch[:, 5], autograd=True)\n",
    "        loss = loss_layer.forward(prediction, target)\n",
    "        total_loss += loss.data\n",
    "\n",
    "    \n",
    "        # compute prediction accuracy\n",
    "        for ix in range(batch_size):\n",
    "            correct += int(np.argmax(prediction.data[ix]) == target.data[ix])\n",
    "        #    print(f\"Actual word: {vocab[target.data[ix]]}, Predicted word: {vocab[np.argmax(prediction.data[ix])]} \")\n",
    "            \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "       \n",
    "        # weights optimization\n",
    "        optim.step()\n",
    "\n",
    "    if((iter+1) % 20 == 0):\n",
    "        print(f\"Iteration# {iter+1}, Loss: {total_loss}, Accuracy: {float(correct)/(float(len(data)))}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the network to predict every next word in the sentence starting from the furst word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = 1000\n",
    "batch_size = 100\n",
    "hidden_neurons = 16\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize the RNN layers\n",
    "embed = Embedding(len(vocab), hidden_neurons)\n",
    "# Note: since we're going to feed in outputs from the embedding layer into the RNN cell, the input neurons size needs to be equal to the the length of the embedding vectors, which is the hidden neurons size\n",
    "model = RNNcell(hidden_neurons, hidden_neurons, len(vocab))\n",
    "loss_layers = [CrossEntropyLoss()]*5\n",
    "params = embed.get_parameters() + model.get_parameters() \n",
    "optim = SGD_Optimizer(params, alpha=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration# 1, Loss: 135.07133534693415, Accuracy: 0.2426\n",
      "Iteration# 6, Loss: 54.475279750867074, Accuracy: 0.5918\n",
      "Iteration# 11, Loss: 46.93914411871837, Accuracy: 0.592\n",
      "Iteration# 16, Loss: 45.278240458112606, Accuracy: 0.5938\n",
      "Iteration# 21, Loss: 44.67016402707425, Accuracy: 0.5928\n",
      "Iteration# 26, Loss: 44.36938368479811, Accuracy: 0.5922\n",
      "Iteration# 31, Loss: 44.192108072885425, Accuracy: 0.5926\n",
      "Iteration# 36, Loss: 44.075317320719265, Accuracy: 0.5934\n",
      "Iteration# 41, Loss: 43.992213317520616, Accuracy: 0.593\n",
      "Iteration# 46, Loss: 43.92964382241847, Accuracy: 0.593\n",
      "Iteration# 51, Loss: 43.88047806524547, Accuracy: 0.5932\n",
      "Iteration# 56, Loss: 43.840560642468525, Accuracy: 0.594\n",
      "Iteration# 61, Loss: 43.807325149922406, Accuracy: 0.5942\n",
      "Iteration# 66, Loss: 43.77909578371092, Accuracy: 0.5942\n",
      "Iteration# 71, Loss: 43.754715062264204, Accuracy: 0.5946\n",
      "Iteration# 76, Loss: 43.73334493043809, Accuracy: 0.5952\n",
      "Iteration# 81, Loss: 43.71436154596063, Accuracy: 0.5962\n",
      "Iteration# 86, Loss: 43.697296234635594, Accuracy: 0.5964\n",
      "Iteration# 91, Loss: 43.68179712572281, Accuracy: 0.5962\n",
      "Iteration# 96, Loss: 43.667600964904516, Accuracy: 0.5964\n",
      "Iteration# 101, Loss: 43.654511817991555, Accuracy: 0.5962\n",
      "Iteration# 106, Loss: 43.64238519927887, Accuracy: 0.5962\n",
      "Iteration# 111, Loss: 43.631115858721785, Accuracy: 0.596\n",
      "Iteration# 116, Loss: 43.62062725873732, Accuracy: 0.5958\n",
      "Iteration# 121, Loss: 43.61086156811725, Accuracy: 0.5958\n",
      "Iteration# 126, Loss: 43.6017705105663, Accuracy: 0.5958\n",
      "Iteration# 131, Loss: 43.5933084660107, Accuracy: 0.5958\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20300\\2226273908.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;31m# backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# weights optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20300\\3176907063.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m     81\u001b[0m                     \u001b[1;31m# cross entropy derivative\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                     \u001b[0mnew_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_output\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"index_select\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                     \u001b[1;31m# gradient of the weights matrix of word embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20300\\3176907063.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"add\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                     \u001b[0mnew_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"neg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20300\\3176907063.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m     59\u001b[0m                     \u001b[0mnew_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                     \u001b[0mnew_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"transpose\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20300\\3176907063.py\u001b[0m in \u001b[0;36mmm\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreation_op\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"mm\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautograd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# train the network to predict the last word (i.e the 6th word) in every sentence in the input set\n",
    "for iter in range(niters):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    # train in batches\n",
    "    for j in range(int(len(data)/batch_size)):\n",
    "    \n",
    "        batch_lo = j * batch_size \n",
    "        batch_hi = min((j+1) * batch_size, len(data)) \n",
    "        batch = data[batch_lo:batch_hi]\n",
    "\n",
    "        #sent = []\n",
    "        #for ix in range(6):\n",
    "        #    sent.append(vocab[batch[0,ix]])\n",
    "        #if((iter%2 == 0) and (j==1)):\n",
    "        #    print(f\"Sentence: {sent}\")\n",
    "\n",
    "        # initilaize hidden state\n",
    "        hidden = model.init_hidden(batch_size) \n",
    "\n",
    "        # forward pass through RNN cells (5 word input sequence so 5 RNN cell passes)\n",
    "        for k in range(5):\n",
    "            input = Tensor(batch[:, k], autograd=True)\n",
    "            # create the word embedding from the input word\n",
    "            rnn_input = embed.forward(input)\n",
    "            # feed the word embedding into the RNN cell\n",
    "            prediction, hidden = model.forward(rnn_input, hidden)\n",
    "    \n",
    "            # compute loss (i.e. compare predicted word from the last RNN cell to last word in the sentence)\n",
    "            target = Tensor(batch[:, k+1], autograd=True)\n",
    "            loss = loss_layers[k].forward(prediction, target)\n",
    "            total_loss += loss.data\n",
    "        \n",
    "            # compute prediction accuracy\n",
    "            for ix in range(batch_size):\n",
    "                correct += int(np.argmax(prediction.data[ix]) == target.data[ix])\n",
    "                incorrect += int(np.argmax(prediction.data[ix]) != target.data[ix])\n",
    "                \n",
    "                #if((iter%2 == 0) and (j==1)):\n",
    "                #    print(f\"Actual word: {vocab[target.data[ix]]}, Predicted word: {vocab[np.argmax(prediction.data[ix])]} \")\n",
    "                #    if(ix == batch_size-1):\n",
    "                #        print(\"\") \n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "        \n",
    "        # weights optimization\n",
    "        optim.step()\n",
    "\n",
    "    if(iter%5 == 0):\n",
    "        print(f\"Iteration# {iter+1}, Loss: {total_loss}, Accuracy: {float(correct)/(float(correct + incorrect))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
