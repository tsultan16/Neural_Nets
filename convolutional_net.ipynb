{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Network**:\n",
    "We will now replace the hidden layer from our simple neural network model with a `convolutional layer`. In the usual hidden layer, we multiply the input vector `L0` with a weights matrix `W0`. When the input size is large, i.e. each instance has a large number of feature attributes (e.g. in the case of image data with lots of pixels), we end up with a large number of weights in `W0`. This can lead to the model overfitting the training data and lower the accuracy of the predictions. This problem can be mitigated by introducing a smaller weights matrix, also called a `kernel`, and applying this `kernel` repeatedly over different subsections of the data. So for example, if we have a 28x28 (=784) pixel image input, then instead of multiplying with a weight matrix with 28x28 rows, we can use a 6x6 kernel and multiply it with every 6x6 subsection of the image. We can also use multiple different kernels to process the inputs and pass on a combination of the different kernel outputs onto the next layer.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "    Input layer class: Input layer does not perform any operations\n",
    "'''\n",
    "class input_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    ''' \n",
    "        Input layer forward pass\n",
    "    '''\n",
    "    def forward(self, L_0):\n",
    "        self.L_0 = L_0\n",
    "        return self.L_0\n",
    "    \n",
    "''' \n",
    "    Hidden layer class: Hidden layer performs 2 operations. First it performs matrix multiplication\n",
    "                        of inputs L_0 with weights W_0. Then it operates on this result with the activation\n",
    "                        function.\n",
    "'''    \n",
    "class hidden_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self, W, activation) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "        self.activation = activation\n",
    "        print(f\"Hidden layer activation function: {activation}\")\n",
    "\n",
    "    ''' \n",
    "        Hidden layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L, dropout): \n",
    "        self.L = L\n",
    "        return self.forward_matrix_mult(dropout)\n",
    "\n",
    "    def forward_matrix_mult(self, dropout):\n",
    "        self.Z =  np.dot(self.L, self.W)\n",
    "        self.dropout = dropout \n",
    "        if(self.dropout):\n",
    "            # generate a random dropout mask with rougly equal numbers of 0s and 1s\n",
    "            self.dropout_mask = np.random.randint(0,2,size=(self.Z.shape))\n",
    "           \n",
    "        if(self.activation == \"relu\"):\n",
    "            if(self.dropout):\n",
    "              # multiply by a factor of 2 to compensate for rougly 1/2 the neurons being turned off by the masking\n",
    "              return 2 * self.dropout_mask * self.forward_relu()\n",
    "            else:\n",
    "                return self.forward_relu()\n",
    "    \n",
    "        elif(self.activation == \"sigmoid\"):\n",
    "            if(self.dropout):\n",
    "              return 2 * self.dropout_mask * self.forward_sigmoid()\n",
    "            else:\n",
    "                return self.forward_sigmoid()\n",
    "    \n",
    "        elif(self.activation == \"tanh\"):\n",
    "            if(self.dropout):\n",
    "              return 2 * self.dropout_mask * self.forward_tanh()\n",
    "            else:\n",
    "                return self.forward_tanh()\n",
    "    \n",
    "    def forward_relu(self):\n",
    "        return Relu(self.Z)\n",
    "    \n",
    "    def forward_sigmoid(self):\n",
    "        return sigmoid(self.Z)\n",
    "   \n",
    "    def forward_tanh(self):\n",
    "        return tanh(self.Z)\n",
    "    \n",
    "    ''' \n",
    "        Hidden layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self, D):\n",
    "        if(self.activation == \"relu\"):\n",
    "           self.backward_relu(D)\n",
    "        elif(self.activation == \"sigmoid\"):\n",
    "           self.backward_sigmoid(D)\n",
    "        elif(self.activation == \"tanh\"):\n",
    "           self.backward_tanh(D)\n",
    "\n",
    "    def backward_relu(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * Relu_deriv(self.Z) \n",
    "        self.backward_matrix_mult(dE_dZ)\n",
    "    \n",
    "    def backward_sigmoid(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * sigmoid_deriv(self.Z) \n",
    "        self.backward_matrix_mult(dE_dZ)\n",
    "    \n",
    "    def backward_tanh(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * tanh_deriv(self.Z) \n",
    "        self.backward_matrix_mult(dE_dZ)\n",
    "    \n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW0\n",
    "        if(self.dropout):\n",
    "            self.W_grad = np.dot((self.L).T, self.dropout_mask * D)\n",
    "        else:\n",
    "            self.W_grad = np.dot((self.L).T, D)\n",
    "\n",
    "    ''' \n",
    "        Gradient descent optimization of hidden layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        self.W -= alpha * self.W_grad\n",
    "\n",
    "''' \n",
    "    Convolutional layer class: This layer performs two operations. First, it computes the matrix multiplication of subsections odf the input with the kernels. Then it operates on the result with the activation function\n",
    "'''\n",
    "class convolutional_layer(object):\n",
    "\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self, K, image_rows, image_cols, kernel_rows, kernel_cols, activation) -> None:\n",
    "        self.K = K\n",
    "        self.image_cols = image_cols\n",
    "        self.image_rows = image_rows\n",
    "        self.kernel_cols = kernel_cols\n",
    "        self.kernel_rows = kernel_rows\n",
    "        self.activation = activation\n",
    "        self.K_grad = np.zeros_like(K)\n",
    "\n",
    "    \n",
    "    ''' \n",
    "        convolutional layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L, dropout):\n",
    "       \n",
    "        # reshape the input image array\n",
    "        L = L.reshape(L.shape[0], self.image_rows, self.image_cols)\n",
    "\n",
    "        # get all sub-sections from the image\n",
    "        sections = []\n",
    "        for i in range(self.image_rows-self.kernel_rows+1):\n",
    "            for j in range(self.image_cols-self.kernel_cols+1):\n",
    "                section = L[:,i:i+self.kernel_rows, j:j+self.kernel_cols]   \n",
    "                section = section.reshape(-1,1,self.kernel_rows,self.kernel_cols)\n",
    "                sections.append(section)\n",
    "     \n",
    "        # concatenate all sections into a single array\n",
    "        expanded_input = np.concatenate(sections, axis=1)    \n",
    "        input_shape = expanded_input.shape \n",
    "        #print(f\"expanded input shape: {input_shape} \")\n",
    "        \n",
    "        # flatten the sections\n",
    "        self.flattened_input = expanded_input.reshape(expanded_input.shape[0]*expanded_input.shape[1], -1) \n",
    "        #print(f\"flattened input shape: {self.flattened_input.shape}\")\n",
    "        #print(expanded_input)\n",
    "\n",
    "        return self.kernel_mult(input_shape, dropout)\n",
    "    \n",
    "    def kernel_mult(self, input_shape, dropout):\n",
    "\n",
    "        # matrix multiplication of flattened image sections with kernels\n",
    "        self.Z = np.dot(self.flattened_input, self.K) \n",
    "        #print(f\"kernel output shape: {self.Z.shape}\")\n",
    "        #print(self.Z)\n",
    "        \n",
    "        # flatten the kernel output for each image\n",
    "        self.Zflat = self.Z.copy()        \n",
    "        self.Zflat = self.Zflat.reshape(input_shape[0], -1)        \n",
    "        \n",
    "        #print(f\"kernel output flattened shape: {self.Zflat.shape}\")\n",
    "        #print(self.Zflat)\n",
    "\n",
    "        self.dropout = dropout \n",
    "        if(self.dropout):\n",
    "            # generate a random dropout mask with rougly equal numbers of 0s and 1s\n",
    "            self.dropout_mask = np.random.randint(0,2,size=(self.Zflat.shape))\n",
    "    \n",
    "        if(self.activation == \"relu\"):\n",
    "            if(self.dropout):\n",
    "              # multiply by a factor of 2 to compensate for rougly 1/2 the neurons being turned off by the masking\n",
    "              return 2 * self.dropout_mask * self.forward_relu()\n",
    "            else:\n",
    "                return self.forward_relu()\n",
    "    \n",
    "        elif(self.activation == \"sigmoid\"):\n",
    "            if(self.dropout):\n",
    "              return 2 * self.dropout_mask * self.forward_sigmoid()\n",
    "            else:\n",
    "                return self.forward_sigmoid()\n",
    "    \n",
    "        elif(self.activation == \"tanh\"):\n",
    "            if(self.dropout):\n",
    "              return 2 * self.dropout_mask * self.forward_tanh()\n",
    "            else:\n",
    "                return self.forward_tanh()\n",
    "    \n",
    "    def forward_relu(self):\n",
    "        return Relu(self.Zflat)\n",
    "    \n",
    "    def forward_sigmoid(self):\n",
    "        return sigmoid(self.Zflat)\n",
    "   \n",
    "    def forward_tanh(self):\n",
    "        return tanh(self.Zflat)\n",
    "    \n",
    "    ''' \n",
    "        convolutional layer backpropagation\n",
    "    '''\n",
    "    def backward(self, D):\n",
    "        if(self.activation == \"relu\"):\n",
    "           self.backward_relu(D)\n",
    "        elif(self.activation == \"sigmoid\"):\n",
    "           self.backward_sigmoid(D)\n",
    "        elif(self.activation == \"tanh\"):\n",
    "           self.backward_tanh(D)\n",
    "\n",
    "    def backward_relu(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * Relu_deriv(self.Zflat) \n",
    "        self.backward_kernel_mult(dE_dZ)\n",
    "    \n",
    "    def backward_sigmoid(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * sigmoid_deriv(self.Zflat) \n",
    "        self.backward_kernel_mult(dE_dZ)\n",
    "    \n",
    "    def backward_tanh(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * tanh_deriv(self.Zflat) \n",
    "        self.backward_kernel_mult(dE_dZ)\n",
    "    \n",
    "    def backward_kernel_mult(self, D):\n",
    "        # dE/dW0\n",
    "        if(self.dropout):\n",
    "            self.Kgrad = np.dot((self.flattened_input).T, self.dropout_mask * D.reshape(self.Z.shape))\n",
    "        else:\n",
    "            self.K_grad = np.dot((self.flattened_input).T, D.reshape(self.Z.shape))\n",
    "\n",
    "    ''' \n",
    "        Gradient descent optimization of convolutional layer kernels\n",
    "    '''\n",
    "    def update_kernels(self, alpha):\n",
    "        self.K -= alpha * self.K_grad\n",
    "\n",
    "\n",
    "''' \n",
    "    Ouput layer class: Performs two operations, first matrix multiplication of inputs L_1 with weights\n",
    "                       W_1. This result is then operated on by squared error function.  \n",
    "'''\n",
    "class output_layer(object):\n",
    "    \n",
    "    ''' \n",
    "        class constructor\n",
    "    '''\n",
    "\n",
    "    def __init__(self, W) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "\n",
    "    ''' \n",
    "        Output layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L, Y, soft):\n",
    "        self.L = L\n",
    "        self.Y = Y\n",
    "        return self.forward_matrix_mult(soft)\n",
    "\n",
    "    def forward_matrix_mult(self, soft):\n",
    "        self.P = np.dot(self.L, self.W) \n",
    "        if(soft):\n",
    "            self.P = softmax(self.P)\n",
    "        \n",
    "        return self.P, self.forward_error()\n",
    " \n",
    "    def forward_error(self):\n",
    "        return np.sum((self.P - self.Y)**2) / self.P.shape[0]\n",
    "\n",
    "    '''     \n",
    "        Output layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self):\n",
    "        return self.backward_error()\n",
    "\n",
    "    def backward_error(self):\n",
    "        # dE/dP\n",
    "        dE_dP = 2*(self.P - self.Y) / self.P.shape[0]\n",
    "        return self.backward_matrix_mult(dE_dP)\n",
    "\n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW1\n",
    "        self.W_grad = np.dot((self.L).T, D)\n",
    "        # dE/dL1\n",
    "        dE_dL = np.dot(D, (self.W).T)\n",
    "        return dE_dL\n",
    "    \n",
    "    ''' \n",
    "        Gradient descent optimization of output layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        self.W -= alpha * self.W_grad\n",
    "\n",
    "'''\n",
    "    A 3-layer convolutional neural network class\n",
    "'''\n",
    "class three_layer_convolutional_net(object):\n",
    "    ''' \n",
    "        class constructor: Takes in the following parameters- number of neurons in input layer (which is the number of feature attributes for each instance), number of hidden layers (has to be at least 1 and can be arbitrarily large), number of neurons in the output layer (which is the number of target attributes) and gradient descent step-size (alpha)\n",
    "    '''\n",
    "    def __init__(self, input_neurons, output_neurons, image_rows, image_cols, kernel_rows, kernel_cols, num_kernels, convolutional_layer_activation = \"relu\") -> None:\n",
    "        self.input_neurons  = input_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.image_rows = image_rows\n",
    "        self.image_cols = image_cols\n",
    "        self.kernel_rows = kernel_rows\n",
    "        self.kernel_cols = kernel_cols\n",
    "        self.num_kernels = num_kernels\n",
    "\n",
    "        np.random.seed(1)\n",
    "        # initialize kernels for convolutional layer \n",
    "        K = 0.02*np.random.random(size=(self.kernel_rows*self.kernel_cols, self.num_kernels)) - 0.01\n",
    "\n",
    "        hidden_neurons = (self.image_rows-self.kernel_rows+1) * (self.image_cols-self.kernel_cols+1) * self.num_kernels\n",
    "\n",
    "        # initialize weights W1 between hidden layer and output layer\n",
    "        W1 = 0.2*np.random.random(size=(hidden_neurons, output_neurons)) - 0.1 \n",
    "\n",
    "        # initialize layer objects\n",
    "        self.layer_0 = input_layer()\n",
    "        self.layer_1 = convolutional_layer(K, self.image_rows, self.image_cols, self.kernel_rows, self.kernel_cols, activation=convolutional_layer_activation)\n",
    "        self.layer_2 = output_layer(W1)\n",
    "\n",
    "    ''' \n",
    "        neural network forward pass\n",
    "    '''\n",
    "    def forward_net(self, L0, Y, dropout, soft):\n",
    "        # input layer forward pass\n",
    "        self.L0 = self.layer_0.forward(L0) \n",
    "        # hidden layer forward pass \n",
    "        self.L1 = self.layer_1.forward(self.L0, dropout) \n",
    "        # output layer forward pass\n",
    "        self.L2, error = self.layer_2.forward(self.L1, Y, soft) \n",
    "\n",
    "        return self.L2, error\n",
    "\n",
    "    ''' \n",
    "        neural network backward pass\n",
    "    ''' \n",
    "    def backward_net(self):\n",
    "       # output layer backpropagation\n",
    "       D = self.layer_2.backward() \n",
    "       # hidden layer backpropagation\n",
    "       self.layer_1.backward(D) \n",
    "\n",
    "    '''     \n",
    "        kernel and weight optimization\n",
    "    '''\n",
    "    def optimize(self, alpha):\n",
    "        # update output layer weights\n",
    "        self.layer_2.update_weights(alpha)\n",
    "        # update hidden layer weights\n",
    "        self.layer_1.update_kernels(alpha)\n",
    "\n",
    "    '''     \n",
    "        train the network\n",
    "    ''' \n",
    "    def train(self, X_train, y_train, X_test, y_test, alpha, batch_size=1, niters=1, dropout=False, soft=False):\n",
    "        print(f\"Dropout Enabled: {dropout}\")\n",
    "        print(f\"Softmax Enabled: {soft}\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Alpha: {alpha}\")\n",
    "        print(\"Training in progress...\")\n",
    "        #training iterations\n",
    "        for i in range(niters):\n",
    "            total_error = 0.0\n",
    "            train_correct_count = 0\n",
    "            # train using batch of instances\n",
    "            for j in range(int(X_train.shape[0]/batch_size)):\n",
    "\n",
    "                lo = j * batch_size\n",
    "                hi = min((j+1) * batch_size, X_train.shape[0])\n",
    "\n",
    "                X = X_train[lo:hi]\n",
    "                y = y_train[lo:hi]\n",
    "\n",
    "                # forward propagation\n",
    "                print(\"Forward propagation in progress...\")\n",
    "                prediction, error = self.forward_net(X, y, dropout, soft)\n",
    "                total_error += error\n",
    "                \n",
    "                print(\"Forward propagation completed.\")\n",
    "\n",
    "                for k in range(hi-lo):\n",
    "                    train_correct_count += int(np.argmax(prediction[k]) == np.argmax(y[k]))\n",
    "                \n",
    "                #if(i == (niters-1)):\n",
    "                #    print(f\"Instance# {j+1}, Target: {y}, Prediction: {prediction}\")\n",
    "\n",
    "                # backpropagation\n",
    "                print(\"Backward propagation in progress...\")\n",
    "                self.backward_net()\n",
    "                print(\"Backward propagation completed\")\n",
    "\n",
    "                # weight optimization\n",
    "                self.optimize(alpha)\n",
    "\n",
    "            # predict using test instances\n",
    "            test_correct_count = 0\n",
    "            for j in range(X_test.shape[0]):\n",
    "                X = X_test[j:j+1]\n",
    "                y = y_test[j:j+1]\n",
    "\n",
    "                # forward propagation\n",
    "                prediction, error = self.forward_net(X, y, dropout=False, soft=False)\n",
    "                test_correct_count += int(np.argmax(prediction) == np.argmax(y))\n",
    "\n",
    "            print(f\"Iteration# {i+1}, Total error: {total_error}, Training accuracy: {train_correct_count/len(y_train)}, Testing accuracy: {test_correct_count/len(y_test)}\")\n",
    "\n",
    "# Relu function\n",
    "def Relu(x):\n",
    "    return x*(x > 0)\n",
    "\n",
    "# Relu derivative function\n",
    "def Relu_deriv(x):\n",
    "    return (x > 0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return (1.0 - np.tanh(x)**2)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-1.0 * x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x)) \n",
    "\n",
    "def softmax(x): \n",
    "    ex = np.exp(x)\n",
    "    return ex/np.sum(ex, axis = 1, keepdims = True)  \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our 3 layer convolutional neural network model with the `MNIST dataset` of handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (60000, 28, 28)\n",
      "y_train shape = (60000,)\n",
      "X_test shape = (10000, 28, 28)\n",
      "y_test shape = (10000,)\n",
      "Dropout Enabled: True\n",
      "Softmax Enabled: True\n",
      "Batch size: 100\n",
      "Alpha: 0.05\n",
      "Training in progress...\n",
      "Forward propagation in progress...\n",
      "Forward propagation completed.\n",
      "Backward propagation in progress...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "maximum supported dimension for an ndarray is 32, found 67600",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15304\\950046049.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# train the net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mthree_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mniters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15304\\3827982929.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_train, y_train, X_test, y_test, alpha, batch_size, niters, dropout, soft)\u001b[0m\n\u001b[0;32m    401\u001b[0m                 \u001b[1;31m# backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Backward propagation in progress...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Backward propagation completed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15304\\3827982929.py\u001b[0m in \u001b[0;36mbackward_net\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m        \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m        \u001b[1;31m# hidden layer backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     '''     \n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15304\\3827982929.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, D)\u001b[0m\n\u001b[0;32m    216\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_sigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"tanh\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_tanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15304\\3827982929.py\u001b[0m in \u001b[0;36mbackward_tanh\u001b[1;34m(self, D)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;31m# dE/dZ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mdE_dZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtanh_deriv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZflat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_kernel_mult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdE_dZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward_kernel_mult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15304\\3827982929.py\u001b[0m in \u001b[0;36mbackward_kernel_mult\u001b[1;34m(self, D)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;31m# dE/dW0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflattened_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_mask\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mK_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflattened_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: maximum supported dimension for an ndarray is 32, found 67600"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    MNIST dataset of handwritten digits:\n",
    "\n",
    "    Each observation is an image. The input values per image are 28x28 pixels (i.e. 784 features/inputs per observation).\n",
    "'''\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # x values contain image pixels (i.e. features) and y vaues are the corresponding labels\n",
    "\n",
    "print(f\"X_train shape = {X_train.shape}\")\n",
    "print(f\"y_train shape = {y_train.shape}\")\n",
    "print(f\"X_test shape = {X_test.shape}\")\n",
    "print(f\"y_test shape = {y_test.shape}\")\n",
    "\n",
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "\n",
    "# flatten image pixels array\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]) \n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]) \n",
    "\n",
    "# normalize of pixel values from (0,255) to (0,1)\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one-hot encode the labels\n",
    "y_train_onehot = np.zeros(shape=(y_train.shape[0], 10))\n",
    "y_test_onehot = np.zeros(shape=(y_test.shape[0], 10))\n",
    "\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_train_onehot[i, y_train[i]] = 1\n",
    "\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_onehot[i, y_test[i]] = 1\n",
    "\n",
    "# training dataset preparation\n",
    "training_images = X_train[0:1000]\n",
    "training_labels = y_train_onehot[0:1000]\n",
    "testing_images = X_test\n",
    "testing_labels = y_test_onehot\n",
    "\n",
    "# initialize a three layer convolutional neural net object\n",
    "three_net = three_layer_convolutional_net(input_neurons=training_images.shape[1], output_neurons=training_labels.shape[1], image_rows=img_rows, image_cols=img_cols, kernel_rows=3, kernel_cols=3, num_kernels=16, convolutional_layer_activation=\"tanh\")\n",
    "\n",
    "# train the net\n",
    "three_net.train(training_images, training_labels, testing_images, testing_labels, alpha=0.05, batch_size=100, niters=300, dropout=True, soft=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden neurons: 32, Output neurons: 10\n",
      "expanded input shape: (2, 16, 3, 3) \n",
      "flattened input shape: (32, 9)\n",
      "kernel output shape: (32, 2)\n",
      "[[ 12.97390274  16.5086609 ]\n",
      " [ 17.4617429   22.51895906]\n",
      " [ 21.94958306  28.52925723]\n",
      " [ 26.43742322  34.53955539]\n",
      " [ 17.4617429   22.51895906]\n",
      " [ 21.94958306  28.52925723]\n",
      " [ 26.43742322  34.53955539]\n",
      " [ 30.92526338  40.54985356]\n",
      " [ 21.94958306  28.52925723]\n",
      " [ 26.43742322  34.53955539]\n",
      " [ 30.92526338  40.54985356]\n",
      " [ 35.41310354  46.56015173]\n",
      " [ 26.43742322  34.53955539]\n",
      " [ 30.92526338  40.54985356]\n",
      " [ 35.41310354  46.56015173]\n",
      " [ 39.9009437   52.57044989]\n",
      " [ 25.94780547  33.01732179]\n",
      " [ 34.92348579  45.03791812]\n",
      " [ 43.89916612  57.05851446]\n",
      " [ 52.87484644  69.07911079]\n",
      " [ 34.92348579  45.03791812]\n",
      " [ 43.89916612  57.05851446]\n",
      " [ 52.87484644  69.07911079]\n",
      " [ 61.85052676  81.09970712]\n",
      " [ 43.89916612  57.05851446]\n",
      " [ 52.87484644  69.07911079]\n",
      " [ 61.85052676  81.09970712]\n",
      " [ 70.82620708  93.12030345]\n",
      " [ 52.87484644  69.07911079]\n",
      " [ 61.85052676  81.09970712]\n",
      " [ 70.82620708  93.12030345]\n",
      " [ 79.8018874  105.14089978]]\n",
      "kernel output flattened shape: (2, 32)\n",
      "[[ 12.97390274  16.5086609   17.4617429   22.51895906  21.94958306\n",
      "   28.52925723  26.43742322  34.53955539  17.4617429   22.51895906\n",
      "   21.94958306  28.52925723  26.43742322  34.53955539  30.92526338\n",
      "   40.54985356  21.94958306  28.52925723  26.43742322  34.53955539\n",
      "   30.92526338  40.54985356  35.41310354  46.56015173  26.43742322\n",
      "   34.53955539  30.92526338  40.54985356  35.41310354  46.56015173\n",
      "   39.9009437   52.57044989]\n",
      " [ 25.94780547  33.01732179  34.92348579  45.03791812  43.89916612\n",
      "   57.05851446  52.87484644  69.07911079  34.92348579  45.03791812\n",
      "   43.89916612  57.05851446  52.87484644  69.07911079  61.85052676\n",
      "   81.09970712  43.89916612  57.05851446  52.87484644  69.07911079\n",
      "   61.85052676  81.09970712  70.82620708  93.12030345  52.87484644\n",
      "   69.07911079  61.85052676  81.09970712  70.82620708  93.12030345\n",
      "   79.8018874  105.14089978]]\n",
      "L1 shape: (2, 32)\n"
     ]
    }
   ],
   "source": [
    "num_images = 2\n",
    "image_rows = 6\n",
    "image_cols = 6\n",
    "images = np.zeros(shape=(num_images,image_rows,image_cols)) # 2 images\n",
    "\n",
    "for k in range(num_images):\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            images[k,i,j] = (k+1)*(i + j + 1)\n",
    "\n",
    "#print(images)\n",
    "kernel_rows = 3\n",
    "kernel_cols = 3\n",
    "num_kernels = 2\n",
    "hidden_neurons = (image_rows-kernel_rows+1) * (image_cols-kernel_cols+1) * num_kernels\n",
    "output_neurons = 10 # number of image labels\n",
    "\n",
    "print(f\"Hidden neurons: {hidden_neurons}, Output neurons: {output_neurons}\")\n",
    "\n",
    "# initiailize kernels and output layer weights \n",
    "kernels = np.random.random(size=(kernel_rows*kernel_cols, num_kernels))\n",
    "W1 = np.random.random(size=(hidden_neurons, output_neurons))\n",
    "\n",
    "\n",
    "clayer = convolutional_layer(kernels,image_rows,image_cols,kernel_rows,kernel_cols, activation = \"tanh\")\n",
    "\n",
    "L0 = images\n",
    "L1 = clayer.forward(L0, dropout = False)\n",
    "print(f\"L1 shape: {L1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
