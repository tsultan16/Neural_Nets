{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Author: Tanzid Sultan\n",
    "\n",
    "**Convolutional Neural Network**:\n",
    "We will now replace the hidden layer from our simple neural network model with a `convolutional layer`. In the usual hidden layer, we multiply the input vector `L0` with a weights matrix `W0`. When the input size is large, i.e. each instance has a large number of feature attributes (e.g. in the case of image data with lots of pixels), we end up with a large number of weights in `W0`. This can lead to the model overfitting the training data and lower the accuracy of the predictions. This problem can be mitigated by introducing a smaller weights matrix, also called a `kernel`, and applying this `kernel` repeatedly over different subsections of the data. So for example, if we have a 28x28 (=784) pixel image input, then instead of multiplying with a weight matrix with 28x28 rows, we can use a 6x6 kernel and multiply it with every 6x6 subsection of the image. We can also use multiple different kernels to process the inputs and pass on a combination of the different kernel outputs onto the next layer.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "    Input layer class: Input layer does not perform any operations\n",
    "'''\n",
    "class input_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    ''' \n",
    "        Input layer forward pass\n",
    "    '''\n",
    "    def forward(self, L_0):\n",
    "        self.L_0 = L_0\n",
    "        return self.L_0\n",
    "    \n",
    "''' \n",
    "    Hidden layer class: Hidden layer performs 2 operations. First it performs matrix multiplication\n",
    "                        of inputs L_0 with weights W_0. Then it operates on this result with the activation\n",
    "                        function.\n",
    "'''    \n",
    "class hidden_layer(object):\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self, W, activation) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "        self.activation = activation\n",
    "        print(f\"Hidden layer activation function: {activation}\")\n",
    "\n",
    "    ''' \n",
    "        Hidden layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L, dropout): \n",
    "        self.L = L\n",
    "        return self.forward_matrix_mult(dropout)\n",
    "\n",
    "    def forward_matrix_mult(self, dropout):\n",
    "        self.Z =  np.dot(self.L, self.W)\n",
    "        self.dropout = dropout \n",
    "        if(self.dropout):\n",
    "            # generate a random dropout mask with rougly equal numbers of 0s and 1s\n",
    "            self.dropout_mask = np.random.randint(0,2,size=(self.Z.shape))\n",
    "           \n",
    "        if(self.activation == \"relu\"):\n",
    "            if(self.dropout):\n",
    "              # multiply by a factor of 2 to compensate for rougly 1/2 the neurons being turned off by the masking\n",
    "              return 2 * self.dropout_mask * self.forward_relu()\n",
    "            else:\n",
    "                return self.forward_relu()\n",
    "    \n",
    "        elif(self.activation == \"sigmoid\"):\n",
    "            if(self.dropout):\n",
    "              return 2 * self.dropout_mask * self.forward_sigmoid()\n",
    "            else:\n",
    "                return self.forward_sigmoid()\n",
    "    \n",
    "        elif(self.activation == \"tanh\"):\n",
    "            if(self.dropout):\n",
    "              return 2 * self.dropout_mask * self.forward_tanh()\n",
    "            else:\n",
    "                return self.forward_tanh()\n",
    "    \n",
    "    def forward_relu(self):\n",
    "        return Relu(self.Z)\n",
    "    \n",
    "    def forward_sigmoid(self):\n",
    "        return sigmoid(self.Z)\n",
    "   \n",
    "    def forward_tanh(self):\n",
    "        return tanh(self.Z)\n",
    "    \n",
    "    ''' \n",
    "        Hidden layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self, D):\n",
    "        if(self.activation == \"relu\"):\n",
    "           self.backward_relu(D)\n",
    "        elif(self.activation == \"sigmoid\"):\n",
    "           self.backward_sigmoid(D)\n",
    "        elif(self.activation == \"tanh\"):\n",
    "           self.backward_tanh(D)\n",
    "\n",
    "    def backward_relu(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * Relu_deriv(self.Z) \n",
    "        self.backward_matrix_mult(dE_dZ)\n",
    "    \n",
    "    def backward_sigmoid(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * sigmoid_deriv(self.Z) \n",
    "        self.backward_matrix_mult(dE_dZ)\n",
    "    \n",
    "    def backward_tanh(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * tanh_deriv(self.Z) \n",
    "        self.backward_matrix_mult(dE_dZ)\n",
    "    \n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW0\n",
    "        if(self.dropout):\n",
    "            self.W_grad = np.dot((self.L).T, self.dropout_mask * D)\n",
    "        else:\n",
    "            self.W_grad = np.dot((self.L).T, D)\n",
    "\n",
    "    ''' \n",
    "        Gradient descent optimization of hidden layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        self.W -= alpha * self.W_grad\n",
    "\n",
    "''' \n",
    "    Convolutional layer class: This layer performs two operations. First, it computes the matrix multiplication of subsections odf the input with the kernels. Then it operates on the result with the activation function\n",
    "'''\n",
    "class convolutional_layer(object):\n",
    "\n",
    "    '''\n",
    "        class constructor\n",
    "    '''\n",
    "    def __init__(self, K, image_rows, image_cols, kernel_rows, kernel_cols, activation) -> None:\n",
    "        self.K = K\n",
    "        self.image_cols = image_cols\n",
    "        self.image_rows = image_rows\n",
    "        self.kernel_cols = kernel_cols\n",
    "        self.kernel_rows = kernel_rows\n",
    "        self.activation = activation\n",
    "        self.K_grad = np.zeros_like(K)\n",
    "\n",
    "    \n",
    "    ''' \n",
    "        convolutional layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L, dropout):\n",
    "       \n",
    "        # reshape the input image array\n",
    "        L = L.reshape(L.shape[0], self.image_rows, self.image_cols)\n",
    "\n",
    "        # get all sub-sections from the image\n",
    "        sections = []\n",
    "        for i in range(self.image_rows-self.kernel_rows+1):\n",
    "            for j in range(self.image_cols-self.kernel_cols+1):\n",
    "                section = L[:,i:i+self.kernel_rows, j:j+self.kernel_cols]   \n",
    "                section = section.reshape(-1,1,self.kernel_rows,self.kernel_cols)\n",
    "                sections.append(section)\n",
    "     \n",
    "        # concatenate all sections into a single array\n",
    "        expanded_input = np.concatenate(sections, axis=1)    \n",
    "        input_shape = expanded_input.shape \n",
    "        \n",
    "        # flatten the sections\n",
    "        self.flattened_input = expanded_input.reshape(expanded_input.shape[0]*expanded_input.shape[1], -1) \n",
    "\n",
    "        return self.kernel_mult(input_shape, dropout)\n",
    "    \n",
    "    def kernel_mult(self, input_shape, dropout):\n",
    "\n",
    "        # matrix multiplication of flattened image sections with kernels\n",
    "        self.Z = np.dot(self.flattened_input, self.K) \n",
    "        \n",
    "        # flatten the kernel output for each image\n",
    "        self.Zflat = self.Z.copy()        \n",
    "        self.Zflat = self.Zflat.reshape(input_shape[0], -1)        \n",
    "    \n",
    "        self.dropout = dropout \n",
    "        if(self.dropout):\n",
    "            # generate a random dropout mask with rougly equal numbers of 0s and 1s\n",
    "            self.dropout_mask = np.random.randint(0,2,size=(self.Zflat.shape))\n",
    "    \n",
    "        if(self.activation == \"relu\"):\n",
    "            if(self.dropout):\n",
    "              # multiply by a factor of 2 to compensate for rougly 1/2 the neurons being turned off by the masking\n",
    "              return 2 * self.dropout_mask * self.forward_relu()\n",
    "            else:\n",
    "                return self.forward_relu()\n",
    "    \n",
    "        elif(self.activation == \"sigmoid\"):\n",
    "            if(self.dropout):\n",
    "              return 2 * self.dropout_mask * self.forward_sigmoid()\n",
    "            else:\n",
    "                return self.forward_sigmoid()\n",
    "    \n",
    "        elif(self.activation == \"tanh\"):\n",
    "            if(self.dropout):\n",
    "              return 2 * self.dropout_mask * self.forward_tanh()\n",
    "            else:\n",
    "                return self.forward_tanh()\n",
    "    \n",
    "    def forward_relu(self):\n",
    "        return Relu(self.Zflat)\n",
    "    \n",
    "    def forward_sigmoid(self):\n",
    "        return sigmoid(self.Zflat)\n",
    "   \n",
    "    def forward_tanh(self):\n",
    "        return tanh(self.Zflat)\n",
    "    \n",
    "    ''' \n",
    "        convolutional layer backpropagation\n",
    "    '''\n",
    "    def backward(self, D):\n",
    "        if(self.activation == \"relu\"):\n",
    "           self.backward_relu(D)\n",
    "        elif(self.activation == \"sigmoid\"):\n",
    "           self.backward_sigmoid(D)\n",
    "        elif(self.activation == \"tanh\"):\n",
    "           self.backward_tanh(D)\n",
    "\n",
    "    def backward_relu(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * Relu_deriv(self.Zflat) \n",
    "        self.backward_kernel_mult(dE_dZ)\n",
    "    \n",
    "    def backward_sigmoid(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * sigmoid_deriv(self.Zflat) \n",
    "        self.backward_kernel_mult(dE_dZ)\n",
    "    \n",
    "    def backward_tanh(self, D):\n",
    "        # dE/dZ\n",
    "        dE_dZ = D * tanh_deriv(self.Zflat) \n",
    "        self.backward_kernel_mult(dE_dZ)\n",
    "    \n",
    "    def backward_kernel_mult(self, D):\n",
    "        # dE/dK\n",
    "        if(self.dropout):\n",
    "            self.Kgrad = np.dot((self.flattened_input).T, (self.dropout_mask * D).reshape(self.Z.shape))\n",
    "        else:\n",
    "            self.K_grad = np.dot((self.flattened_input).T, D.reshape(self.Z.shape))\n",
    "\n",
    "    ''' \n",
    "        Gradient descent optimization of convolutional layer kernels\n",
    "    '''\n",
    "    def update_kernels(self, alpha):\n",
    "        self.K -= alpha * self.K_grad\n",
    "\n",
    "\n",
    "''' \n",
    "    Ouput layer class: Performs two operations, first matrix multiplication of inputs L_1 with weights\n",
    "                       W_1. This result is then operated on by squared error function.  \n",
    "'''\n",
    "class output_layer(object):\n",
    "    \n",
    "    ''' \n",
    "        class constructor\n",
    "    '''\n",
    "\n",
    "    def __init__(self, W) -> None:\n",
    "        self.W = W\n",
    "        self.W_grad = np.zeros_like(W)\n",
    "\n",
    "    ''' \n",
    "        Output layer forward propagation\n",
    "    '''\n",
    "    def forward(self, L, Y, soft):\n",
    "        self.L = L\n",
    "        self.Y = Y\n",
    "        return self.forward_matrix_mult(soft)\n",
    "\n",
    "    def forward_matrix_mult(self, soft):\n",
    "        self.P = np.dot(self.L, self.W) \n",
    "        if(soft):\n",
    "            self.P = softmax(self.P)\n",
    "        \n",
    "        return self.P, self.forward_error()\n",
    " \n",
    "    def forward_error(self):\n",
    "        return np.sum((self.P - self.Y)**2) / self.P.shape[0]\n",
    "\n",
    "    '''     \n",
    "        Output layer backpropagation of derivatives\n",
    "    '''\n",
    "    def backward(self):\n",
    "        return self.backward_error()\n",
    "\n",
    "    def backward_error(self):\n",
    "        # dE/dP\n",
    "        dE_dP = 2*(self.P - self.Y) / self.P.shape[0]\n",
    "        return self.backward_matrix_mult(dE_dP)\n",
    "\n",
    "    def backward_matrix_mult(self, D):\n",
    "        # dE/dW1\n",
    "        self.W_grad = np.dot((self.L).T, D)\n",
    "        # dE/dL1\n",
    "        dE_dL = np.dot(D, (self.W).T)\n",
    "        return dE_dL\n",
    "    \n",
    "    ''' \n",
    "        Gradient descent optimization of output layer weights\n",
    "    '''\n",
    "    def update_weights(self, alpha):\n",
    "        self.W -= alpha * self.W_grad\n",
    "\n",
    "'''\n",
    "    A 3-layer convolutional neural network class\n",
    "'''\n",
    "class three_layer_convolutional_net(object):\n",
    "    ''' \n",
    "        class constructor: Takes in the following parameters- number of neurons in input layer (which is the number of feature attributes for each instance), number of hidden layers (has to be at least 1 and can be arbitrarily large), number of neurons in the output layer (which is the number of target attributes) and gradient descent step-size (alpha)\n",
    "    '''\n",
    "    def __init__(self, input_neurons, output_neurons, image_rows, image_cols, kernel_rows, kernel_cols, num_kernels, convolutional_layer_activation = \"relu\") -> None:\n",
    "        self.input_neurons  = input_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.image_rows = image_rows\n",
    "        self.image_cols = image_cols\n",
    "        self.kernel_rows = kernel_rows\n",
    "        self.kernel_cols = kernel_cols\n",
    "        self.num_kernels = num_kernels\n",
    "\n",
    "        np.random.seed(1)\n",
    "        # initialize kernels for convolutional layer \n",
    "        K = 0.02*np.random.random(size=(self.kernel_rows*self.kernel_cols, self.num_kernels)) - 0.01\n",
    "\n",
    "        hidden_neurons = (self.image_rows-self.kernel_rows+1) * (self.image_cols-self.kernel_cols+1) * self.num_kernels\n",
    "\n",
    "        # initialize weights W1 between hidden layer and output layer\n",
    "        W1 = 0.2*np.random.random(size=(hidden_neurons, output_neurons)) - 0.1 \n",
    "\n",
    "        # initialize layer objects\n",
    "        self.layer_0 = input_layer()\n",
    "        self.layer_1 = convolutional_layer(K, self.image_rows, self.image_cols, self.kernel_rows, self.kernel_cols, activation=convolutional_layer_activation)\n",
    "        self.layer_2 = output_layer(W1)\n",
    "\n",
    "    ''' \n",
    "        neural network forward pass\n",
    "    '''\n",
    "    def forward_net(self, L0, Y, dropout, soft):\n",
    "        # input layer forward pass\n",
    "        self.L0 = self.layer_0.forward(L0) \n",
    "        # hidden layer forward pass \n",
    "        self.L1 = self.layer_1.forward(self.L0, dropout) \n",
    "        # output layer forward pass\n",
    "        self.L2, error = self.layer_2.forward(self.L1, Y, soft) \n",
    "\n",
    "        return self.L2, error\n",
    "\n",
    "    ''' \n",
    "        neural network backward pass\n",
    "    ''' \n",
    "    def backward_net(self):\n",
    "       # output layer backpropagation\n",
    "       D = self.layer_2.backward() \n",
    "       # hidden layer backpropagation\n",
    "       self.layer_1.backward(D) \n",
    "\n",
    "    '''     \n",
    "        kernel and weight optimization\n",
    "    '''\n",
    "    def optimize(self, alpha):\n",
    "        # update output layer weights\n",
    "        self.layer_2.update_weights(alpha)\n",
    "        # update hidden layer weights\n",
    "        self.layer_1.update_kernels(alpha)\n",
    "\n",
    "    '''     \n",
    "        train the network\n",
    "    ''' \n",
    "    def train(self, X_train, y_train, X_test, y_test, alpha, batch_size=1, niters=1, dropout=False, soft=False):\n",
    "        print(f\"Dropout Enabled: {dropout}\")\n",
    "        print(f\"Softmax Enabled: {soft}\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Alpha: {alpha}\")\n",
    "        print(\"Training in progress...\")\n",
    "        #training iterations\n",
    "        for i in range(niters):\n",
    "            total_error = 0.0\n",
    "            train_correct_count = 0\n",
    "            # train using batch of instances\n",
    "            for j in range(int(X_train.shape[0]/batch_size)):\n",
    "\n",
    "                lo = j * batch_size\n",
    "                hi = min((j+1) * batch_size, X_train.shape[0])\n",
    "\n",
    "                X = X_train[lo:hi]\n",
    "                y = y_train[lo:hi]\n",
    "\n",
    "                # forward propagation\n",
    "                prediction, error = self.forward_net(X, y, dropout, soft)\n",
    "                total_error += error\n",
    "                \n",
    "                for k in range(hi-lo):\n",
    "                    train_correct_count += int(np.argmax(prediction[k]) == np.argmax(y[k]))\n",
    "                \n",
    "                #if(i == (niters-1)):\n",
    "                #    print(f\"Instance# {j+1}, Target: {y}, Prediction: {prediction}\")\n",
    "\n",
    "                # backpropagation\n",
    "                self.backward_net()\n",
    "                \n",
    "                # weight optimization\n",
    "                self.optimize(alpha)\n",
    "\n",
    "            # predict using test instances\n",
    "            test_correct_count = 0\n",
    "            for j in range(X_test.shape[0]):\n",
    "                X = X_test[j:j+1]\n",
    "                y = y_test[j:j+1]\n",
    "\n",
    "                # forward propagation\n",
    "                prediction, error = self.forward_net(X, y, dropout=False, soft=False)\n",
    "                test_correct_count += int(np.argmax(prediction) == np.argmax(y))\n",
    "\n",
    "            print(f\"Iteration# {i+1}, Total error: {total_error}, Training accuracy: {train_correct_count/len(y_train)}, Testing accuracy: {test_correct_count/len(y_test)}\")\n",
    "\n",
    "# Relu function\n",
    "def Relu(x):\n",
    "    return x*(x > 0)\n",
    "\n",
    "# Relu derivative function\n",
    "def Relu_deriv(x):\n",
    "    return (x > 0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return (1.0 - np.tanh(x)**2)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-1.0 * x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x)) \n",
    "\n",
    "def softmax(x): \n",
    "    ex = np.exp(x)\n",
    "    return ex/np.sum(ex, axis = 1, keepdims = True)  \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our 3 layer convolutional neural network model with the `MNIST dataset` of handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (60000, 28, 28)\n",
      "y_train shape = (60000,)\n",
      "X_test shape = (10000, 28, 28)\n",
      "y_test shape = (10000,)\n",
      "Dropout Enabled: True\n",
      "Softmax Enabled: True\n",
      "Batch size: 100\n",
      "Alpha: 0.05\n",
      "Training in progress...\n",
      "Iteration# 1, Total error: 8.99563648783043, Training accuracy: 0.12, Testing accuracy: 0.1167\n",
      "Iteration# 2, Total error: 8.982189246819486, Training accuracy: 0.144, Testing accuracy: 0.137\n",
      "Iteration# 3, Total error: 8.973909173428853, Training accuracy: 0.155, Testing accuracy: 0.1606\n",
      "Iteration# 4, Total error: 8.96136492440829, Training accuracy: 0.166, Testing accuracy: 0.1951\n",
      "Iteration# 5, Total error: 8.94987377521972, Training accuracy: 0.21, Testing accuracy: 0.2339\n",
      "Iteration# 6, Total error: 8.94157253184402, Training accuracy: 0.241, Testing accuracy: 0.2738\n",
      "Iteration# 7, Total error: 8.931137270023754, Training accuracy: 0.262, Testing accuracy: 0.3142\n",
      "Iteration# 8, Total error: 8.92264070092538, Training accuracy: 0.284, Testing accuracy: 0.3468\n",
      "Iteration# 9, Total error: 8.90531280816144, Training accuracy: 0.338, Testing accuracy: 0.3774\n",
      "Iteration# 10, Total error: 8.897566656890863, Training accuracy: 0.344, Testing accuracy: 0.4038\n",
      "Iteration# 11, Total error: 8.891996282700434, Training accuracy: 0.363, Testing accuracy: 0.4291\n",
      "Iteration# 12, Total error: 8.880555077583791, Training accuracy: 0.38, Testing accuracy: 0.4531\n",
      "Iteration# 13, Total error: 8.867554209446237, Training accuracy: 0.403, Testing accuracy: 0.4716\n",
      "Iteration# 14, Total error: 8.857886248524236, Training accuracy: 0.456, Testing accuracy: 0.4873\n",
      "Iteration# 15, Total error: 8.842917251123746, Training accuracy: 0.468, Testing accuracy: 0.5032\n",
      "Iteration# 16, Total error: 8.837401413171737, Training accuracy: 0.477, Testing accuracy: 0.5175\n",
      "Iteration# 17, Total error: 8.82816908367246, Training accuracy: 0.48, Testing accuracy: 0.531\n",
      "Iteration# 18, Total error: 8.816790155024192, Training accuracy: 0.497, Testing accuracy: 0.5426\n",
      "Iteration# 19, Total error: 8.807406541140304, Training accuracy: 0.529, Testing accuracy: 0.5538\n",
      "Iteration# 20, Total error: 8.79349685379529, Training accuracy: 0.526, Testing accuracy: 0.5644\n",
      "Iteration# 21, Total error: 8.78338030231006, Training accuracy: 0.549, Testing accuracy: 0.5722\n",
      "Iteration# 22, Total error: 8.774112657785667, Training accuracy: 0.559, Testing accuracy: 0.5792\n",
      "Iteration# 23, Total error: 8.765162912053583, Training accuracy: 0.573, Testing accuracy: 0.5851\n",
      "Iteration# 24, Total error: 8.752458203931884, Training accuracy: 0.588, Testing accuracy: 0.5912\n",
      "Iteration# 25, Total error: 8.741031900546846, Training accuracy: 0.588, Testing accuracy: 0.5961\n",
      "Iteration# 26, Total error: 8.729915611991348, Training accuracy: 0.599, Testing accuracy: 0.6008\n",
      "Iteration# 27, Total error: 8.718639590183354, Training accuracy: 0.61, Testing accuracy: 0.6049\n",
      "Iteration# 28, Total error: 8.709202535926288, Training accuracy: 0.605, Testing accuracy: 0.609\n",
      "Iteration# 29, Total error: 8.698765378296535, Training accuracy: 0.617, Testing accuracy: 0.6135\n",
      "Iteration# 30, Total error: 8.685435482444122, Training accuracy: 0.621, Testing accuracy: 0.6166\n",
      "Iteration# 31, Total error: 8.67150380588274, Training accuracy: 0.633, Testing accuracy: 0.62\n",
      "Iteration# 32, Total error: 8.662761813447723, Training accuracy: 0.622, Testing accuracy: 0.6244\n",
      "Iteration# 33, Total error: 8.657567669873506, Training accuracy: 0.639, Testing accuracy: 0.6277\n",
      "Iteration# 34, Total error: 8.641839262792118, Training accuracy: 0.646, Testing accuracy: 0.6308\n",
      "Iteration# 35, Total error: 8.62848251068809, Training accuracy: 0.645, Testing accuracy: 0.633\n",
      "Iteration# 36, Total error: 8.62000274651728, Training accuracy: 0.645, Testing accuracy: 0.6355\n",
      "Iteration# 37, Total error: 8.610331778504557, Training accuracy: 0.655, Testing accuracy: 0.637\n",
      "Iteration# 38, Total error: 8.598138498590723, Training accuracy: 0.664, Testing accuracy: 0.6401\n",
      "Iteration# 39, Total error: 8.585782895289354, Training accuracy: 0.664, Testing accuracy: 0.6412\n",
      "Iteration# 40, Total error: 8.579163152495436, Training accuracy: 0.673, Testing accuracy: 0.6435\n",
      "Iteration# 41, Total error: 8.557792589120375, Training accuracy: 0.664, Testing accuracy: 0.6452\n",
      "Iteration# 42, Total error: 8.560965384630236, Training accuracy: 0.665, Testing accuracy: 0.6468\n",
      "Iteration# 43, Total error: 8.545394466467421, Training accuracy: 0.669, Testing accuracy: 0.6492\n",
      "Iteration# 44, Total error: 8.528143079884133, Training accuracy: 0.671, Testing accuracy: 0.6504\n",
      "Iteration# 45, Total error: 8.519081329646797, Training accuracy: 0.675, Testing accuracy: 0.6516\n",
      "Iteration# 46, Total error: 8.505200677058877, Training accuracy: 0.679, Testing accuracy: 0.6523\n",
      "Iteration# 47, Total error: 8.495700348920472, Training accuracy: 0.682, Testing accuracy: 0.6535\n",
      "Iteration# 48, Total error: 8.489099049858249, Training accuracy: 0.661, Testing accuracy: 0.6549\n",
      "Iteration# 49, Total error: 8.474208333972893, Training accuracy: 0.682, Testing accuracy: 0.6558\n",
      "Iteration# 50, Total error: 8.463066100008941, Training accuracy: 0.69, Testing accuracy: 0.6559\n",
      "Iteration# 51, Total error: 8.458606065697973, Training accuracy: 0.697, Testing accuracy: 0.6568\n",
      "Iteration# 52, Total error: 8.444226829459996, Training accuracy: 0.682, Testing accuracy: 0.6573\n",
      "Iteration# 53, Total error: 8.431144542201123, Training accuracy: 0.685, Testing accuracy: 0.6594\n",
      "Iteration# 54, Total error: 8.423849547836259, Training accuracy: 0.696, Testing accuracy: 0.6603\n",
      "Iteration# 55, Total error: 8.40889545873648, Training accuracy: 0.692, Testing accuracy: 0.6618\n",
      "Iteration# 56, Total error: 8.39535768443395, Training accuracy: 0.685, Testing accuracy: 0.6628\n",
      "Iteration# 57, Total error: 8.389780441612375, Training accuracy: 0.69, Testing accuracy: 0.6635\n",
      "Iteration# 58, Total error: 8.376698666079536, Training accuracy: 0.693, Testing accuracy: 0.6648\n",
      "Iteration# 59, Total error: 8.367502480616254, Training accuracy: 0.701, Testing accuracy: 0.6659\n",
      "Iteration# 60, Total error: 8.351006084410056, Training accuracy: 0.705, Testing accuracy: 0.6671\n",
      "Iteration# 61, Total error: 8.342934372520135, Training accuracy: 0.708, Testing accuracy: 0.6691\n",
      "Iteration# 62, Total error: 8.33233016388881, Training accuracy: 0.698, Testing accuracy: 0.67\n",
      "Iteration# 63, Total error: 8.317779966233292, Training accuracy: 0.695, Testing accuracy: 0.6703\n",
      "Iteration# 64, Total error: 8.312883895718972, Training accuracy: 0.704, Testing accuracy: 0.6714\n",
      "Iteration# 65, Total error: 8.302569634826142, Training accuracy: 0.689, Testing accuracy: 0.6725\n",
      "Iteration# 66, Total error: 8.287933072749098, Training accuracy: 0.698, Testing accuracy: 0.6731\n",
      "Iteration# 67, Total error: 8.273565029545665, Training accuracy: 0.717, Testing accuracy: 0.6734\n",
      "Iteration# 68, Total error: 8.259039574594691, Training accuracy: 0.713, Testing accuracy: 0.674\n",
      "Iteration# 69, Total error: 8.250959350346251, Training accuracy: 0.712, Testing accuracy: 0.6747\n",
      "Iteration# 70, Total error: 8.235947096252907, Training accuracy: 0.704, Testing accuracy: 0.6749\n",
      "Iteration# 71, Total error: 8.23066152765188, Training accuracy: 0.711, Testing accuracy: 0.6755\n",
      "Iteration# 72, Total error: 8.21994859151095, Training accuracy: 0.702, Testing accuracy: 0.6759\n",
      "Iteration# 73, Total error: 8.20474423071612, Training accuracy: 0.717, Testing accuracy: 0.6771\n",
      "Iteration# 74, Total error: 8.200557002780318, Training accuracy: 0.712, Testing accuracy: 0.6776\n",
      "Iteration# 75, Total error: 8.185264424096308, Training accuracy: 0.712, Testing accuracy: 0.6781\n",
      "Iteration# 76, Total error: 8.174025840288202, Training accuracy: 0.724, Testing accuracy: 0.6789\n",
      "Iteration# 77, Total error: 8.164495385610794, Training accuracy: 0.711, Testing accuracy: 0.68\n",
      "Iteration# 78, Total error: 8.151292658233926, Training accuracy: 0.717, Testing accuracy: 0.681\n",
      "Iteration# 79, Total error: 8.140402606002032, Training accuracy: 0.722, Testing accuracy: 0.6814\n",
      "Iteration# 80, Total error: 8.122826362369679, Training accuracy: 0.726, Testing accuracy: 0.6822\n",
      "Iteration# 81, Total error: 8.118114438247826, Training accuracy: 0.719, Testing accuracy: 0.683\n",
      "Iteration# 82, Total error: 8.104564429636948, Training accuracy: 0.719, Testing accuracy: 0.6829\n",
      "Iteration# 83, Total error: 8.089090952846414, Training accuracy: 0.729, Testing accuracy: 0.683\n",
      "Iteration# 84, Total error: 8.085808164205751, Training accuracy: 0.719, Testing accuracy: 0.684\n",
      "Iteration# 85, Total error: 8.0693418149334, Training accuracy: 0.717, Testing accuracy: 0.685\n",
      "Iteration# 86, Total error: 8.062498111016756, Training accuracy: 0.724, Testing accuracy: 0.6858\n",
      "Iteration# 87, Total error: 8.044713920048364, Training accuracy: 0.728, Testing accuracy: 0.6864\n",
      "Iteration# 88, Total error: 8.040415874618303, Training accuracy: 0.722, Testing accuracy: 0.6866\n",
      "Iteration# 89, Total error: 8.030458560588727, Training accuracy: 0.724, Testing accuracy: 0.6869\n",
      "Iteration# 90, Total error: 8.011458050229134, Training accuracy: 0.738, Testing accuracy: 0.6874\n",
      "Iteration# 91, Total error: 8.00093630127979, Training accuracy: 0.739, Testing accuracy: 0.6879\n",
      "Iteration# 92, Total error: 7.985840909380153, Training accuracy: 0.72, Testing accuracy: 0.6883\n",
      "Iteration# 93, Total error: 7.979258469312316, Training accuracy: 0.73, Testing accuracy: 0.6888\n",
      "Iteration# 94, Total error: 7.969103088185805, Training accuracy: 0.737, Testing accuracy: 0.6891\n",
      "Iteration# 95, Total error: 7.954589949989612, Training accuracy: 0.73, Testing accuracy: 0.6897\n",
      "Iteration# 96, Total error: 7.945602905108426, Training accuracy: 0.724, Testing accuracy: 0.6904\n",
      "Iteration# 97, Total error: 7.934250458783966, Training accuracy: 0.732, Testing accuracy: 0.6911\n",
      "Iteration# 98, Total error: 7.9220376050401144, Training accuracy: 0.725, Testing accuracy: 0.6918\n",
      "Iteration# 99, Total error: 7.915203198507167, Training accuracy: 0.731, Testing accuracy: 0.6928\n",
      "Iteration# 100, Total error: 7.899721083363747, Training accuracy: 0.733, Testing accuracy: 0.6932\n",
      "Iteration# 101, Total error: 7.889966769237719, Training accuracy: 0.727, Testing accuracy: 0.6937\n",
      "Iteration# 102, Total error: 7.875879966308413, Training accuracy: 0.738, Testing accuracy: 0.6945\n",
      "Iteration# 103, Total error: 7.8680807196841664, Training accuracy: 0.73, Testing accuracy: 0.6948\n",
      "Iteration# 104, Total error: 7.853964045683909, Training accuracy: 0.735, Testing accuracy: 0.6952\n",
      "Iteration# 105, Total error: 7.836862169639944, Training accuracy: 0.735, Testing accuracy: 0.6956\n",
      "Iteration# 106, Total error: 7.8323474233906785, Training accuracy: 0.728, Testing accuracy: 0.6961\n",
      "Iteration# 107, Total error: 7.824608924532836, Training accuracy: 0.732, Testing accuracy: 0.6966\n",
      "Iteration# 108, Total error: 7.8099448377034175, Training accuracy: 0.735, Testing accuracy: 0.6967\n",
      "Iteration# 109, Total error: 7.798242862234415, Training accuracy: 0.742, Testing accuracy: 0.6969\n",
      "Iteration# 110, Total error: 7.792006653455477, Training accuracy: 0.743, Testing accuracy: 0.6976\n",
      "Iteration# 111, Total error: 7.770580750786432, Training accuracy: 0.73, Testing accuracy: 0.6979\n",
      "Iteration# 112, Total error: 7.762296859493796, Training accuracy: 0.744, Testing accuracy: 0.6981\n",
      "Iteration# 113, Total error: 7.752023319006158, Training accuracy: 0.737, Testing accuracy: 0.6985\n",
      "Iteration# 114, Total error: 7.741594795889514, Training accuracy: 0.734, Testing accuracy: 0.699\n",
      "Iteration# 115, Total error: 7.730987508019082, Training accuracy: 0.745, Testing accuracy: 0.6995\n",
      "Iteration# 116, Total error: 7.724559811782305, Training accuracy: 0.734, Testing accuracy: 0.7\n",
      "Iteration# 117, Total error: 7.704204382419978, Training accuracy: 0.742, Testing accuracy: 0.7003\n",
      "Iteration# 118, Total error: 7.696998145172827, Training accuracy: 0.748, Testing accuracy: 0.7003\n",
      "Iteration# 119, Total error: 7.685784931203884, Training accuracy: 0.736, Testing accuracy: 0.7009\n",
      "Iteration# 120, Total error: 7.676173799219903, Training accuracy: 0.738, Testing accuracy: 0.7009\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    MNIST dataset of handwritten digits:\n",
    "\n",
    "    Each observation is an image. The input values per image are 28x28 pixels (i.e. 784 features/inputs per observation).\n",
    "'''\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # x values contain image pixels (i.e. features) and y vaues are the corresponding labels\n",
    "\n",
    "print(f\"X_train shape = {X_train.shape}\")\n",
    "print(f\"y_train shape = {y_train.shape}\")\n",
    "print(f\"X_test shape = {X_test.shape}\")\n",
    "print(f\"y_test shape = {y_test.shape}\")\n",
    "\n",
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "\n",
    "# flatten image pixels array\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]) \n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]) \n",
    "\n",
    "# normalize of pixel values from (0,255) to (0,1)\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one-hot encode the labels\n",
    "y_train_onehot = np.zeros(shape=(y_train.shape[0], 10))\n",
    "y_test_onehot = np.zeros(shape=(y_test.shape[0], 10))\n",
    "\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_train_onehot[i, y_train[i]] = 1\n",
    "\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_onehot[i, y_test[i]] = 1\n",
    "\n",
    "# training dataset preparation\n",
    "training_images = X_train[0:1000]\n",
    "training_labels = y_train_onehot[0:1000]\n",
    "testing_images = X_test\n",
    "testing_labels = y_test_onehot\n",
    "\n",
    "# initialize a three layer convolutional neural net object\n",
    "three_net = three_layer_convolutional_net(input_neurons=training_images.shape[1], output_neurons=training_labels.shape[1], image_rows=img_rows, image_cols=img_cols, kernel_rows=3, kernel_cols=3, num_kernels=16, convolutional_layer_activation=\"tanh\")\n",
    "\n",
    "# train the net\n",
    "three_net.train(training_images, training_labels, testing_images, testing_labels, alpha=0.05, batch_size=100, niters=300, dropout=True, soft=True)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
